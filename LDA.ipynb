{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0b783670-fe2c-473f-b272-0bd89b7a0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe1d710b-0c52-4241-874c-851c3c5166b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('articles.csv')\n",
    "reddit = pd.read_csv('reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9209a70a-bb6b-4a11-95dc-501f321d592a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meme Mondays</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com//r/datascience/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Friendly reminder not to work too hard. You'll...</td>\n",
       "      <td>The year just started and there are already ov...</td>\n",
       "      <td>https://www.reddit.com//r/datascience/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data scientist quits her job at Spotify</td>\n",
       "      <td>In summary and basically talks about how she w...</td>\n",
       "      <td>https://www.reddit.com//r/datascience/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XKCD Comic does machine learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com//r/datascience/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I investigated the Underground Economy of Glas...</td>\n",
       "      <td>Online company reviews are high stakes.\\n\\nTop...</td>\n",
       "      <td>https://www.reddit.com//r/datascience/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                       Meme Mondays   \n",
       "1  Friendly reminder not to work too hard. You'll...   \n",
       "2            Data scientist quits her job at Spotify   \n",
       "3                   XKCD Comic does machine learning   \n",
       "4  I investigated the Underground Economy of Glas...   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                NaN   \n",
       "1  The year just started and there are already ov...   \n",
       "2  In summary and basically talks about how she w...   \n",
       "3                                                NaN   \n",
       "4  Online company reviews are high stakes.\\n\\nTop...   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.reddit.com//r/datascience/comments...  \n",
       "1  https://www.reddit.com//r/datascience/comments...  \n",
       "2  https://www.reddit.com//r/datascience/comments...  \n",
       "3  https://www.reddit.com//r/datascience/comments...  \n",
       "4  https://www.reddit.com//r/datascience/comments...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6023fb85-360a-4c57-af0e-1ee3447d9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 994 entries, 0 to 993\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   994 non-null    object\n",
      " 1   text    893 non-null    object\n",
      " 2   link    994 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7687583b-5bb9-40ce-82ad-7ec99d270f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2f841907-2683-45d0-95a3-0b4d03f3f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_2468\\3878923509.py:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  text = re.sub('http\\S+', '', text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "custom_stop_words = ['ds', 'data', 'data science', 'science', 'u', 'like', 'would', 'one', 'also', 'even', 'much', 'hey']\n",
    "\n",
    "def text_processing(text):\n",
    "    \n",
    "    text = str(text).replace('\\n', ' ')\n",
    "    text = str(text).replace('e.g.', ' ')\n",
    "    \n",
    "    text = re.sub('http\\S+', '', text)\n",
    "    text = re.sub('-', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = ' '.join([word for word in text.split() if word not in (stop_words)])\n",
    "    text = ''.join([word for word in text if not word.isdigit()])\n",
    "\n",
    "    for word in custom_stop_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(word), '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e839f10b-8166-4edd-805d-88f4ceb377e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(text):\n",
    "    text = text_processing(text)\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "            \n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(word for word in text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "reddit['text'] = reddit['text'].apply(lambda x: lemmatization(x))\n",
    "articles['text'] = articles['text'].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a617235a-78b2-4fc8-8bc9-9238d8491c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world director make k vp anything make k world compensation increase become smaller higher promotion present completely achievable following path reality take lot luck politics become anything higher manager happens rarely'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80024ca-fb2d-49f1-bf11-ffb44ecb1a5a",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34dcc68e-3b6d-4fdb-9d96-94e3e43fa35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "for word, num in kw_extractor.extract_keywords(reddit.text[4]):\n",
    "    print(type(reddit.text[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f6997a40-3e90-4b15-8f46-279cb6807ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "659aed74-10f0-448c-abe0-bbf618b6a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        new = [word for word, num in kw_extractor.extract_keywords(' '.join(new))]\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "data_words = gen_words(reddit.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "63409de4-b1cf-4ac4-b5c8-7306c0e8995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "018f5138-1627-48da-9bf8-91b41b5724b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model  = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, random_state =100, update_every = 1,\n",
    "                                            passes = 10, alpha = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3a72341d-009e-4050-9493-e06589c796dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\sklearn\\manifold\\_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el246830528326178889856295555\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el246830528326178889856295555_data = {\"mdsDat\": {\"x\": [0.1545480038785646, 0.04521633154786671, -0.1318803255865792, -0.09594902919744946, 0.05982742037212322, -0.06316786831017832, -0.026493894052840663, -0.16733113263061064, 0.06736940681353441, 0.15786108716556937], \"y\": [0.0938267416860398, 0.16054106936686655, -0.10926771006156584, 0.15224636866733646, -0.17160485625584246, 0.06093895858791773, -0.11991944998455112, 0.0013484750992674155, -0.021725862914606944, -0.04638373419086153], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [11.33134143003549, 10.750642138338257, 10.723365448430016, 10.338370396572339, 10.017477705810238, 9.935686899494996, 9.476026744019014, 9.406702704120287, 9.224175756902115, 8.79621077627725]}, \"tinfo\": {\"Term\": [\"working\", \"curious\", \"thought\", \"scientist\", \"job\", \"machine learning\", \"learning\", \"position\", \"year\", \"python\", \"give\", \"key aspect lot\", \"make final modeling\", \"mcol midwest type\", \"back time relearn\", \"datasets server power\", \"week major finance\", \"financial software\", \"complaint people\", \"searching job stressfuldon\", \"thought\", \"job find\", \"presentation\", \"low hanging fruit\", \"curious\", \"trench scientist engineer\", \"background natural fit\", \"company joined good\", \"find fending hostile\", \"stand case urgent\", \"solution running poc\", \"distinct misfortune working\", \"working\", \"start\", \"scientist\", \"python\", \"ide\", \"job posting require\", \"understand\", \"startup\", \"microsoft scientist excited\", \"interviewing senior role\", \"including amazon msft\", \"awareness recent relevant\", \"mention medtech startup\", \"create path world\", \"role online landed\", \"job tailor portfolio\", \"year worked year\", \"working\", \"lot\", \"time\", \"skill\", \"year work experience\", \"month\", \"start\", \"junior mid level\", \"research\", \"offered\", \"friend\", \"anecdotal confirmation bias\", \"wrong thought building\", \"analyst basic\", \"move dedicated statistic\", \"edit nearing\", \"deck day long\", \"learning make beautiful\", \"company\", \"good\", \"scientist\", \"job\", \"machine learning\", \"test\", \"great question\", \"train\", \"title\", \"provide context working\", \"computer vision deep\", \"hope challenging interesting\", \"provide presentation odd\", \"talent acquisition girl\", \"honestly amount gate\", \"hyper parameter tuning\", \"interesting\", \"gpt\", \"graph\", \"forward exciting insight\", \"struggling dax creating\", \"case copy guy\", \"cybersecurity yoe\", \"domain quick blurb\", \"project recommender search\", \"hoggers extremely high\", \"super fresh neurocomputing\", \"job\", \"python\", \"thing\", \"field\", \"scientist\", \"expectation\", \"applicant\", \"machine learning\", \"open\", \"local paper thought\", \"involve quantitative analysis\", \"disorganized future applicant\", \"confluence writing code\", \"youtube video\", \"university entered program\", \"trapped indoors\", \"stats analytics background\", \"beating dead\", \"worried\", \"thought\", \"month ago\", \"quit hate community\", \"fulltime benefit scientist\", \"language solve\", \"integrate machine learning\", \"thinking lot job\", \"risk taking edit\", \"wellbeing due constant\", \"clientfacing people find\", \"large work load\", \"year\", \"machine learning engineer\", \"learning engineer feel\", \"level massively fail\", \"colleague overuse oop\", \"weekly report\", \"november getting lot\", \"world hating\", \"advice greatly appreciated\", \"effectively guide open\", \"dumbest thing guy\", \"management team company\", \"shiny powerpoint deck\", \"position\", \"learning\", \"job\", \"machine learning\", \"scientist\", \"level\", \"experience\", \"past month subreddit\", \"decision simply step\", \"polars ingest kaggle\", \"turn notebook complete\", \"gpt started smoking\", \"education completely\", \"company burnt wrestling\", \"based response solution\", \"scientist paper past\", \"emotionless uncaring blunt\", \"making reached find\", \"scientist\", \"machine learning\", \"job\", \"work\", \"company\", \"machine learning algorithm\", \"machine learning engineer\", \"learning engineer\"], \"Freq\": [3.0, 3.0, 4.0, 7.0, 7.0, 3.0, 2.0, 1.0, 3.0, 3.0, 1.1007799777285008, 0.5884395630284445, 0.5884395063713064, 0.5884394497141683, 0.5884393363998921, 0.588439279742754, 0.5884391097713397, 0.5884390531142017, 0.5884390531142017, 0.5884390531142017, 0.593115306662893, 0.588909420674554, 1.0963150686703254, 1.0926863768297281, 2.1411997492381643, 0.5740745732277746, 0.5740739819379597, 0.574073820677101, 0.5740735519090033, 0.5740734981553838, 0.5740732293872861, 0.574073121880047, 1.6167980357684302, 1.0955304808395345, 1.0963195839743665, 0.5761238762190779, 0.5760707138893538, 0.5752919852030929, 0.5750601458420214, 1.093854434462025, 0.5734084479198271, 0.5734082870681221, 0.5734080725991818, 0.5734077508957716, 0.5734076972785365, 0.5734076972785365, 0.5734075900440664, 0.5734075900440664, 0.5734075900440664, 1.6166349154183906, 1.0941463266896485, 1.094997124975455, 0.5756362976534765, 0.575500002641974, 0.5743402082305623, 0.574199784691961, 0.5738496105298357, 1.075911638594723, 1.075904194911421, 1.0757093151472001, 0.563388248835126, 0.5633880937583905, 0.5633877836049196, 0.5633876802204293, 0.5633876285281841, 0.5633874734514487, 0.5633874217592035, 1.07414231642768, 1.075908640444504, 1.5883900044087929, 1.0779697136431816, 0.5637459591715739, 1.0598853518206146, 1.0598800425170483, 1.0598793412882752, 1.0553267637434218, 0.5549989323363987, 0.5549984314587038, 0.5549983813709343, 0.5549981309320868, 0.5549978304054699, 0.554997630054392, 0.5563885173254722, 0.5558621950436288, 1.0554154390788777, 1.0532429846147262, 0.552664318033762, 0.5526641193185126, 0.5526638709244508, 0.5526638212456385, 0.5526635728515767, 0.5526634734939521, 0.5526631754210779, 0.5526631257422655, 2.065377745584444, 1.0544396478466604, 1.0554203076024882, 1.0541845968240453, 1.0575552048845018, 0.5547099921687915, 0.5536774677328681, 0.5534031413310605, 1.0308199794062782, 0.5399888105362218, 0.5399888105362218, 0.5399885262532573, 0.5399884788727632, 0.5399883841117751, 0.5399882419702927, 0.5399881472093045, 0.5399880998288105, 0.5399880998288105, 1.0095372300253176, 2.008783985299413, 1.0270658534438153, 0.5379732783452172, 0.5379720084306933, 0.5379718673290795, 0.5379714910581095, 0.5379713969903669, 0.5379713969903669, 0.5379713029226244, 0.5379713029226244, 0.5379712558887532, 1.0259800294919574, 0.5380604791425292, 0.5380363037327027, 0.5326615981823066, 0.5326614598186179, 0.5326614136973883, 0.5326612753336996, 0.5326611830912404, 0.5326611369700108, 0.5326611369700108, 0.5326611369700108, 0.5326609986063221, 0.5326609524850925, 1.0157187070415838, 1.0157101284928818, 1.4962401168197703, 1.0149968175561943, 1.0126590246712324, 0.5348562307706515, 0.5337959959450616, 0.5328930806335698, 0.5198313350412848, 0.5198309392087891, 0.5198308512460122, 0.5198307632832355, 0.5198307632832355, 0.5198307632832355, 0.5198304554135167, 0.5198303674507398, 0.5198303234693514, 0.5198301475437979, 1.465544833582606, 0.993031403029808, 0.9918302713125612, 0.5221253162975337, 0.521673451513096, 0.5207450044039389, 0.5199019691510577, 0.5198802863265757], \"Total\": [3.0, 3.0, 4.0, 7.0, 7.0, 3.0, 2.0, 1.0, 3.0, 3.0, 1.5727431192619832, 1.0386117509383657, 1.0386116942812276, 1.0386116409751667, 1.0386115301890473, 1.0386114676526752, 1.038611297681261, 1.0386112410241228, 1.0386112410241228, 1.0386112441290485, 4.518938927970513, 1.5604586434208185, 1.5477950289636018, 1.5477590498068943, 3.0945964855921875, 1.0255543608327915, 1.0255537695429766, 1.025553611823189, 1.0255533430550914, 1.0255532887216814, 1.025553016992303, 1.025552909485064, 3.634363361766889, 2.553313484179824, 7.502946398366994, 3.0040790582209898, 1.5279973448356179, 1.530103602948663, 2.0446499374834732, 1.5465212548699485, 1.0249488627263996, 1.0249486989134138, 1.0249484844444734, 1.0249481627410633, 1.0249481126648994, 1.0249481183626634, 1.024948001889358, 1.0249480054304292, 1.0249480087900305, 3.634363361766889, 2.53981537196091, 3.617757869125982, 3.0129207758016507, 1.529516750777636, 1.5467820708114672, 2.553313484179824, 1.5467909927305439, 1.528364122684627, 1.5283568150385145, 1.5281642920132135, 1.0158406717012614, 1.0158405197294516, 1.015840206471055, 1.0158401030865647, 1.0158400513943195, 1.0158398996686613, 1.015839844625339, 2.5225929252459642, 3.0361428391680425, 7.502946398366994, 7.988099018320875, 3.923373593674182, 1.513101524720406, 1.513096234854502, 1.5130955463724263, 1.5129529457218032, 1.0082150561799825, 1.0082145553022877, 1.0082145085741194, 1.0082142547756707, 1.0082139542490538, 1.0082137572575771, 1.4991167717733793, 3.098244364907915, 1.508844087214211, 1.5089246310765205, 1.0060928775129807, 1.0060926787977313, 1.0060924304036696, 1.0060923807248572, 1.0060921358718666, 1.0060920329731708, 1.006091741610975, 1.0060916852214843, 7.988099018320875, 3.0040790582209898, 3.0427869065064965, 3.4995455199229575, 7.502946398366994, 1.4903584353470274, 1.4786891939613644, 3.923373593674182, 1.4857809538493516, 0.994571031795974, 0.994571031795974, 0.9945707475130094, 0.9945707063679268, 0.9945706112507612, 0.9945704632300449, 0.9945703684690568, 0.9945703210885626, 0.994570324319328, 1.9886006382929942, 4.518938927970513, 1.4821411274588945, 0.9927389748637729, 0.992737704949249, 0.9927375638476352, 0.9927371875766652, 0.9927370935089226, 0.9927370990065961, 0.9927369994411801, 0.9927370027922573, 0.9927369524073089, 3.006946696239026, 2.4585425727055017, 1.4836241840324405, 0.9879106376338266, 0.9879104959190607, 0.9879104531489082, 0.9879103114341423, 0.9879102191916832, 0.9879101763012189, 0.9879101823007648, 0.9879101877685383, 0.9879100380663661, 0.9879099885855352, 1.9749545863432123, 2.4968332971816305, 7.988099018320875, 3.923373593674182, 7.502946398366994, 1.4604831144217743, 2.5347805306135536, 1.5226147123474523, 0.9762480426094873, 0.9762476467769916, 0.9762475588142148, 0.9762474708514381, 0.9762474708514381, 0.9762474742110394, 0.976247166112205, 0.9762470750189425, 0.9762470369167879, 0.9762468617579974, 7.502946398366994, 3.923373593674182, 7.988099018320875, 2.4747693760561864, 2.5225929252459642, 2.032274877087108, 2.4585425727055017, 1.478626932962427], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.4779, -8.1042, -8.1042, -8.1042, -8.1042, -8.1042, -8.1042, -8.1042, -8.1042, -8.1042, -8.0963, -8.1034, -7.4293, -7.4327, -6.7599, -8.0763, -8.0763, -8.0763, -8.0763, -8.0763, -8.0763, -8.0763, -7.0408, -7.4301, -7.4293, -8.0727, -8.0728, -8.0742, -8.0746, -7.429, -8.0749, -8.0749, -8.0749, -8.0749, -8.0749, -8.0749, -8.0749, -8.0749, -8.0749, -7.0384, -7.4288, -7.428, -8.071, -8.0713, -8.0733, -8.0735, -8.0741, -7.409, -7.409, -7.4092, -8.056, -8.056, -8.056, -8.056, -8.056, -8.056, -8.056, -7.4107, -7.409, -7.0195, -7.4071, -8.0553, -7.3925, -7.3925, -7.3925, -7.3968, -8.0395, -8.0395, -8.0395, -8.0395, -8.0395, -8.0395, -8.037, -8.0379, -7.3885, -7.3906, -8.0355, -8.0355, -8.0355, -8.0355, -8.0355, -8.0355, -8.0355, -8.0355, -6.7172, -7.3895, -7.3885, -7.3897, -7.3865, -8.0318, -8.0336, -8.0341, -7.3647, -8.0113, -8.0113, -8.0113, -8.0113, -8.0113, -8.0113, -8.0113, -8.0113, -8.0113, -7.3856, -6.6976, -7.361, -8.0077, -8.0077, -8.0077, -8.0077, -8.0077, -8.0077, -8.0077, -8.0077, -8.0077, -7.3621, -8.0075, -8.0076, -7.998, -7.998, -7.998, -7.998, -7.998, -7.998, -7.998, -7.998, -7.998, -7.998, -7.3526, -7.3526, -6.9652, -7.3533, -7.3556, -7.9939, -7.9959, -7.9976, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -7.9749, -6.9384, -7.3276, -7.3289, -7.9705, -7.9714, -7.9731, -7.9748, -7.9748], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8208, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 1.6094, 0.147, 1.2031, 1.8853, 1.882, 1.8619, 1.65, 1.65, 1.65, 1.65, 1.65, 1.65, 1.65, 1.4202, 1.3841, 0.3069, 0.5788, 1.2547, 1.252, 0.9617, 1.8864, 1.6519, 1.6519, 1.6519, 1.6519, 1.6519, 1.6519, 1.6519, 1.6519, 1.6519, 1.4227, 1.3906, 1.0376, 0.5776, 1.2553, 1.242, 0.7406, 1.2412, 1.9183, 1.9183, 1.9182, 1.6798, 1.6798, 1.6798, 1.6798, 1.6798, 1.6798, 1.6798, 1.4155, 1.2319, 0.7167, 0.2664, 0.3292, 1.9448, 1.9448, 1.9448, 1.9406, 1.7039, 1.7039, 1.7039, 1.7039, 1.7039, 1.7039, 1.3097, 0.5828, 1.9516, 1.9495, 1.71, 1.71, 1.71, 1.71, 1.71, 1.71, 1.71, 1.71, 0.9564, 1.2621, 1.2502, 1.1092, 0.3497, 1.3207, 1.3267, 0.3504, 1.9908, 1.7456, 1.7456, 1.7456, 1.7456, 1.7456, 1.7456, 1.7456, 1.7456, 1.7456, 1.6785, 1.5457, 1.997, 1.7511, 1.7511, 1.7511, 1.7511, 1.7511, 1.7511, 1.7511, 1.7511, 1.7511, 1.2885, 0.8444, 1.3494, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7656, 1.7184, 1.4839, 0.7083, 1.0313, 0.3806, 1.3788, 0.8255, 1.3335, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 1.8006, 0.7978, 1.0569, 0.3447, 0.8749, 0.8548, 1.0692, 0.8772, 1.3856]}, \"token.table\": {\"Topic\": [9, 4, 4, 6, 10, 3, 1, 2, 10, 7, 6, 8, 9, 2, 4, 10, 10, 2, 1, 5, 7, 3, 2, 6, 6, 1, 10, 4, 7, 2, 6, 9, 4, 10, 9, 10, 6, 9, 1, 3, 7, 9, 2, 4, 6, 9, 10, 1, 2, 6, 4, 8, 1, 3, 4, 6, 9, 6, 10, 6, 5, 6, 5, 5, 5, 7, 2, 6, 3, 8, 1, 2, 3, 4, 5, 3, 7, 1, 2, 4, 5, 6, 8, 9, 10, 1, 2, 2, 5, 3, 2, 3, 1, 8, 8, 2, 6, 9, 6, 10, 7, 8, 4, 9, 10, 9, 7, 3, 6, 7, 2, 4, 6, 7, 9, 10, 1, 3, 10, 6, 7, 8, 10, 1, 10, 9, 1, 3, 3, 2, 3, 8, 4, 9, 4, 7, 1, 9, 10, 6, 9, 2, 6, 5, 5, 2, 6, 8, 9, 8, 4, 8, 3, 2, 3, 4, 5, 6, 9, 10, 10, 1, 9, 2, 3, 5, 8, 10, 2, 2, 2, 3, 9, 3, 7, 6, 6, 5, 5, 2, 3, 6, 7, 8, 1, 2, 3, 7, 10, 1, 2, 3, 4, 6, 5, 5, 7, 2, 10, 1, 2, 9, 7, 1, 9, 8, 4, 6, 9, 10, 2, 3, 9, 6, 7, 4, 2, 4, 7, 8, 3, 5, 3, 7], \"Freq\": [1.0122377762561834, 0.984406793145073, 0.9844063423108149, 0.6762746384323197, 0.6762746384323197, 0.9756590980422432, 0.9628238960701513, 0.9750829548855695, 1.0243307583492283, 1.005459318006887, 0.9939444625368813, 1.007316134270521, 1.0122374487677575, 0.3964175075542541, 0.3964175075542541, 0.3964175075542541, 1.024330435075549, 0.9750831048434798, 0.9628241641347438, 0.9918523738235214, 1.0054589317756004, 0.9756591402865176, 0.6462878146833015, 0.32314390734165077, 0.993944511615854, 0.9628239540432386, 1.0243298386822106, 0.9844070904540885, 1.005458890180077, 0.9750837726179391, 0.993944753512473, 1.0122377645064777, 0.9844069434232507, 1.0243304386006205, 1.012237770108897, 1.02433089390799, 0.6709795283355119, 0.6709795283355119, 0.3945114726591126, 0.3945114726591126, 0.3945114726591126, 0.3945114726591126, 0.28575139094690644, 0.28575139094690644, 0.28575139094690644, 0.28575139094690644, 0.28575139094690644, 0.9628241641347438, 0.9750833603847765, 0.9939440208263455, 0.654379902230665, 1.0073154218023, 0.6358317437556201, 0.3293652680300173, 0.3293652680300173, 0.3293652680300173, 0.3293652680300173, 0.6627590010617377, 1.0243304386006205, 0.6627236240994783, 0.6608964961809974, 0.9939451430132796, 0.9918531589174907, 0.9918524197933465, 0.6670594438197437, 0.6670594438197437, 0.6544513990026469, 0.6544513990026469, 0.9756587918094287, 1.0073159467724422, 0.32276343703758226, 0.32276343703758226, 0.32276343703758226, 0.32276343703758226, 0.32276343703758226, 0.9756585876543257, 1.0054586027849841, 0.1251862298785329, 0.1251862298785329, 0.1251862298785329, 0.1251862298785329, 0.2503724597570658, 0.1251862298785329, 0.1251862298785329, 0.1251862298785329, 0.6408372334737511, 0.6408372334737511, 0.6535505164963338, 0.6535505164963338, 0.9756592477879381, 0.6464997563987, 0.6464997563987, 0.9628236914289862, 1.007315564975921, 1.007316185395415, 0.4005073150573479, 0.4005073150573479, 0.4005073150573479, 0.6763031145364716, 0.6763031145364716, 0.6740251411122417, 0.6740251411122417, 0.9844071437942258, 0.6847049377875991, 0.6847049377875991, 1.0122373035633354, 1.0054586027849841, 0.39372940688516744, 0.39372940688516744, 0.39372940688516744, 0.6460953984567331, 0.2548826860669964, 0.2548826860669964, 0.2548826860669964, 0.2548826860669964, 0.2548826860669964, 0.49205942132853403, 0.49205942132853403, 0.49205942132853403, 0.40674504118899624, 0.40674504118899624, 0.40674504118899624, 0.40674504118899624, 0.9628237439518252, 1.0243310776940462, 1.012237917895133, 0.962823793368122, 0.9756591457102805, 0.9756584317191838, 0.6465034854427706, 0.6465034854427706, 0.6746995825657188, 0.9844068933305197, 1.0122376377955882, 0.6542974717424217, 0.6730467215972896, 0.6567649661405646, 0.6567649661405646, 1.0243302540102657, 0.5063407568533415, 0.5063407568533415, 0.6460803796931669, 0.9939448551687982, 0.9918518810747495, 0.9918526694730195, 0.3328807200540848, 0.3328807200540848, 0.3328807200540848, 0.3328807200540848, 1.0073141332415436, 0.6542943433162143, 1.0073160366432077, 0.9756592511587227, 0.1332809734876486, 0.1332809734876486, 0.2665619469752972, 0.1332809734876486, 0.1332809734876486, 0.1332809734876486, 0.1332809734876486, 1.024330853929162, 0.9628241612563835, 1.0122379685944616, 0.33190384826296304, 0.33190384826296304, 0.33190384826296304, 0.33190384826296304, 0.33190384826296304, 0.9750836704013179, 0.9750834120443095, 0.3916479532168453, 0.3916479532168453, 0.3916479532168453, 0.6466125162205372, 1.0054593212730243, 0.9939442171420907, 0.9939451987219801, 0.9918529651226938, 0.660894185659341, 0.32864608358267394, 0.32864608358267394, 0.32864608358267394, 0.32864608358267394, 1.0073160422216179, 0.22129088618798992, 0.22129088618798992, 0.22129088618798992, 0.44258177237597984, 0.22129088618798992, 0.27641429752223606, 0.27641429752223606, 0.27641429752223606, 0.27641429752223606, 0.27641429752223606, 0.660959088534586, 0.6608967968992122, 1.0054592733737895, 0.975082392695361, 1.024330346305435, 0.4890812758054741, 0.4890812758054741, 0.4890812758054741, 1.005459177575334, 0.9628241116118589, 1.012237492591112, 1.0073161376708113, 0.40407805659596796, 0.40407805659596796, 0.40407805659596796, 0.40407805659596796, 0.5503027080450416, 0.5503027080450416, 1.01223773230953, 0.5028661767193213, 0.5028661767193213, 0.9844064895800078, 0.3325632613477192, 0.3325632613477192, 0.3325632613477192, 0.3325632613477192, 0.653801273828208, 0.653801273828208, 0.9756592445898968, 1.0054590279340858], \"Term\": [\"advice greatly appreciated\", \"analyst basic\", \"anecdotal confirmation bias\", \"applicant\", \"applicant\", \"awareness recent relevant\", \"back time relearn\", \"background natural fit\", \"based response solution\", \"beating dead\", \"case copy guy\", \"clientfacing people find\", \"colleague overuse oop\", \"company\", \"company\", \"company\", \"company burnt wrestling\", \"company joined good\", \"complaint people\", \"computer vision deep\", \"confluence writing code\", \"create path world\", \"curious\", \"curious\", \"cybersecurity yoe\", \"datasets server power\", \"decision simply step\", \"deck day long\", \"disorganized future applicant\", \"distinct misfortune working\", \"domain quick blurb\", \"dumbest thing guy\", \"edit nearing\", \"education completely\", \"effectively guide open\", \"emotionless uncaring blunt\", \"expectation\", \"expectation\", \"experience\", \"experience\", \"experience\", \"experience\", \"field\", \"field\", \"field\", \"field\", \"field\", \"financial software\", \"find fending hostile\", \"forward exciting insight\", \"friend\", \"fulltime benefit scientist\", \"give\", \"good\", \"good\", \"good\", \"good\", \"gpt\", \"gpt started smoking\", \"graph\", \"great question\", \"hoggers extremely high\", \"honestly amount gate\", \"hope challenging interesting\", \"hyper parameter tuning\", \"hyper parameter tuning\", \"ide\", \"ide\", \"including amazon msft\", \"integrate machine learning\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interviewing senior role\", \"involve quantitative analysis\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job find\", \"job find\", \"job posting require\", \"job posting require\", \"job tailor portfolio\", \"junior mid level\", \"junior mid level\", \"key aspect lot\", \"language solve\", \"large work load\", \"learning\", \"learning\", \"learning\", \"learning engineer\", \"learning engineer\", \"learning engineer feel\", \"learning engineer feel\", \"learning make beautiful\", \"level\", \"level\", \"level massively fail\", \"local paper thought\", \"lot\", \"lot\", \"lot\", \"low hanging fruit\", \"machine learning\", \"machine learning\", \"machine learning\", \"machine learning\", \"machine learning\", \"machine learning algorithm\", \"machine learning algorithm\", \"machine learning algorithm\", \"machine learning engineer\", \"machine learning engineer\", \"machine learning engineer\", \"machine learning engineer\", \"make final modeling\", \"making reached find\", \"management team company\", \"mcol midwest type\", \"mention medtech startup\", \"microsoft scientist excited\", \"month\", \"month\", \"month ago\", \"move dedicated statistic\", \"november getting lot\", \"offered\", \"open\", \"past month subreddit\", \"past month subreddit\", \"polars ingest kaggle\", \"position\", \"position\", \"presentation\", \"project recommender search\", \"provide context working\", \"provide presentation odd\", \"python\", \"python\", \"python\", \"python\", \"quit hate community\", \"research\", \"risk taking edit\", \"role online landed\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist\", \"scientist paper past\", \"searching job stressfuldon\", \"shiny powerpoint deck\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"solution running poc\", \"stand case urgent\", \"start\", \"start\", \"start\", \"startup\", \"stats analytics background\", \"struggling dax creating\", \"super fresh neurocomputing\", \"talent acquisition girl\", \"test\", \"thing\", \"thing\", \"thing\", \"thing\", \"thinking lot job\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"time\", \"time\", \"time\", \"time\", \"time\", \"title\", \"train\", \"trapped indoors\", \"trench scientist engineer\", \"turn notebook complete\", \"understand\", \"understand\", \"understand\", \"university entered program\", \"week major finance\", \"weekly report\", \"wellbeing due constant\", \"work\", \"work\", \"work\", \"work\", \"working\", \"working\", \"world hating\", \"worried\", \"worried\", \"wrong thought building\", \"year\", \"year\", \"year\", \"year\", \"year work experience\", \"year work experience\", \"year worked year\", \"youtube video\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 7, 1, 2, 9, 5, 3, 4, 8, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el246830528326178889856295555\", ldavis_el246830528326178889856295555_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el246830528326178889856295555\", ldavis_el246830528326178889856295555_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el246830528326178889856295555\", ldavis_el246830528326178889856295555_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "5      0.154548  0.093827       1        1  11.331341\n",
       "6      0.045216  0.160541       2        1  10.750642\n",
       "0     -0.131880 -0.109268       3        1  10.723365\n",
       "1     -0.095949  0.152246       4        1  10.338370\n",
       "8      0.059827 -0.171605       5        1  10.017478\n",
       "4     -0.063168  0.060939       6        1   9.935687\n",
       "2     -0.026494 -0.119919       7        1   9.476027\n",
       "3     -0.167331  0.001348       8        1   9.406703\n",
       "7      0.067369 -0.021726       9        1   9.224176\n",
       "9      0.157861 -0.046384      10        1   8.796211, topic_info=                            Term      Freq     Total Category  logprob  \\\n",
       "969                      working  3.000000  3.000000  Default  10.0000   \n",
       "4631                     curious  3.000000  3.000000  Default   9.0000   \n",
       "2372                     thought  4.000000  4.000000  Default   8.0000   \n",
       "1043                   scientist  7.000000  7.000000  Default   7.0000   \n",
       "810                          job  7.000000  7.000000  Default   6.0000   \n",
       "...                          ...       ...       ...      ...      ...   \n",
       "2317                        work  0.522125  2.474769  Topic10  -7.9705   \n",
       "7508                     company  0.521673  2.522593  Topic10  -7.9714   \n",
       "1023  machine learning algorithm  0.520745  2.032275  Topic10  -7.9731   \n",
       "3614   machine learning engineer  0.519902  2.458543  Topic10  -7.9748   \n",
       "3611           learning engineer  0.519880  1.478627  Topic10  -7.9748   \n",
       "\n",
       "      loglift  \n",
       "969   10.0000  \n",
       "4631   9.0000  \n",
       "2372   8.0000  \n",
       "1043   7.0000  \n",
       "810    6.0000  \n",
       "...       ...  \n",
       "2317   0.8749  \n",
       "7508   0.8548  \n",
       "1023   1.0692  \n",
       "3614   0.8772  \n",
       "3611   1.3856  \n",
       "\n",
       "[163 rows x 6 columns], token_table=       Topic      Freq                         Term\n",
       "term                                               \n",
       "12642      9  1.012238   advice greatly appreciated\n",
       "14821      4  0.984407                analyst basic\n",
       "7446       4  0.984406  anecdotal confirmation bias\n",
       "2434       6  0.676275                    applicant\n",
       "2434      10  0.676275                    applicant\n",
       "...      ...       ...                          ...\n",
       "1863       8  0.332563                         year\n",
       "1405       3  0.653801         year work experience\n",
       "1405       5  0.653801         year work experience\n",
       "14742      3  0.975659             year worked year\n",
       "10411      7  1.005459                youtube video\n",
       "\n",
       "[224 rows x 3 columns], R=10, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 7, 1, 2, 9, 5, 3, 4, 8, 10])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds = 'mmds', R=10)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
