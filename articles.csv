link,title,text
https://www.theguardian.com/world/2024/apr/11/idf-colonel-discusses-data-science-magic-powder-for-locating-terrorists,IDF colonel discusses ‘data science magic powder’ for locating terrorists,"A video has surfaced of a senior official at Israel’s cyber intelligence agency, Unit 8200, talking last year about the use of machine learning “magic powder” to help identify Hamas targets in Gaza.

The footage raises questions about the accuracy of a recent statement about use of artificial intelligence (AI) by the Israeli Defense Forces (IDF), which said it “does not use an artificial intelligence system that identifies terrorist operatives or tries to predict whether a person is a terrorist”.

However, in the video, the head of data science and AI at Unit 8200 – named only as “Colonel Yoav” – said he would reveal an “example of one of the tools we use” before describing how the intelligence division used machine learning techniques in Israel’s May 2021 offensive in Gaza for “finding new terrorists”.

“Let’s say we have some terrorists that form a group and we know only some of them,” he said. “By practising our data science magic powder we are able to find the rest of them.”

The descriptions in the video of technology used by Unit 8200 bear similarities with recent testimony from six IDF insiders about their use of an AI tool called “Lavender” during its offensive on Hamas. They said that the AI-generated database had been used to assist intelligence officers involved in the bombing campaign in Gaza, helping identify tens of thousands of potential human targets.

In its rebuttal, the IDF said some of the accounts were “baseless”. However, the accounts are consistent with the remarks by Yoav during an AI conference at Tel Aviv university in February last year. The video, in which Yoav can be heard talking but not seen, was hosted on the university’s YouTube channel, and until recently it had fewer than 100 views.

As he took to the stage wearing a military uniform, the audience was instructed not to take any photos of Yoav or record his presentation. “It’s good because it’s a bad hair day for me,” he joked.

In the 10-minute presentation – titled “digital transformation and artificial intelligence in the intelligence domain” – the colonel offered a rare insight into how opaque AI systems are being used by secretive military and intelligence bodies.

When using AI to predict whether someone is a terrorist, he explained, Unit 8200 takes information it has about people it believes are members of terrorist groups and aims “to find the rest of the group”.

Referring to a specific example, the official said that in the IDF’s May 2021 military operation in Gaza, his department applied this principle to “find Hamas squad missile commanders and anti-tank missile terrorists in Gaza in order to operate against them”.

He explained that using a form of machine learning – known as “positive unlabelled learning” – “we take the original sub-group, we calculate their close circles, we then calculate relevant features, and at last we rank the results and determine the threshold.”

The colonel said intelligence officers’ feedback is used “to enrich and improve our algorithm” and stressed that “people of flesh and blood” make decisions. “Ethically speaking we put a lot of emphasis on this,” he said, adding that “these tools are meant to help break their barriers”.

According to Yoav, Unit 8200 was able to break “the human barrier” during the May 2021 offensive when it managed to produce more than 200 new targets. “There were times when this amount took us almost a year,” he said.

Contacted for comment about the video, the IDF said the colonel’s participation in the conference had been approved by the military. However, a spokesperson denied that his remarks conflict with the IDF’s recent denials about its use of AI. In a subtle change of wording not used in its original statement, the IDF told the Guardian its AI systems do not “choose targets” for attack.

“The IDF never denied the existence of a database of operatives in terrorist organisations, that cross-checks existing information on such operatives,” it said. “At the same time, the IDF fully stands by its statement that it does not use AI systems that choose targets for attack, and that the database in question is not a list of operatives eligible to attack. There is no contradiction.”

In their testimony, the six intelligence officers who spoke out last week said that the Lavender tool had been used to help identify potential targets at an unprecedented scale and pace. The IDF was not accused of using systems that automatically select targets for attack.

Four of the sources said that, at one stage early in the war, Lavender listed as many as 37,000 men in the enclave who had been linked by the AI system to Hamas or Palestinian Islamic Jihad. None of the sources denied that humans were involved in the process of authorising strikes, although some confessed to minimal human oversight.

“I would invest 20 seconds for each target at this stage, and do dozens of them every day,” one intelligence officer said. “I had zero added-value as a human, apart from being a stamp of approval. It saved a lot of time.”

Their accounts were published by the Israeli-Palestinian publication +972 magazine, its Hebrew-language outlet Local Call and the Guardian.

Col Yoav’s description of the partnership between AI and intelligence personnel echoes a model for targeting operations envisioned by his commander, the Unit 8200 chief Yossi Sariel, who the Guardian revealed left his identity exposed online when he secretly authored a book published in 2021, The Human Machine Team.

At one stage, the colonel explained that the AI and data science department he runs at Unit 8200 is also known internally as “the human machine integration centre”.

Speaking eight months before the IDF commenced operations in Gaza after the Hamas-led 7 October attacks, the colonel spoke optimistically about how the IDF is moving “from the postcard age to the digital era” where “suddenly you can react during battle with applied data-science driven solutions”.

Looking ahead, he added: “I’m curious with respect to how will the next operation look like, digitally speaking”."
https://towardsdatascience.com/one-mindset-shift-that-will-make-you-a-better-data-scientist-a015f8000ad7,One Mindset Shift That Will Make You a Better Data Scientist,"After transitioning out of quant finance, my first experience as a data scientist was in consulting. Most of the feedback I received in my early days at McKinsey was not related to my code or technical skills, but rather consisted of advice like “you need to tie your work…"
https://www.insurancebusinessmag.com/uk/news/claims/ageas-uk-names-new-head-of-claims-data-and-analytics-485477.aspx,Ageas UK names new head of claims data and analytics,"Ageas UK has named Philippa King (pictured above) as its new head of claims data and analytics.

King transitions to the role after a seven-year tenure in the company’s actuarial department, where she was primarily focused on Household Reserving and Analysis.

Additionally, King worked as an actuarial analyst at Lane Clark & Peacock from October 2014 to February 2017, dealing with clients across various sectors including the London Market, personal lines, MDOs, and reinsurers.

Her academic credentials include a PhD from the University of Bristol, where her research involved developing microscopy solutions to study the interaction between nanoparticles and human cells, incorporating extensive coding for data analysis and collection automation.

King’s background includes significant contributions to the Institute and Faculty of Actuaries, where she served on the General Insurance Board from September 2018 to March 2020. In this capacity, she focused on representing and advocating for early career members, promoting their involvement with the Institute.

She also chaired the Flood Working Party from August 2016 to October 2018, collaborating with Flood Re and leading research on insurance issues related to flooding.

In her new position, King will report directly to Stephen Linklater, claims director at Ageas. The role is designed to spearhead the company’s initiatives in data and analytics, incorporating advanced data science techniques and artificial intelligence to refine decision-making processes.

“I’m delighted to welcome Philippa to our team,” Linklater said. “Her depth of knowledge and experience in data science and analytics will be invaluable as we build on our solid foundations, ensuring that our Claims Data Centre of Excellence delivers our ambition to lead the market in data-driven decision making.”

“I am excited to be taking on this new position at Ageas, to further build on our strategic advantage by finding additional ways to leverage the wealth of data that exists within the business,” King said."
https://www.insurancetimes.co.uk/news/ageas-uk-appoints-head-of-claims-data-and-analytics/1451649.article,Ageas UK appoints head of claims data and analytics,"”I am excited to be taking on this new position at Ageas, to further build on our strategic advantage by finding additional ways to leverage the wealth of data that exists within the business.”

Read more…

Explore more people move-related stories here or discover other news content here

We’re glad you’ve chosen Insurance Times as your source for industry news and hope you’ve been enjoying reading articles from our award-winning team of journalists.

Gain access to more of our exclusive, breaking stories, interviews and news analysis as it happens. Registering is quick, easy, free, and will also have the additional benefits:

Top level access to our Five Star service ratings across our annual Personal Lines, Commercial Lines, eTrading and MGA reports.

A choice of daily and weekly email newsletters that suit you, to keep informed of news across the industry directly to your inbox.

As a subscriber you will benefit from unlimited access to our news and news analysis, magazine editions, special supplements, exclusive research reports and full access to the Five Star service rating microsites - view subscription options."
https://jpt.spe.org/uk-offshore-energy-data-and-digital-maturity-survey-reveals-latest-trends,UK Offshore Energy Data and Digital Maturity Survey Reveals Latest Trends,"In 2023, Deloitte supported the Offshore Energy Digital Strategy Group (OEDSG) in developing a new survey, exploring the maturity in the UK Offshore Energy sector, building on the findings of a baseline study conducted in 2020. The 2023 survey reflects the growing energy mix in the region, with insights from renewable providers as well as oil and gas and supply chain partners.

More than 30 oil, gas and renewable operator companies contributed to the 2023 Offshore Energy Digital and Data Maturity Survey with input also provided by technology developers and supply chain companies supporting diverse offshore energies.

The survey aimed to identify changes since the 2020 survey and identify further insights to support OEDSG digital and data initiatives. The OEDSG comprises of Offshore Energies UK (OEUK), Opportunity North East (ONE), Net Zero Technology Centre (NZTC), Technology Leadership Board (TLB), North Sea Transition Authority (NSTA), The Crown Estate and Crown Estate Scotland.

“Data and digital technologies play a key role in driving greater collaboration and efficiency across the UK energy sector,” said Daniel Brown, OEUK’s head of data and digital. “This survey provides invaluable insight of how and where we can work together to shape and accelerate our digital efforts in support of delivering net zero.”

Findings from 2023 data reveal progress in the pace and integration of digitalization with organizations recording an 8% improvement across key metrics relating to strategy, leadership, training, and capabilities. In comparison, the survey identifies a lag in data maturity, which suggests more companies need to focus attention on developing data strategies if they are to capitalize on the opportunities ahead. There are, however, signs that companies are increasingly adopting a wider range of digital technology and data indicating this situation will improve in the future.

Feedback highlighted how increasing collaboration and coordination could enable organizations to unlock the full potential of data and digital technologies, enabling them to play a key role in accelerating the shift to cleaner energy production. It cites evidence of progress with organizations adopting data-related technologies including cloud platforms, data mesh, and data visualization tools that are enabling greater levels of collaboration.

Read the full story here."
https://www.wtwco.com/en-hk/insights/2024/04/overcoming-m-and-a-talent-risks-with-data-science,Overcoming M&A talent risks with data science,"Global disruptions, economic uncertainty and the high cost of capital weighed down M&A activity for most of 2023. However, 2024 has seen renewed activity, especially in the U.S., buoyed by a stronger-than-expected economy and employment rate.

As before, M&A success will continue to require thorough due diligence, thoughtful integration and deal value capture. However, traditional approaches to talent retention – if only to avoid disruption to a target’s operations – are under pressure in today’s still cost-conscious business environment.

For example, acquirers historically have used cash bonuses to retain a target’s key leadership members and employees. Typically, these time-bound (vs. performance-based) retention bonuses are awarded over a period of at least one year and as a percentage of base salary. They also can be materially significant (2% of deal value on average) and even cause unexpected harm (e.g., disenfranchising employees who don’t receive one).

However, as organizations keep a close eye on expenses and seek alternatives to cash, HR and compensation professionals are moving up the maturity curve with artificial intelligence (AI).

Redefining your approach to retention

Several key learnings about the state of retention agreements emerged in our 2024 M&A Retention Study:

94% of acquirers cited key skills and critical industry/market knowledge as driving factors for talent retention

Four in five acquirers said they use cash for retention, with payment value as high as 50% of base salary for senior leadership and 30% for salaried employees

More than half of retention agreements for senior leaders are time-based

With this in mind, we can harness the power of data science to inform retention agreement decision making. Experienced HR professionals are working across different domains of AI and machine learning to retain the key talent required to move the combined organization forward.

Skill scans: Know what you’re paying for

Skill scans use machine learning to collect vast amounts of data on the prevalent and emerging skills in both the external market and the internal organization. Prevalent skills are those skills most historically required for a job or work area as listed in job descriptions or postings.

Emerging skills are the new skills cropping up within these same sources. Collecting and comparing these digital data efficiently during M&A stages can help an acquirer focus recruiting efforts, become more strategic in the use of cash retention bonuses where needed, and better direct training and personalized career management efforts during integration.

Talent flows: Follow the boomerangs

A talent flow analysis uncovers the places where organizations are gaining talent as well as to where they are losing it. Here again, HR-led AI methods use web crawling to collect market data on talent movement based on a range of sources, such as hiring or new role announcements and job postings across organizations.

When the acquiring company understands these talent flows, they can identify back-up talent with relevant institutional knowledge and skills. Acquiring organizations also gain important insights into the pace of talent movement, and what the competition for talent is like beyond their typical industry and revenue competitors.

Sourcing top talent: Know where to look

Using external labor market data and job postings, machine learning can help an acquirer explore talent options. For example, for one energy company a review of local labor markets determined that the target organization was the main employer in the region. In fact, they were consuming all available talent in that region! In this case, sourcing alternative back-up talent meant the acquirer had to go to a different U.S. state. The good news was that key talent was available in these other states and at a better cost.

With this kind of data in-hand, acquirers are in a better negotiation position with individual offer letters or can choose to abstain from committing to lucrative retention payouts.

Alternatives to pay: Raising sightlines to careers

It’s important for organizations to recognize that pay is not the only lever when it comes to retention. WTW’s December 2023 Global Salary Budget Planning Report found that 96% of organizations increased their compensation spend – the most seen in nearly two decades – due to employee retention concerns. When you add in M&A activity, these increases may climb, as more than one-third of acquirers are using base pay increases to retain leaders and salaried talent.

However, organizations can look beyond pay to actions such as enhancing career opportunities and professional networks. A talent marketplace, where new work assignments and skill building opportunities are transparently listed on a tech-enabled platform, can help activate careers for merging talent.

HR’s use of a work architecture – the building blocks for how work and skills are managed – also identifies new career paths and skill-based networks. A work architecture that defines key skills for work and job levels can help acquirers identify skill-based pay premiums versus general pay retention agreements.

Employee experience: Intentionally enhance it

Tracking how employees experience an integration and how it affects their engagement, productivity and presenteeism are critical to being resilient through the process. Cultural misalignment is the most cited reason employees leave during an integration. Capturing and linking data on engagement, wellbeing, careers and performance can identify areas for change management, communications and even new total rewards and wellbeing initiatives in demand through the M&A implementation.

Emerging job and title analysis: Add a spark

Organizations can mitigate other talent risks by collecting data on emerging jobs and titles. This HR-led data science method uses public data acquisition through web scraping over time, comparing the jobs and titles of today versus those most common over the last few years, to identify what’s emerging.

This data can be used to develop new career propositions. For example, one company used new emerging titles and job descriptions as a key retention tool through integration to the final state. In another instance, a merging company’s leader positions were below the management level of the acquirers. To effectively transition these leaders in the new career framework, new job titles and accountabilities were used based on identified emerging work and market skills.

This review effectively reset the role for incoming leaders and created a new focus for their careers. This helped ease the path from the former organization and established a future-oriented mindset.

Leverage new capabilities for success

A successful M&A process has an early and intense focus on people. Data science brings new capabilities to this process, effectively addressing the people factors and transaction goals while providing cost-effective alternatives to finding and retaining talent."
https://www.cdotrends.com/story/3942/oracle-train-10000-ai-data-science,"Oracle To Train 10,000 in AI, Data Science","Oracle has announced a collaboration with the Singapore Government to train up to 10,000 students and professionals in the latest digital technologies including AI, cloud, cybersecurity, and data science by 2027.

The announcement was made at the CloudWorld Tour Singapore conference on Wednesday, which was attended by Oracle chief executive officer Safra Catz.

AI and data science

Specifically, Oracle will offer up to 300 internships and apprenticeships over three years to students and graduates across five polytechnics. This includes AI, generative AI, cloud computing, cybersecurity, data management, and autonomous databases.

Oracle will also provide training and certifications for up to 9,700 learners in Singapore over the next three years. They can obtain a new “Gen AI Professional Certification”, which Oracle says was designed to empower digital professionals in Singapore with advanced expertise and skills in AI technologies.

“When we launched the refreshed National AI Strategy (NAIS 2.0) in December last year, we set ambitious goals, one of which involves tripling our AI practitioner pool to 15,000 over the next five years,” said Minister Josephine Teo, who helms the Communications and Information, Smart Nation, and Cybersecurity portfolios.

“Although it has only been four months, Singapore is already making good progress towards realizing our vision of cultivating a skilled tech workforce capable of driving innovation and meeting industry demands. I am delighted to welcome like-minded partners such as Oracle to join us on our AI journey,” she said.

Billed as a whole-of-government and whole-of-economy approach, the National AI Strategy 2.0 (NAIS 2.0) builds on the country’s first AI strategy launched in 2019. We reported on it in December last year.

Garrett Ilg, executive vice president and general manager of Oracle for Japan and Asia Pacific pointed to Oracle’s decades of data expertise and “full stack cloud-based AI” as an enabler for digital skills development.

“Our TIP Alliance collaboration and Oracle University platform strongly align with Singapore’s Smart Nation initiative. By enhancing the digital literacy of the workforce, we aim to bolster local employment opportunities and equip individuals to navigate the complexities of the digital economy effectively,” he said."
https://www.abdn.ac.uk/achds/events/20574/,"HDR UK Seminar Series, Using wearables and mobile health devices in healthcare and research","External event

In this talk, Dr Peter H. Charlton (University of Cambridge) will provide an introduction to wearable devices, including the underlying technology, their functionality, and potential applications. He will then illustrate potential opportunities with examples from our research into screening for atrial fibrillation, a common heart arrhythmia which increases the risk of stroke by five-fold, yet is often not recognised. To do so, he will present early findings from two ongoing research programmes: a study of the acceptability and performance of wearables for atrial fibrillation screening in older adults; and the SAFER Trial which uses handheld ECG recorders in the general population. Finally, Dr Charlton will discuss the next steps to realise the full potential of wearables and mobile health devices in healthcare and research, ranging from their use in virtual wards to large-scale, low-cost clinical studies.

Speaker

Dr Peter H Charlton, University of Cambridge

Contact

To join via Zoom please click here:

https://sanger.zoom.us/j/91597738053?pwd=S3hnbTZia1c4YmVOTmIvZjVKU002Zz09.

Add to Calendar"
https://www.kdnuggets.com/5-free-courses-to-master-math-for-data-science,5 Free Courses to Master Math for Data Science,"Image by storyset on Freepik

When you’re learning data science, building a good foundation in math will make your learning journey easier and much more effective. Even if you’ve already landed your first data role, learning math fundamentals for data science will only take your skills further.

From exploratory data analysis to building machine learning models, having a good foundation in math topics like linear algebra and statistics will give you a better understanding of why you do what you do. So even if you are a beginner, this list of courses will help you learn:

Basic math skills

Calculus

Linear Algebra

Probability and Statistics

Optimization

Sounds interesting, yes? Let’s get started!

1. Data Science Math Skills – Duke University

Data science courses require you to be comfortable with math as a prerequisite. To be specific, most courses assume that you're comfortable with high school algebra and calculus. But no worries if you are not there yet.

The Data Science Math Skills course, offered by Duke University on Coursera will help you get up and running with math fundamentals in as little time as possible. The topics covered in this course include:

Problem solving

Functions and graphs

Intro to calculus

Intro to probability

It’s recommended that you go through this course before you start the other courses that explore specific math topics in greater depth.

Link: Data Science Math Skills – Duke University on Coursera

2. Calculus – 3Blue1Brown

When we talk about math for data science, calculus is definitely something you should be comfortable with. But most learners find high school calculus intimidating (I’ve been there, too!). This, however, is partly because of how we learn—mostly focusing on concepts, a small number of illustrative examples, and a ton of practice exercises.

But you’ll understand and learn calculus much better if there are helpful visualizations—to help go from intuition to equation—focusing on the why.

The Calculus course by Grant Sanderson of 3Blue1Brown is exactly what all of us need! Through a series of lessons with super helpful visualizations—going from geometry to formula wherever possible—this course will help you learn the following and more:

Limits and derivatives

Power rule, chain rule, product rule

Implicit differentiation

Higher order derivatives

Taylor series

Integration

Link: Calculus - 3Blue1Brown

3. Linear Algebra – 3Blue1Brown

As a data scientist, the datasets that you work are essentially matrices of dimensions num_samples x num_features. You can, therefore, think of each data point as a vector in the feature space. So understanding how matrices work, common operations on matrices, matrix decomposition techniques are all important.

If you loved the calculus course from 3Blue1Brown, you’ll probably enjoy the linear algebra course from Grant Sanderson just as much if not more. The Linear Algebra course from 3Blue1Brown will help you learn help you learn the following:

Fundamentals of vectors and vector spaces

Linear combinations, span, and basis

Linear transformation and matrices

Matrix multiplication

3D linear transformation

Determinant

Inverses, column space, and null space

Dot and cross products

Eigenvalues and eigenvectors

Abstract vector spaces

Link: Linear Algebra - 3Blue1Brown

4. Probability and Statistics – Khan Academy

Statistics and probability are great skills to add to your data science toolbox. But they are by no means easy to master. However, it’s relatively easier to get your fundamentals down and build on them.

The Statistics and Probability course from Khan Academy will help you learn the probability and statistics you need to start working with data more effectively. Here is an overview of the topics covered:

Analyzing categorical and quantitative data

Modeling data distributions

Probability

Counting, permutations, and combinations

Random variables

Sampling distribution

Confidence interval

Hypothesis testing

Chi-square test

ANOVA

If you’re interested in diving deep into statistics, also check out 5 Free Courses to Master Statistics for Data Science.

Link: Statistics and Probability - Khan Academy

5. Optimization for Machine Learning – ML Mastery

If you’ve ever trained a machine learning model, you know that the algorithm learns the optimal values of the parameters of the model. Under the hood, it runs an optimization algorithm to find the optimal value.

The Optimization for Machine Learning Crash Course from Machine Learning Mastery is a comprehensive resource to learn optimization for machine learning.

This course takes a code-first approach using Python. So after understanding the importance of optimization, you’ll write Python code to see popular optimization algorithms in action. Here’s an overview of the topics covered:

The need for optimization

Grid search

Optimization algorithms in SciPy

BFGS algorithm

Hill climbing algorithm

Simulated annealing

Gradient descent

Link: Optimization for Machine Learning Crash Course - MachineLearningMastery.com

Wrapping Up

I hope you found these resources helpful. Because most of these courses are tailored towards beginners, you should be able to pick up all the essential math without feeling overwhelmed.

If you’re looking for courses to learn Python for data science, read 5 Free Courses to Master Python for Data Science.

Happy learning!"
https://hackernoon.com/exploring-the-intersection-of-data-science-and-cyber-security-insights-and-applications,Exploring the Intersection of Data Science and Cyber Security: Insights and Applications,Pritesh is a Tech enthusiast and stays up-to-date with emerging technologies in the field.
https://towardsdatascience.com/why-human-centred-approaches-lead-to-better-algorithm-design-44f9ff8cf352,Why Human-Centred Approaches Lead to Better Algorithm Design,"Algorithms often evoke fearful thoughts of cold, hard mathematical formulas beyond the minds of many.

This is the approach taught in many computer science courses and textbooks — it was what I learned when I was a Comp Sci student in…"
https://www.kdnuggets.com/5-free-resources-to-master-your-data-science-job-search,5 Free Resources to Master Your Data Science Job Search,"Image by Author

Being on the job hunt is tough, there’s no two ways about it. Sending out resumes, re-writing cover letters, the interminable wait between applying and waiting to hear back (or just getting ghosted) – it’s not fun.

The good news is it’s a lot easier than it used to be. You don’t have to physically mail or drop off letters anymore; you can do a lot of applications with a few button clicks. There are plenty of specialized job boards, interview prep tools, and additional resources to make it more likely to find, apply, and actually get your dream data science job.

Let’s talk about the best free resources at your fingertips to get that data science job.

Kaggle – Real World Skills

It doesn’t matter how shiny your resume is if you don’t have the skills to back up your credentials. One of the best ways to get data science skills is by doing your own projects.

It’s sometimes tricky to get ideas for data science projects, which is where Kaggle comes in. Kaggle hosts a huge log of datasets, machine learning competitions, and includes answers and different approaches for how to tackle various projects.

Source: https://www.kaggle.com/datasets

It's an excellent resource because it allows you to apply your data science skills in practical scenarios, receive feedback, and learn from the solutions of others. Not only that, but if you actually win a Kaggle competition, that can serve as a bit of a flex to any employers. Most data scientists know of Kaggle and will be suitably impressed that you can tackle those problems.

In short, the most valuable asset Kaggle provides is real-world data and real-world problems. It offers valuable exposure to industry-level problems – and the opportunity to be noticed by top companies.

StrataScratch – Interview Prep

I may be slightly biased here as the founder of StrataScratch, but I founded the company because I noticed a real problem: it’s hard to prep for data science interviews. So I started collecting interview questions from as many different companies as I could and categorizing them by difficulty, type of question, and company. The result is a database of over a thousand real-life interview questions – both coding and non-coding – plus the solutions if you’re really stumped.

In my experience interviewing for data science jobs, it’s not just about having the skills, it’s also about being able to stay calm and think through whatever they throw your way. As you might imagine, it’s a lot easier to do that if you’ve seen the interview question – or some variation of it – before.

It’s a good idea to practice interview questions at every stage in your data science job hunt, too, not just when you have an interview lined up. Practicing IRL interview questions gives you a sense of what problems data science companies are interested in solving, as well as the skills you should focus on learning or honing.

edX and Coursera (In Audit Mode) – Gain Knowledge

Fun fact: while edX and Coursera have very expensive data science courses, you can get all the same knowledge absolutely for free simply by auditing the courses. Now, this means you don’t get a certificate of your accomplishments, which can definitely be valuable, but you do get world-class lessons, tutorials, and guides for free.

Source: https://www.edx.org/verified-certificate

Just find the course with the information you’re interested in, and sign up under audit mode. You can use this to shore up weak points on your resume, learn skills to do projects for your portfolio, or just explore a topic you’re passionate about.

KDNuggets and Towards Data Science – Read Data Science Blogs

You’re reading this on KDNuggets, so you should already know it’s a useful resource to get a data science job. KDNuggets doesn’t just offer blog posts, though. There are datasets (again, useful for projects), live and virtual events (great for networking), programming cheat sheets, and curated tool recommendations.

I’m throwing in Towards Data Science, too, since it’s another blog packed with tutorials, guides, how-tos, personal stories and experiences, and more. While some stories are paywalled, many are left free. You can easily browse the TDS homepage and look for free stories that don’t have a little star next to the author's name.

In short, one of the best ways to get a data science job is to learn from other data scientists. Many of them are kind enough to post content online for free for you to read and enjoy.

Wellfound – Job Board

Not sure where to start your data science job search? Classic contenders like LinkedIn and Indeed definitely win in terms of volume, but I love Wellfound to find data science jobs for the curated aspect.

Wellfound has a few advantages over other job boards. One, the filtering options are powerful. You can easily find jobs based on investment round, salary, equity, markets, company size, and more.

Two, it’s primarily startups. If you’ve tried and failed to get a FAANG job, it might be time to turn your sights to a different scene. Startups are hungry for data science talent, and if you can broaden your horizons to consider a slightly less conventional employer, you might have better luck.

Three, it’s just a bit newer and fresher, so I find it to be a better job-hunting experience. Features include telling you who invested in the company, how recently the recruiter was reviewing applicants, and pulling in stats from Glassdoor about leadership and life/work balance ratings.

Source: https://wellfound.com/

Final Thoughts

Job hunting is never fun, and it feels like this year has been worse in terms of companies ghosting more, making you sit through multiple rounds of interviews only to say the position was actually filled internally, or just straight up posting non-existent jobs to make themselves look better in front of potential investors. Perhaps you’ve even run into a scam job posting.

Hopefully, this list of free resources makes your life a little easier. With these five free tools, you’ll be better equipped to find and get your ideal data science job."
https://towardsdatascience.com/towards-reliable-synthetic-control-156106a1a7cb,Towards Reliable Synthetic Control,"Introduction

In recent years, the Synthetic Control (SC) approach has gained increasing adoption in industry for measuring the the Average Treatment Effect (ATE) of interventions when Randomized Control Trials (RCTs) are not available. One such example is measuring the financial impact of outdoor advertisements on billboards whereby we cannot conduct random treatment assignment in practice.

The basic idea of SC is to estimate ATE by comparing the treatment group against the predicted counterfactual. However, applying SC in practice is usually challenged by the limited knowledge of its validity due to the absence of the true counterfactual in the real world. To mitigate the concern, in this article, I would like to discuss the actionable best practices that help to maximise the reliability of the SC estimation.

The insights and conclusions are obtained through experiments based on diverse synthetic data. The code for data generation, causal inference modeling, and analysis is available in the Jupyter notebook hosted on Github.

Synthetic Control in a Nutshell

The key to measure the ATE of such events is to identify the counterfactual of the treatment group, which is the treatment group in the absence of the treatment, and quantify the post-treatment difference between the two. It is simple for RCTs as the randomised control statistically approximates the counterfactual. However, it’s challenging otherwise due to the unequal pre-experiment statistics between the treatment and control.

As a causal inference technique, SC represents the counterfactual by a synthetic control group created based on some untreated control units. This synthetic control group statistically equals the treatment group pre treatment and is expected to approximate the untreated behaviour of the treatment group post treatment. Mathematically presented below, it is created using the function f whose parameters are obtained by minimising the pre-treatment difference between the treated group and the control synthesised by f [1]:

In practice, the popular options for the function f include but are not limited to the weighted sum [1], Bayesian Structural Time Series (BSTS) [2], etc.

Actions towards Reliable Synthetic Control

Despite the solid theoretical foundation, applying SC in practice usually faces the challenge that we don’t know how accurate the estimated ATE is because there exists no post-treatment counterfactual in reality to validate the synthesised one. However, there are some actions we can take to optimise the modeling process and maximise the reliability. Next, I will describe these actions and demonstrate how they influence the estimated ATE via a range of experiments based on the synthetic time-series data with diverse temporal characteristics.

Experiment Setup

All the experiments presented in this article are based on synthetic time-series data. These data are generated using the timeseries-generator package that produces time series capturing the real-world factors including GDP, holidays, weekends, and so on.

The data generation aims to simulate the campaign performance of the stores in New Zealand from 01/01/2019 to 31/12/2019. To make the potential conclusions statistically significant, 500 time series are generated to represent the stores. Each time series has the statistically randomised linear trend, white noise, store factor, holiday factor, weekday factor, and seasonality. A random sample of 10 stores are presented below.

Store1 is selected to be the treatment group whereas others play the role of control groups. Next, the outcome of store1 is uplifted by 20% from 2019-09-01 onwards to simulate the treated behaviour whereas its original outcome serves as the real counterfactual. This 20% uplift establishes the actual ATE to validate the actions later on.

cutoff_date_sc = '2019-09-01'

df_sc.loc[cutoff_date_sc:] = df_sc.loc[cutoff_date_sc:]*1.2

The figure below visualises the simulated treatment effect and the true counterfactual of the treatment group.

Given the synthetic data, the BSTS in Causalimpact is adopted to estimate the synthesised ATE. Then, the estimation is compared against the actual ATE using Mean Absolute Percentage Error (MAPE) to evaluate the corresponding action.

Next, let’s go through the actions along with the related experiments to see how to produce reliable ATE estimation.

Treatment-control Correlation

The first action to achieve reliable ATE estimation is selecting the control groups that exhibit high pre-treatment correlations with the treatment group. The rationale is that a highly correlated control is likely to consistently resemble the untreated treatment group over time.

To validate this hypothesis, let’s evaluate the ATE estimation produced using every single control with its full data since 01/01/2019 to understand the impact of correlation. Firstly, the correlation coefficients between the treatment group (store1) and the control groups (store2 to 499) are calculated [3].

def correlation(x, y):

shortest = min(x.shape[0], y.shape[0])

return np.corrcoef(x.iloc[:shortest].values, y.iloc[:shortest].values)[0, 1]

As shown in the figure below, the distribution of the correlations range from -0.1 to 0.9, which provides a comprehensive understanding about the impact across various scenarios.

Then, every individual control is used to predict the counterfactual, estimate the ATE, and report the MAPE. In the figure below, the averaged MAPE of ATE with its 95% confidence interval is plotted against the corresponding pre-treatment correlation. Here, the correlation coefficients are rounded to one decimal place to facilitate aggregation and improve the statistical significance in the analysis. Looking at the results, it is obvious that the estimation shows a higher reliability when the control gets more correlated with the treatment group.

Now let’s see some examples that demonstrate the impact of pre-treatment correlation: store88 with a correlation of 0.88 delivers a MAPE of 0.12 that is superior to 0.62 given by store3 with a correlation of 0.43. Besides the promising accuracy, the probabilistic intervals are correspondingly narrow, which implies high prediction certainty.

Model Fitting Window

Next, the fitting window, which is the length of the pre-treatment interval used for fitting the model, needs to be properly configured. This is because too much context could result in a loss of recency while insufficient context might lead to overfitting.

To understand how fitting window impacts the accuracy of ATE estimation, a wide range of values from 1 month to 8 months before the treatment date are experimented. For each fitting window, every single unit of the 499 control groups is evaluated individually and then aggregated to calculate the averaged MAPE with the 95% confidence interval. As depicted in the figure below, there exists a sweet spot nearby 2 and 3 months that optimise the reliability. Identifying the optimal point is outside the scope of this discussion but it’s worth noting that the training window needs to be carefully selected.

The figure shows two examples: the MAPE of control group 199 is reduced from 0.89 to 0.68 when its fitting window is increased from 1 month to 3 months because the short window contains insufficient knowledge to produce the counterfactual.

Number of Control Units

Lastly, the number of the selected control groups matters.

This hypothesis is validated by investigating the estimation accuracy for different numbers of controls ranging from 1 to 10. In detail, for each control count, the averaged MAPE is calculated based on the estimations produced by 50 random control sets with each containing the corresponding number of control groups. This operation avoids unnecessarily enumerating every possible combination of controls while statistically controls for correlation. In addition, the fitting window is set to 3 months for every estimation.

Looking at the results below, increasing the number of controls is overall leading towards a more reliable ATE estimation.

The examples below demonstrate the effect. The first estimation is generated using store311 whereas the second one further adds store301 and store312.

Conclusions

In this article, I discussed the possible actions that make the SC estimation more reliable. Based on the experiments with diverse synthetic data, the pre-treatment correlation, fitting window, and number of control units are identified as compelling directions to optimise the estimation. Finding the optimal value for each action is out of the scope of this discussion. However, if you feel interested, parameter search using an isolated blank period for validation [4] is one possible solution.

All the images are produced by the author unless otherwise noted. The discussions are inspired by the great work “Synthetic controls in action” [1].

References

[1] Abadie, Alberto, and Jaume Vives-i-Bastida. “Synthetic controls in action.” arXiv preprint arXiv:2203.06279 (2022).

[2]Brodersen, Kay H., et al. “Inferring causal impact using Bayesian structural time-series models.” (2015): 247–274.

[3]https://medium.com/@dreamferus/how-to-synchronize-time-series-using-cross-correlation-in-python-4c1fd5668c7a

[4]Abadie, Alberto, and Jinglong Zhao. “Synthetic controls for experimental design.” arXiv preprint arXiv:2108.02196 (2021)."
https://towardsdatascience.com/using-clustering-algorithms-for-player-recruitment-98208d3a6cb4?source=rss-------1,Using Clustering Algorithms for Player Recruitment,"Sports Analytics

Some days ago, I was fortunate to be able to participate in a football analytics hackathon that was organized by xfb Analytics[1], Transfermarkt[2], and Football Forum Hungary[3]."
https://towardsdatascience.com/coverage-vs-accuracy-striking-a-balance-in-data-science-d555415eebe4,Coverage vs. Accuracy: Striking a Balance in Data Science,"Introduction

Every day, numerous data science projects are discarded due to insufficient prediction accuracy. It’s a regrettable outcome, considering that often these models could be exceptionally well-suited for some subsets of the dataset.

Data Scientists often try to improve their models by using more complex models and by throwing more and more data at the problem. But many times there is a much simpler and more productive approach: Instead of trying to make all of our predictions better all at once, we could start by making good predictions for the easy parts of the data, and only then work on the harder parts.

This approach can greatly affect our ability to solve real-world problems. We start with the quick gain on the easy problems and only then focus our effort on the harder problems.

Breaking down the complexity

In real-world datasets, complexity is the rule rather than the exception. Consider a medical diagnosis task, where subtle variations in symptoms can make the difference between life-threatening conditions and minor ailments. Achieving high accuracy in such scenarios can be challenging, if not impossible, due to the inherent noise and nuances in the data.

This is where the idea of coverage comes into play. Coverage refers to the portion of the data that a model successfully predicts or classifies with high confidence or high precision. Instead of striving for high accuracy across the entire dataset, researchers can choose to focus on a subset of the data where prediction is relatively straightforward. By doing so, they can achieve high accuracy on this subset while acknowledging the existence of a more challenging, uncovered portion.

For instance, consider a trained model with a 50% accuracy rate on a test dataset. In this scenario, it’s possible that if we could identify and select only the predictions we are very sure about (although we should decide what “very sure” means), we could end up with a model that covers fewer cases, let’s say around 60%, but with significantly improved accuracy, perhaps reaching 85%.

I don’t know any product manager who would say no in such a situation. Especially if there is no model in production, and this is the first model.

The two-step model

We want to divide our data into two distinct subsets: the covered and the uncovered. The covered data is the part of the data where the initial model achieves high accuracy and confidence. The uncovered data is the part of the data where our model does not give confident predictions and does not achieve high accuracy.

In the first step, a model is trained on the data. Once we identify a subset of data where the model achieves high accuracy, we deploy that model and let it run on that subset — the covered data.

In the second step, we move our focus to the uncovered data. We try to develop a better model for this data by collecting more data, using more advanced algorithms, feature engineering, and incorporating domain-specific knowledge to find patterns in the data.

At this step, the first thing you should do is look at the errors by eye. Many times you will easily identify many patterns this way before using any fancy tricks.

An example

This example will show how the concept of agile workflow can create great value. This is a very simple example that is meant to visualize this concept. Real-life examples will be a lot less obvious but the idea that you will see here is just as relevant.

Let’s look at this two-dimensional data that I simulated from three equally sized classes.

num_samples_A = 500

num_samples_B = 500

num_samples_C = 500

# Class A

mean_A = [3, 2]

cov_A = [[0.1, 0], [0, 0.1]] # Low variance

class_A = np.random.multivariate_normal(mean_A, cov_A, num_samples_A)

# Class B

mean_B = [0, 0]

cov_B = [[1, 0.5], [0.5, 1]] # Larger variance with some overlap with class C

class_B = np.random.multivariate_normal(mean_B, cov_B, num_samples_B)

# Class C

mean_C = [0, 1]

cov_C = [[2, 0.5], [0.5, 2]] # Larger variance with some overlap with class B

class_C = np.random.multivariate_normal(mean_C, cov_C, num_samples_C)

Now we try to fit a machine learning classifier to this data, it looks like an SVM classifier with a Gaussian (‘rbf’) kernel might do the trick:

import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC

# Creating DataFrame

data = np.concatenate([class_A, class_B, class_C])

labels = np.concatenate([np.zeros(num_samples_A), np.ones(num_samples_B), np.ones(num_samples_C) * 2])

df = pd.DataFrame(data, columns=['x', 'y'])

df['label'] = labels.astype(int)

# Splitting data into train and test sets

X_train, X_test, y_train, y_test = train_test_split(df[['x', 'y']], df['label'], test_size=0.2, random_state=42)

# Training SVM model with RBF kernel

svm_rbf = SVC(kernel='rbf', probability= True)

svm_rbf.fit(X_train, y_train)

# Predict probabilities for each class

svm_rbf_probs = svm_rbf.predict_proba(X_test)

# Get predicted classes and corresponding confidences

svm_rbf_predictions = [(X_test.iloc[i]['x'], X_test.iloc[i]['y'], true_class, np.argmax(probs), np.max(probs)) for i, (true_class, probs) in enumerate(zip(y_test, svm_rbf_probs))]

svm_predictions_df = pd.DataFrame(svm_rbf_predictions).rename(columns={0:'x',1:'y' ,2: 'true_class', 3: 'predicted_class', 4: 'confidence'})

How does this model perform on our data?

accuracy = (svm_predictions_df['true_class'] == svm_predictions_df['predicted_class']).mean()*100

print(f'Accuracy = {round(accuracy,2)}%')

Accuracy = 75.33%

75% percent accuracy is disappointing, but does this mean that this model is useless?

Now we want to look at the most confident predictions and see how the model performs on them. How do we define the most confident predictions? We can try out different confidence (predict_proba) thresholds and see what coverage and accuracy we get for each threshold and then decide which threshold meets our business needs.

thresholds = [.5, .55, .6, .65, .7, .75, .8, .85, .9]

results = []

for threshold in thresholds:

svm_df_covered = svm_predictions_df.loc[svm_predictions_df['confidence'] > threshold]

coverage = len(svm_df_covered) / len(svm_predictions_df) * 100

accuracy_covered = (svm_df_covered['true_class'] == svm_df_covered['predicted_class']).mean() * 100

results.append({'Threshold': threshold, 'Coverage (%)': round(coverage,2), 'Accuracy on covered data (%)': round(accuracy_covered,2)})

results_df = pd.DataFrame(results)

print(results_df)

And we get

Or if we want a more detailed look we can create a plot of the coverage and accuracy by threshold:

We can now select the threshold that fits our business logic. For example, if our company’s policy is to guarantee at least 90% accuracy, then we can choose a threshold of 0.75 and get an accuracy of 90% for 62% of the data. This is a huge improvement to throwing out the model, especially if we don’t have any model in production!

Now that our model is happily working in production on 60% of the data, we can shift our focus to the rest of the data. We can collect more data, do more feature engineering, try more complex models, or get help from a domain expert."
https://towardsdatascience.com/simulated-data-real-learnings-simulating-systems-79374a9379fd,"Simulated Data, Real Learnings : Simulating Systems","INTRODUCTION

Simulation is a powerful tool in the data science tool box. In this article, we’ll talk about how simulating systems can help us formulate better strategies and make better decisions."
https://towardsdatascience.com/the-4-hats-of-a-full-stack-data-scientist-5b916bd2f079,The 4 Hats of a Full-Stack Data Scientist,"What is a Full Stack Data Scientist?

When I first learned data science (5+ years ago), data engineering and ML engineering were not as widespread as they are today. Consequently, the role of a data scientist was often more broadly defined than what we may see these days.

For example, data scientists may have written ETL scripts, set up databases, performed feature engineering, trained ML models, and deployed models into production.

Although it is becoming more common to split these tasks across multiple roles (e.g., data engineers, data scientists, and ML engineers), many situations still call for contributors who are well-versed in all aspects of ML model development. I call these contributors full-stack data scientists.

More specifically, I see a full-stack data scientist as someone who can manage and implement an ML solution end-to-end. This involves formulating business problems, designing ML solutions, sourcing and preparing data for development, training ML models, and deploying models so their value can be realized.

Why do we need them?

Given the rise of specialized roles for implementing ML projects, this notion of FSDS may seem outdated. At least, that was what I thought in my first corporate data science role.

These days, however, the value of learning the full tech stack is becoming increasingly obvious to me. This all started last year when I interviewed top data science freelancers from Upwork.

Almost everyone I spoke to fit the full stack data scientist definition given above. This wasn’t just out of fun and curiosity but from necessity.

A key takeaway from these interviews was data science skills (alone) are limited in their potential business impact. To generate real-world value (that a client will pay for), building solutions end-to-end is a must.

But this isn’t restricted to freelancing. Here are a few other contexts where FSDS can be beneficial

An SMB (small-medium business) with only 1 dedicated resource for AI/ML projects

A lone AI/ML contributor is embedded in a business team

Founder who wants to build an ML product

Individual contributor at a large enterprise who can explore projects outside established teams

In other words, full-stack data scientists are generalists who can see the big picture and dive into specific aspects of a project as needed. This makes them a valuable resource for any business looking to generate value via AI and machine learning.

4 Hats of FSDS

While FSDS requires several skills, the role can be broken down into four key hats: Project Manager, Data Engineer, Data Scientist, and ML Engineer.

Of course, no one can be world-class in all hats (probably). But one can certainly be above average across the board (it just takes time).

Here, I’ll break down each of these hats based on my experience as a data science consultant and interviews with 27 data/ML professionals.

Hat 1: Project Manager

The key role of a project manager (IMO) is to answer 3 questions: what, why, and how. In other words, what are we building? Why are we building it? How will we do it?

While it might be easy to skip over this work (and start coding), failing to put on the PM hat properly risks spending a lot of time (and money) solving the wrong problem. Or solving the right problem in an unnecessarily complex and expensive way.

The starting point for this is defining the business problem. In most contexts, the full-stack data scientist isn’t solving their problem, so this requires the ability to work with stakeholders to uncover the problem's root causes. I discussed some tips on this in a previous article.

Once the problem is clearly defined, one can identify how AI can solve it. This sets the target from which to work backward to estimate project costs, timelines, and requirements.

Key skills

Communication and managing relationships

Diagnose problems and design solutions

Estimating project timelines, costs, and requirements

Hat 2: Data Engineer

In the context of FSDS, data engineering is concerned with making data readily available for model development or inference (or both).

Since this is inherently product-focused, the DE hat may be more limited than a typical data engineering role. More specifically, this likely won’t require optimizing data architectures for several business use cases.

Instead, the focus will be on building data pipelines. This involves designing and implementing ETL (or ELT) processes for specific use cases.

ETL stands for extract, transform, and load. It involves extracting data from their raw sources, transforming it into a meaningful form (e.g., data cleaning, deduplication, exception handling, feature engineering), and loading it into a database (e.g., data modeling and database design).

Another important area here is data monitoring. While the details of this will depend on the specific use case, the ultimate goal is to give ongoing visibility to data pipelines via alerting systems, dashboards, or the like.

Key skills

Python, SQL, CLI (e.g. bash)

Data pipelines, ETL/ELT (Airflow, Docker)

A cloud platform (AWS, GCP, or Azure)

Hat 3: Data Scientist

I define a data scientist as someone who uses data to uncover regularities in the world that can be used to drive impact. In practice, this often boils down to training a machine learning model (because computers are much better than humans at finding regularities in data).

For most projects, one must switch between this Hat and Hats 1 and 2. During model development, it is common to encounter insights that require revisiting the data preparation or project scoping.

For example, one might discover that an exception was not properly handled for a particular field or that the extracted fields do not have the predictive power that was assumed at the project's outset.

An essential part of model training is model validation. This consists of defining performance metrics that can be used to evaluate models. Bonus points if this metric can be directly translated into a business performance metric.

With a performance metric, one can programmatically experiment with and evaluate several model configurations by adjusting, for example, train-test splits, hyperparameters, predictor choice, and ML approach. If no model training is required, one may still want to compare the performance of multiple pre-trained models.

Key Skills

Python (pandas/polars, sklearn, TensorFlow/PyTorch)

Exploratory Data Analysis (EDA)

Model Development (feature engineering, experiment tracking, hyperparameter tuning)

Hat 4: ML Engineer

The final hat involves taking the ML model and turning it into an ML solution—that is, integrating the model into business workflows so its value can be realized.

A simple way to do this is to containerize the model and set up an API so external systems can make inference calls. For example, the API could be connected to an internal website that allows business users to run a calculation.

Some use cases, however, may not be so simple and require more sophisticated solutions. This is where an orchestration tool can help define complex workflows. For example, if the model requires monthly updates as new data become available, the whole model development process, from ETL to training to deployment, may need to be automated.

Another important area of consideration is model monitoring. Like data monitoring, this involves tracking model predictions and performance over time and making them visible through automated alerts or other means.

While many of these processes can run on local machines, deploying these solutions using a cloud platform is common practice. Every ML engineer (MLE) I have interviewed uses at least 1 cloud platform and recommended cloud deployments as a core skill of MLEs.

Key Skills

Containerize scripts (Docker), build APIs (FastAPI)

Orchestration — connecting data and ML pipelines (AirFlow)

A cloud platform (AWS, GCP, or Azure)

Becoming the Unicorn

While a full-stack data scientist may seem like a technical unicorn, the point (IMO) isn’t to become a guru of all aspects of the tech stack. Rather, it is to learn enough to be dangerous.

In other words, it’s not about mastering everything but being able to learn anything you need to get the job done. From this perspective, I surmise that most data scientists will become “full stack” given enough time.

Toward this end, here are 3 principles I am using to accelerate my personal FSDS development.

Have a reason to learn new skills — e.g. build end-to-end projects

Just learn enough to be dangerous

Keep things as simple as possible — i.e. don’t overengineer solutions

What’s next?

A full-stack data scientist can manage and implement an ML solution end-to-end. While this may seem like overkill for contexts where specialized roles exist for key stages of model development, this generalist skillset is still valuable in many situations.

As part of my journey toward becoming a full-stack data scientist, future articles of this series will walk through each of the 4 FSDS Hats via the end-to-end implementation of a real-world ML project.

In the spirit of learning, if you feel anything is missing here, I invite you to drop a comment (they are appreciated) 😁"
https://towardsdatascience.com/addressing-spatial-dependencies-674c6d670071,How to Address Spatial Dependencies,"Remote sensing, a field that deals with tons of spatial data extracted and processed from satellite images, aerial photos, and other sensor-based technologies, or any field using data with spatial features, presents a non-trivial challenge. When we analyze all this data, we have to deal with spatial dependencies (i.e., how…"
https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474,Learning Generalist Models for Anomaly Detection,"Generalist Anomaly Detection (GAD) aims to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data.

Work to be published at CVPR 2024 [1].

Overview

Some recent studies have showed that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images.

In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL.

It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training.

Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.

Introduction

Anomaly Detection (AD) is a crucial computer vision task that aims to detect samples that substantially deviate from the majority of samples in a dataset, due to its broad real-life applications such as industrial inspection, medical imaging analysis, and scientific discovery, etc. [2–3]. Current AD paradigms are focused on individually building one model on the training data, e.g.,, a set of anomaly-free samples, of each target dataset, such as data reconstruction approach, one-class classification, and knowledge distillation approach. Although these approaches have shown remarkable detection performance on various AD benchmarks, they require the availability of large training data and the skilled detection model training per dataset. Thus, they become infeasible in application scenarios where training on the target dataset is not allowed due to either data privacy issues, e.g., arising from using those data in training the models due to machine unlearning [3], or unavailability of large-scale training data in the deployment of new applications. To tackle these challenges, this work explores the problem of learning Generalist Anomaly Detection (GAD) models, aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any training on the target data.

Being pre-trained on web-scale image-text data, large Visual-Language Models (VLMs) like CLIP have exhibited superior generalization capabilities in recent years, achieving accurate visual recognition across different datasets without any fine-tuning or adaptation on the target data. More importantly, some very recent studies (e.g., WinCLIP [5]) show that these VLMs can also be utilized to achieve remarkable generalization on different defect detection datasets. Nevertheless, a significant limitation of these models is their dependency on a large set of manually crafted prompts specific to defects. This reliance restricts their applicability, making it challenging to extend their use to detecting anomalies in other data domains, e.g., medical image anomalies or semantic anomalies in one-vs-all or multi-class settings.

To address this problem, we propose to train a GAD model that aims to utilize few-shot normal images from any target dataset as sample prompts for supporting GAD on the fly, as illustrated in Figure 1(Top). The few-shot setting is motivated by the fact that it is often easy to obtain few-shot normal images in real-world applications. Furthermore, these few-shot samples are not used for model training/tuning; they are just used as sample prompts for enabling the anomaly scoring of test images during inference. This formulation is fundamentally different from current few-shot AD methods that use these target samples and their extensive augmented versions to train the detection model, which can lead to an overfitting of the target dataset and fail to generalize to other datasets, as shown in Figure 1(Bottom).

We then introduce an GAD approach, the first of its kind, that learns an in-context residual learning model based on CLIP, termed InCTRL. It trains an GAD model to discriminate anomalies from normal samples by learning to identify the residuals/discrepancies between query images and a set of few-shot normal images from auxiliary data. The few-shot normal images, namely in-context sample prompts, serve as prototypes of normal patterns. When comparing with the features of these normal patterns, per definition of anomaly, a larger residual is typically expected for anomalies than normal samples in datasets of different domains, so the learned in-context residual model can generalize to detect diverse types of anomalies across the domains. To capture the residuals better, InCTRL models the in-context residuals at both the image and patch levels, gaining an in-depth in-context understanding of what constitutes an anomaly. Further, our in-context residual learning can also enable a seamless incorporation of normal/abnormal text prompt-guided prior knowledge into the detection model, providing an additional strength for the detection from the text-image-aligned semantic space.

Extensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulates three types of popular AD tasks, including industrial defect anomaly detection, medical image anomaly detection, and semantic anomaly detection under both one-vs-all and multi-class settings. Our results show that InCTRL significantly surpasses existing state-of-the-art methods.

Approach

Our approach InCTRL is designed to effectively model the in-context residual between a query image and a set of few-shot normal images as sample prompts, utilizing the generalization capabilities of CLIP to detect unusual residuals for anomalies from different application domains.

CLIP is a VLM consisting of a text encoder and a visual encoder, with the image and text representations from these encoders well aligned by pre-training on web-scale text-image data. InCTRL is optimized using auxiliary data via an in-context residual learning in the image encoder, with the learning augmented by text prompt-guided prior knowledge from the text encoder.

To be more specific, as illustrated in Fig.2, we first simulate an in-context learning example that contains one query image x and a set of few-shot normal sample prompts P’, both of which are randomly sampled from the auxiliary data. Through the visual encoder, we then perform multi-layer patch-level and image-level residual learning to respectively capture local and global discrepancies between the query and few-shot normal sample prompts. Further, our model allows a seamless incorporation of normal and abnormal text prompts-guided prior knowledge from the text encoder based on the similarity between these textual prompt embeddings and the query images . The training of InCTRL is to optimize a few projection/adaptation layers attached to the visual encoder to learn a larger anomaly score for anomaly samples than normal samples in the training data, with the original parameters in both encoders frozen; during inference, a test image, together with the few-shot normal image prompts from the target dataset and the text prompts, is put forward through our adapted CLIP-based GAD network, whose output is the anomaly score for the test image.

Empirical Results

Datasets and Evaluation Metrics. To verify the efficiency of our method, we conduct comprehensive experiments across nine real-world AD datasets, including five industrial defect inspection dataset (MVTec AD, VisA, AITEX, ELPV, SDD), two medical image datasets (BrainMRI, HeadCT), and two semantic anomaly detection datasets: MNIST and CIFAR-10 under both one-vs-all and multi-class protocols. Under the one-vs-all protocol, one class is used as normal, with the other classes treated as abnormal; while under the multi-class protocol, images of even-number classes from MNIST and animal-related classes from CIFAR-10 are treated as normal, with the images of the other classes are considered as anomalies.

To assess the GAD performance, MVTec AD, the combination of its training and test sets, is used as the auxiliary training data, on which GAD models are trained, and they are subsequently evaluated on the test set of the other eight datasets without any further training. We train the model on VisA when evaluating the performance on MVTec AD.

The few-shot normal prompts for the target data are randomly sampled from the training set of target datasets and remain the same for all models for fair comparison. We evaluate the performance with the number of few-shot normal prompt set to K = 2, 4, 8. The reported results are averaged over three independent runs with different random seeds.

As for evaluation metrics, we use two popular metrics AUROC (Area Under the Receiver Operating Characteristic) and AUPRC (Area Under the Precision-Recall Curve) to evaluate the AD performance.

Results. The main results are reporeted in Tables 1 and 2. For the 11 industrial defect AD datasets, InCTRL significantly outperforms all competing models on almost all cases across the three few-shot settings in both AUROC and AUPRC. With more few-shot image prompts, the performance of all methods generally gets better. InCTRL can utilize the increasing few-shot samples well and remain the superiority over the competing methods.

Ablation Study. We examine the contribution of three key components of our approach on the generalization: text prompt-guided features (T), patch-level residuals (P), and image-level residuals (I), as well as their combinations. The results are reported in Table 3. The experiment results indicate that for industrial defect AD datasets, visual residual features play a more significant role compared to text prompt-based features, particularly on datasets like ELPV, SDD, and AITEX. On the medical image AD datasets, both visual residuals and textual knowledge contribute substantially to performance enhancement, exhibiting a complementary relation. On semantic AD datasets, the results are dominantly influenced by patch-level residuals and/or text prompt-based features. Importantly, our three components are generally mutually complementary, resulting in the superior detection generalization across the datasets.

Significance of In-context Residual Learning. To assess the importance of learning the residuals in InCTRL, we experiment with two alternative operations in both multi-layer patch-level and image-level residual learning: replacing the residual operation with 1) a concatenation operation and 2) an average operation, with all the other components of InCTRL fixed. As shown in Table 3, the in-context residual learning generalizes much better than the other two alternative ways, significantly enhancing the model’s performance in GAD across three distinct domains.

Conclusion

In this work we introduce a GAD task to evaluate the generalization capability of AD methods in identifying anomalies across various scenarios without any training on the target datasets. This is the first study dedicated to a generalist approach to anomaly detection, encompassing industrial defects, medical anomalies, and semantic anomalies. Then we propose an approach, called InCTRL, to addressing this problem under a few-shot setting. InCTRL achieves a superior GAD generalization by holistic in-context residual learning. Extensive experiments are performed on nine AD datasets to establish a GAD evaluation benchmark for the aforementioned three popular AD tasks, on which InCTRL significantly and consistently outperforms SotA competing models across multiple few-shot settings.

Please check out the full paper [1] for more details of the approach and the experiments. Code is publicly available at https://github.com/mala-lab/InCTRL.

References"
https://towardsdatascience.com/productionize-llm-rag-app-in-django-part-i-celery-26053b4acad6,Productionize LLM RAG App in Django — Part I: Celery,"It’s been a while since my last LLM post and I’m excited to share that my prototype has been successfully productionized as Outside’s first LLM-powered chatbot, Scout. If you are an Outside+ member, you can check it out over at https://scout.outsideonline.com/."
https://towardsdatascience.com/how-to-use-synthetic-and-simulated-data-effectively-04d8582b6f88,How to Use Synthetic and Simulated Data Effectively,"TDS Editors

·

Follow

Published in

Towards Data Science

·

3 min read

·

6 days ago

--

Using synthetic data isn’t exactly a new practice: it’s been a productive approach for several years now, providing practitioners with the data they need for their projects in situations where real-world datasets prove inaccessible, unavailable, or limited from a copyright or approved-use perspective.

The recent rise of LLMs and AI-generated tools has transformed the synthetic-data scene, however, just as it has numerous other workflows for machine learning and data science professionals. This week, we’re presenting a collection of recent articles that cover the latest trends and possibilities you should be aware of, as well as the questions and considerations you should keep in mind if you decide to create your own toy dataset from scratch. Let’s dive in!

How To Use Generative AI and Python to Create Designer Dummy Datasets If it’s been a while since the last time you found yourself in need of synthetic data, don’t miss Mia Dwyer’s concise tutorial, which outlines a streamlined method for creating a dummy dataset with GPT-4 and a little bit of Python. Mia keeps things fairly simple, and you can adapt and build on this approach so it fits your specific needs.

Creating Synthetic User Research: Using Persona Prompting and Autonomous Agents For a more advanced use case that also relies on the power of generative-AI applications, we recommend catching up with Vincent Koc’s guide to synthetic user research. It leverages an architecture of autonomous agents to “create and interact with digital customer personas in simulated research scenarios,” making user research both more accessible and less resource-heavy.

Synthetic Data: The Good, the Bad and the Unsorted Working with generated data solves some common problems, but can introduce a few others. Tea Mustać focuses on a promising use case—training AI products, which often requires massive amounts of data—and unpacks the legal and ethical concerns that synthetic data can help us bypass, as well as those it can’t.

Simulated Data, Real Learnings: Scenario Analysis In his ongoing series, Jarom Hulet looks at the different ways that simulated data can empower us to make better business and policy decisions and draw powerful insights along the way. After covering model testing and power analysis in previous articles, the latest installment zooms in on the possibility of simulating more complex scenarios for optimized outcomes.

Evaluating Synthetic Data — The Million Dollar Question The main assumption behind every process that relies on synthetic data is that the latter sufficiently resembles the statistical properties and patterns of the real data it emulates. Andrew Skabar, PhD offers a detailed guide to help practitioners evaluate the quality of their generated datasets and the degree to which they meet that crucial threshold.

For more thought-provoking articles on other topics—from data career moves to multi-armed pendulums—we invite you to explore these recent standouts:

The question of copyright in the context of generative-AI tools continues to dominate industry conversations; Stephanie Kirmer unpacks the stakes and looks into the future in her latest deep dive.

We’re thrilled to welcome back Fraser King, who shared an accessible walkthrough of his research on image inpainting of radar blind zones using deep learning.

How can you make the jump from data scientist to ML/AI product manager? Anna Via offers pragmatic tips for a successful transition, based on her own experiences in the past couple of years.

Finding product-market fit is every startup’s goal—and one that often remains elusive. Myriam Barnés presents a quantitative approach based on user data, focusing on both growth and cohorts analysis.

It can be tough for data teams to scale their platforms effectively; Mahdi Karabiben outlines several key principles that will help data managers stay on the right path.

To end on a more theoretical note, we invite you to read Oliver W. Johnson’s debut TDS article, which relies on VPython simulations to model chaotic motion and investigate what defines a chaotic system.

Thank you for supporting the work of our authors! If you’re feeling inspired to join their ranks, why not write your first post? We’d love to read it.

Until the next Variable,

TDS Team"
https://sproutsocial.com/insights/best-times-to-post-on-social-media/,Best times to post on social media in 2024,"The internet never sleeps so the best time to post on social media seems like it’s at every hour of the day. We’re going to demystify this myth so your social team can plan, prioritize and create a successful social media marketing strategy.

With over 5 billion social media users worldwide and across several platforms, social media presents many opportunities for businesses across all industries and sizes. Where social media had once been used solely for brand awareness, the impact of social media on business spans across several teams: customer experience, sales, product development, content and marketing.

We worked closely with Sprout’s Data Science team to review findings and trends in social media usage over the past year from Sprout Social’s more than 34,000 customers and understand when their content was most and least frequently engaged with, broken out by platform and industry.

For more information on platform-specific optimal engagement times, including industry breakdowns, review our guides on the:

Best times to post on Facebook in 2024

Best times to post on Instagram in 2024

Best times to post on LinkedIn in 2024

Best times to post on TikTok in 2024

Best times to post on X (FKA Twitter) in 2024

Use the data from this article to help inform your publishing calendar. However, the best way to learn how to identify your best times to post globally across each network is to start a free trial of Sprout Social and test our ViralPost® feature with your social profiles.

Start a free Sprout Social trial

Disclaimer: Data from Sprout Social include users from various plan types, industries and locations. All time frames are recorded globally (not by Central Time), meaning you should be able to publish with the times provided in any timezone and see positive engagement results. Number of engagements represents total engagements a brand received on the specific channel during that hour time frame. Industry-specific data includes mid- to high-level engagement times. We recommend testing with your own accounts to find the most optimal send times.

Best Times to Post on Social Media Overall

Best times to post on social media:

Mondays from 11 a.m. to noon

Tuesdays from 10 a.m. to 2 p.m. and 3 to 4 p.m.

Wednesdays from 9 a.m. to 3 p.m.

Thursdays from 9 a.m. to 2 p.m.

Fridays from 10 to 11 a.m.

Best days to post on social media: Tuesdays, Wednesdays and Thursdays

Worst days to post on social media: Sundays

Each social channel has its benefits depending on your goals, campaigns, content type and target audience. If you’re seeking to uncover changes to and improve your engagement, revisit your social media KPIs. Does your current social strategy align with meeting those KPIs? Of course, if increasing likes and comments are some of your KPIs, knowing which days get the most engagement helps you reach those goals as well.

While interpreting the data for each social network individually, we consistently notice that the highest times of engagement are Tuesdays through Thursdays generally between 9 a.m. and 2 p.m. Midweek mornings prove to be a successful time across most social platforms, including Facebook, Instagram, Twitter and LinkedIn. Pinterest and TikTok see higher engagement in the afternoons.

There’s more to posting on social media than this quick stat, though. Each network sees varying ranges of engagement throughout the week and at remarkably different times. And when you isolate industries on each network, engagement varies even more. So even though Tuesdays through Thursdays between 9 a.m. and 2 p.m. are generally the best times to post on social, consider other variables before you schedule any posts.

Best time to post on social media in the UK

Overall, the best times to post on social media in the UK are Tuesday and Thursday between 9 a.m. and 2 p.m. Weekday mornings are a great time to post across many of the most popular social platforms in the UK including Facebook, Instagram, Twitter and LinkedIn. If you target UK users of Tiktok or Pinterest, afternoon posts receive higher engagement. It is important to remember this data is collated and industry, content and goals will also play a role in the right time to post for your business but this is a useful starting point.

Best Times to Post on Facebook

Best times to post on Facebook:

Mondays from 9 a.m. to noon

Tuesdays from 9 a.m. to 2 p.m. and 5 p.m.

Wednesdays from 9 a.m. to 3 p.m. and 5 p.m.

Thursdays from 8 a.m. to 2 p.m. and 5 p.m.

Fridays from 9 to 11 a.m.

Best days to post on Facebook: Mondays through Thursdays

Worst days to post on Facebook: Sundays

It’s true: Facebook is still the most used platform by marketers worldwide at 89%, and by monthly active users at 3.05 billion (yes, billion). The impact Facebook has on our social media use as consumers and marketers is still ever present. Compared to last year, high engagement periods on Facebook are still within mid-mornings during weekdays and matching general business hours.

Optimal send times for Facebook are Mondays through Fridays starting mostly at 9 a.m. until early afternoon, generally 2 p.m. What does this mean for marketers? Most users are taking a quick break in their routines for Facebook, making it the best time to schedule posts for target audiences to interact with.

With a solid Facebook marketing strategy, businesses connect with the right audiences with their content, whether it’s keeping audiences informed of products or sharing access to exclusive deals. Remember, engagement is a combination of how often you post on social media and what content types you post.

Best Times to Post on Instagram

Best times to post on Instagram:

Mondays from 11 a.m. to 2 p.m.

Tuesdays from 10 a.m. to 4 p.m.

Wednesdays from 9 a.m. to 4 p.m.

Thursdays from 9 a.m. to 1 p.m. and 2 p.m.

Fridays at 11 a.m.

Best days to post on Instagram: Tuesdays, Wednesdays and Thursdays

Worst days to post on Instagram: Sundays

Where Facebook was the most-used platform by marketers worldwide, Instagram comes in second at 80%. Interestingly, Instagram ties with Facebook for the social media platforms that offer marketers the highest ROI at 29%. This is because social media continues to evolve with ecommerce and offering frictionless experiences to users. And even though the competition for short-form video between Instagram and TikTok continues to heat up, Reels still generate more likes and saves than static Instagram posts.

The best times to post on Instagram are weekdays generally between 10 a.m. and 2 p.m., with Tuesdays and Wednesdays engagement times extending to 4 p.m. We recommend scheduling posts Tuesdays through Thursdays between 10 and and 3 p.m. for optimal engagement. It’s wise to test these benchmarks against your current metrics too.

Instagram has grown a long way since its photo-only days. The app, which has 2 billion monthly active users, is a platform to share images, videos, Reels and live streams, just to name a few creative content types. It is also an avenue to collaborate and partner with other brands and provides a stream of revenue through its ecommerce capabilities.

The Instagram algorithm is unpredictable. To be successful on the platform is a balance of consistently posting high-quality content that will engage and entertain your audiences. That’s a lot to ask for with each piece of content published. But using that as a guiding post can help with your Instagram marketing strategy.

Best Times to Post on LinkedIn

Best times to post on LinkedIn Company Pages:

Tuesdays and Wednesdays from 10 a.m. to noon

Thursdays at 10 a.m.

Best days to post on LinkedIn Company Pages: Tuesdays through Thursdays

Worst days to post on LinkedIn Company Pages: Weekends

Unsurprising for this professional-focused social platform, LinkedIn engagement times are consistent with general business hours. High engagement hours are more concentrated midday and don’t extend too far out past the workday hours. Businesses are less likely to get eyes on their LinkedIn marketing efforts on the weekend when engagement and usage drops.

Because LinkedIn content is centered around professional thought-leadership and career growth, it makes sense that a majority of audiences engage with this content throughout their working day. By midday, there’s buzz on the platform, and the best times to post on LinkedIn are Tuesdays and Wednesdays between 10 a.m. and noon and Thursdays at 10 a.m..

Yes, LinkedIn is a business and employment-focused platform, but it’s more than a job board and networking platform. LinkedIn provides a platform for professional storytelling and advocacy, from both employees and employers alike. Contributors of all levels can share their subject-matter expertise, create conversations that challenge business-as-usual, and uplift professional voices around the world.

With over 1 billion members in 200 countries and regions, LinkedIn is important for a wide range of business opportunities. Businesses on LinkedIn are finding new ways to generate leads, find co-marketing partners, highlight company culture and attract talent. Because of these additional business opportunities LinkedIn provides, it makes sense that posting and engagement match with the business hours.

Best Times to Post on Pinterest

Best times to post on Pinterest:

Tuesdays through Fridays at 1 a.m.

Thursdays at 3 a.m.

Best days to post on Pinterest: Weekdays

Worst days to post on Pinterest: Weekends

While people may associate Pinterest as the visual search engine where users go to research and discover DIY projects and inspiration products, it also presents an opportunity to expand your community and brand awareness. While not all brands may be on the platform, those who use it know more of Pinterest’s benefits.

This is the second year we’re providing the best times to post on Pinterest. We’re seeing a significant shift in engagement hours this year compared to last. Whereas last year we saw a nearly universal spread across all hours of the day every day of the week, this year we’re seeing significant engagement at the early hours of the morning and lower moderate engagement midday during weekdays. Interestingly, Tuesdays through Fridays at 1 a.m. and Thursdays at 3 a.m. are peak time to post on Pinterest.

It may be a combination of when users are struck with inspiration and the Pinterest algorithm that affects engagement times, but that shouldn’t stop you from creating a social media calendar with posts scheduled at those hours.

Best Times to Post on TikTok

Best times to post on TikTok:

Tuesdays from 4 to 6 p.m.

Wednesdays from 9 to 11 a.m., noon, and 2 to 6 p.m.

Thursdays from 9 to 11 a.m. and 2 to 6 p.m.

Fridays from 4 to 6 p.m.

Best days to post on TikTok: Wednesdays and Thursdays

Worst days to post on TikTok: Sundays

As the youngest digital platform on this list, TikTok has grown significantly and made waves in half as many years as the others on this list. With the app now amassing over 1.5 billion active global users who spend an average of 23 hours on the app every month, TikTok has competitive power against other platforms.

The best times to post on TikTok are Tuesdays and Fridays from 4 to 6 p.m., Wednesdays, Thursdays mornings from 9 to 11 a.m. and Thursdays from 2 to 6 p.m. Afternoons are when the highest engagements are on the platform; users are looking to fill their afternoon entertainment boost. However, generally, mid-morning through the afternoon (9 a.m. to 5 p.m.) on Tuesdays through Fridays see above average engagement on the app.

With TikTok trends changing quickly, it’s important to understand how trends start, evolve and influence. Once you understand how these forces, moments or signals work, you can create a better TikTok trend strategy to implement rather than participating reactively.

Best Times to Post on X (FKA Twitter)

Best times to post on X (Twitter):

Mondays from 10 a.m. to noon

Tuesdays from 9 a.m. to 3 p.m.

Wednesdays from 9 a.m. to 3 p.m.

Thursdays from 9 a.m. to 3 p.m.

Fridays 10 a.m. to noon

Best days to post on X (Twitter): Tuesdays through Thursdays

Worst days to post on X (Twitter): Sundays

X (formerly known as Twitter) has consistently been the platform for up-to-the-minute conversations and breaking news. And that didn’t stop X from being part of the news itself on the network last year. Businesses and brands continuously look to X to participate in trending topics that align with their brand values in the process. However, due to global conflicts and vocal executives, some brands have limited their social strategies around X.

Despite all this, X peak times for engagement extended slightly compared to last year, with hours extending further into the weekday afternoons. The best times to post on Twitter are Mondays through Fridays beginning at 9 a.m. and extending until nearly 3 p.m..

This extension of engagement on X could reflect an interest in developing news and updates as the day unfolds, as well as a desire for entertainment. There were also d several global events that kept users engaged on the app last year. Brands also use X as part of their customer service strategy, as an alternative to traditional customer support options.

How to find your own best times to post on social media

Social marketers can plan content more effectively when they’re armed with data. And as platforms evolve, how brands and consumers use social will evolve with them, or vice versa. Staying aware of new content formats and trends might help you reach your audience in unexpected ways, or even find new segments and personas you weren’t focusing on before.

While you may be ready to jump in and post at these provided peak engagement times as part of your strategy, consider pairing this data with your own social media data analysis. You may discover that your specific target audience is active near the peak times we’ve outlined or hours earlier.

A social media management tool like Sprout Social can help you drill into your analytics and develop tailored findings for your brand accounts across all networks, holistically and individually. Gain competitive insights by comparing your performance to your competitors, to find new opportunities for engagement.

With features like post tagging and cross-network reporting, you can separate different factors like post author or types of content, and drill down into how your posting time influenced your social success.

Sprout’s patented ViralPost® technology uses the same data we’ve used to look at all our customer accounts to determine the best time to post tailored for your account. ViralPost® algorithmically determines the results for our Optimal Send Times feature, a publishing option available in all plans. Our proprietary machine learning feature isn't new, but we're constantly learning and improving our product to help brands surface insights faster. Our goal is to help people who use our product work more effectively.

With Optimal Send Times, the specific times offered are based on when your audience engages the most using your publishing history. ViralPost® also continually updates and refines to save you time from doing the repeated analysis and allowing you more time to craft great content.

Want to see it in action? Sign up for a free 30-day trial of Sprout Social or request a personalized demo and try it for yourself.

Start a free Sprout Social trial

How Sprout gathered the data

We understand that these reports can raise questions about just where the data is coming from. That’s why we want to be clear about the data we pulled and how we got here.

Sprout Social’s Data Science team gathers this information by analyzing nearly 2 billion engagements across 400,000 social profiles across Twitter, Facebook, LinkedIn, Instagram, Pinterest and TikTok. We synthesize this information in collaboration with our Data Science team to provide you with the article you read above.

Unfortunately, we do not have engagement data for YouTube or Threads, yet.

Start a free Sprout Social trial"
https://www.ukauthority.com/articles/iai-highlights-work-on-rapid-data-sharing-solution/,i.AI highlights work on rAPId data sharing solution,"The Incubator for AI (i.AI) unit in the Cabinet Office has begun work on a project to expand the functions of rAPId data sharing solution for government bodies.

It said the solution has been developed by the No.10 data science team for the creation of APIs to enable automated approach to data sharing rather than the use of spreadsheets and emails.

A working version is now available as open source on GitHub and i.AI said it is being actively used by several government departments including the Cabinet Office and HM Treasury.

The team is now working on Project Condor, an effort to develop an open source cloud infrastructure that will enable departments to create their own infrastructures of data, analytics and AI.

“The purpose of this is to reduced reliance on contractors and increasing government’s cloud expertise and capability, allowing it to better leverage data at scale,” i.AI said.

Components of template

rAPId consists of components that combine to form a replicable template for data storage infrastructure in AWS. These include a RESTful API, an infrastructure written in Terraform, a user interface to be deployed as a static site and hosted through a cloud delivery network, a software development kit.

i.AI added: “The solution makes data more accessible for policy and decision makers and enables data professionals to work with big data shared in rAPId.”"
https://blogs.oracle.com/ai-and-datascience/post/bike-sharing-demand-forecasting-oci-ads,Bike sharing demand forecasting using OCI Accelerated Data Science,"The challenge

Predicting bike-sharing demand is difficult. Factors such as weather, holidays, events, and even day-to-day fluctuations in commuter behavior can significantly impact rental patterns. Traditional forecasting methods often fall short, leaving operators grappling with oversupply or undersupply, leading to inefficient resource allocation. This blog post demonstrates the utilization of Oracle Cloud Infrastructure (OCI) Data Science and the Accelerated Data Science (ADS) library to leverage historical bike-sharing data for automated forecasting of future trends, mitigating the necessity for extensive data science or machine learning expertise.

Our example use case uses a dataset on Seoul bike sharing demand from the UCI Machine Learning Repository and the Forecast Operator. The Forecast Operator is a low-code tool for integrating enterprise-grade AI forecasting into your application. Here, we explore a simple use case, but this low-code tool is built for extensibility and developed in partnership with Oracle’s own applications. Learn more about forecasting from our documentation.

Exploratory data analysis

The following table shows the first five rows of the dataset:

Date Rented bike count Hour Temperature(°C) Humidity(%) Wind speed (m/s) Visibility (10m) Dew point temperature(°C) Solar radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons Holiday Functioning day 01/12/17 254 0 -5.2 37 2.2 2000 -17.6 0 0 0 Winter None Yes 01/12/17 204 1 -5.5 38 0.8 2000 -17.6 0 0 0 Winter None Yes 01/12/17 173 2 -6 39 1 2000 -17.7 0 0 0 Winter None Yes 01/12/17 107 3 -6.2 40 0.9 2000 -17.6 0 0 0 Winter None Yes 01/12/17 78 4 -6 36 2.3 2000 -18.6 0 0 0 Winter None Yes

Column Name Description Date Date of the observation (DD/MM/YYYY format) Rented bike count Target variable: Number of bikes rented Hour Hour of the day (0-23) Temperature(°C) Temperature in degrees Celsius Humidity(%) Humidity in percentage Wind speed (m/s) Wind speed in m/s Visibility (10m) Visibility Dew point temperature(°C) Dew point temperature in degrees Celsius Solar radiation (MJ/m2) Solar radiation in MJ/m2 Rainfall(mm) Amount of rainfall (mm) Snowfall (cm) Amount of snowfall (mm) Seasons Contains values : {'Autumn', 'Summer', 'Winter', 'Spring'} Holiday Whether the day is a holiday Functioning day Whether the day is a functioning day

Data preparation

Before we can start with forecasting operator, we need to get the data ready. We split the raw data into two parts: Historical and additional. The historical data stores the target variable, timestamps, and categories for past observations. The additional data contains timestamps, categories, and any other columns with values over the horizon, providing context for the forecast. We can also extract a test dataset to evaluate the accuracy of our predictions.

The following code block declares variables:

# Declaring variables for forecasting operator data = pd.read_csv(""SeoulBikeData.csv"", encoding='unicode_escape') timestamp_col = ""Timestamp"" series_col = ""City"" data[series_col] = ""seoul"" target_col = ""Rented Bike Count"" horizon = 24 ​​​​​

Use the following code block to create the training, testing, and additional datasets:

# Creating historical, additional and test dataset from raw data data[timestamp_col] = pd.to_datetime(data['Date'] + ' ' + data['Hour'].astype(str).str.zfill(2)) data.drop(['Date', 'Hour'], axis=1, inplace=True) data.sort_values(by=timestamp_col,inplace=True) primary_data = data[[timestamp_col, target_col, series_col]] test_data = primary_data.iloc[-horizon:] train_data = primary_data.iloc[:-horizon] additional_data = data.drop([target_col], axis=1, inplace=True)

Writing out the data

# Writing back the datasets train_data_path = ""bike_data_train.csv"" test_data_path = ""bike_data_test.csv"" additional_data_path = ""bike_data_additional.csv"" train_data.to_csv(train_data_path, index=False) test_data.to_csv(test_data_path, index=False) additional_data.to_csv(additional_data_path, index=False)

While this dataset only has data for Seoul, the forecasting operator can leverage city column (target_category_columns) to generate forecasts for multiple cities simultaneously.

Setting up environment

Access the OCI Data Science from the Oracle Cloud Console and initiate a notebook session. Within the session, navigate to the environment explorer and proceed with the installation of the AI Forecasting Operator.

The forecast YAML for ADS operator

The command, ads operator init -t forecast, generates baseline YAML configuration files for forecasting operator. Several configuration files that enables different ways to run your forecasting task, including running the task within containerized Data Science jobs or a Data Science job within a conda runtime environment.

This forecast.yaml file serves as a starting point for your forecasting configuration. However, you need to fill in some key details like:

The name of your datetime column

File paths for your historical, additional, and test datasets

Names of your target category columns and the target column

The forecast horizon you want

Building upon the initial configuration, we created the following forecast.yaml file for our current bike sharing demand forecasting use case:

# Updated config kind: operator spec: datetime_column: name: {timestamp_col} historical_data: url: {train_data_path} additional_data: url: {additioanl_data_path} test_data: url: {test_data_path} output_directory: url: seoul_bike/ model: neuralprophet target_category_columns: [{series_col}] target_column: {target_col} horizon: {horizon} type: forecast version: v1

The model parameter within the forecasting operator offers configurable options for model selection. Specify the desired modeling framework directly by assigning the parameter a supported value such as prophet, arima, neuralprophet, automlx, autots, and auto. Setting this parameter to 'auto' triggers automatic selection of the most suitable framework based on the provided dataset.

Running the job

For local processing, run the command, ads operator run -f forecast.yaml -b local. Alternatively, use the parameter, --backend-config, to launch the forecasting job on other environments, such as ads operator run -f forecast.yaml --backend-config forecast_job_container_backend.yaml.

The report

When the job is completed, you can find the results in the output directory. Check out files like forecast.csv, metrics.csv, report.csv, and report.html for the details. Using the ADS's Forecasting Operator, organizations can easily implement demand forecasting scenarios with minimal effort. The following visualization, extracted from report.html, shows the predicted number of rented bike count (blue line) compared to the actual rentals (green dots) from December 11th onwards.

Explore OCI Data Science

Try a 30-day trial with US$300 in free credits gives you access to OCI Data Science service.

Ready to learn more about the Oracle Cloud Infrastructure Data Science service?"
https://towardsdatascience.com/how-to-make-the-most-out-of-llm-production-data-simulated-user-feedback-843c444febc7,How to Make the Most Out of LLM Production Data: Simulated User Feedback,"A Novel Approach: Simulate User Feedback

In response to these challenges in collecting user feedback, we have developed a novel approach to simulate user feedback using a small sample of user (or internally labeled) feedback data. Specifically, we use metric ensembling and conformal prediction to learn user preferences and use them offline during the development phase. At its core, we learn how users weigh different criteria (e.g., tone, conciseness, etc) and leverage conformal prediction to provide predictions to quantify confidence. This method drastically accelerates LLM app development by providing a way to anticipate how users might react to new features or changes before they are fully implemented.

To evaluate its effectiveness, we compared this approach with the more conventional one of using a single LLM call that assesses different aspects of the response to make a judgment. To compare the two alternatives (the proposed approach vs. the single LLM call), we conducted an experiment using the Unified-Feedback dataset. We used Kendall’s tau, a measure of rank correlation, to compare the rankings produced by our user feedback simulation and the single LLM call approach against the ground truth established by human evaluations. This analysis allows us to assess not only the degree of agreement, but also the order of preference that each method predicts compared to the human rankings.

Our experiment revealed that the user feedback simulation has a correlation of 93% that significantly exceeded that of the single LLM call approach, which attains roughly 70% correlation. This indicates that, in terms of ranking , the simulated user feedback simulation provides a closer approximation to human judgment.

The reason why the simulated user feedback performs better is twofold:

it learns from actual user feedback the importance of different criteria, making the approach custom to your use case

while individual criteria may have appeared in the LLM training set, the complex (and potentially large) set of different criteria likely have not appeared in the training data, making it more difficult for the LLM evaluator to get right.

While single LLM calls can identify major improvements in the pipeline, they fall short of detecting the more frequent, minor enhancements critical in mature pipelines. Simulated user feedback, however, exhibits a high correlation with human judgment, enabling the detection of these incremental advances.

As a side note, while we could have used the data to fine-tune an LLM, this has the typical drawback of requiring more data and not being as interpretable.

In the next section, we will walk through an example on how to create your simulated user feedback.

How It Works

In this section we will show how we can use the open-source library continuous-eval to create simulated user feedback.

Consider a Q&A chatbot application. After deployment, users begin rating responses with thumbs up or down, indicating a need for performance enhancement. For this example we will use the example named correctness in continuous-eval:

dataset = Dataset(example_data_downloader(""correctness""))

# Samples are annotated with ""correct"", ""incorrect"" or ""refuse-to-answer""

# We remove the samples where the LLL refused to answer (i.e., said ""I don't know"")

dataset.filter(lambda x: x[""annotation""] != ""refuse-to-answer"")

dataset.sample(300) # Only for this example: randomly sample 300 examples

As we mentioned, we want to create some custom criteria. We leverage the LLMBasedCustomMetric class to define the Tone and Conciseness metrics. To do so we need to define the metric and provide a scoring rubric.

For the tone:

tone = LLMBasedCustomMetric(

name=""Tone"",

definition=""The Tone/Content Issues metric evaluates the appropriateness and accuracy of the tone and content in responses to specific questions. It focuses on ensuring that the tone is professional and suitable for the context, and that the content accurately addresses the question without unnecessary deviations or inaccuracies. This metric is crucial for maintaining a professional image and ensuring clear, direct communication."",

scoring_rubric=""""""Use the following rubric to assign a score to the answer based on its tone:

- Score 1: The response is inappropriate or inaccurate, with a tone that is either too informal, overly strong, or not suited to the professional context. The content may be irrelevant, incorrect, or fail to directly address the question posed.

- Score 2: The response is mostly appropriate and accurate but may contain minor tone or content issues. The tone is generally professional but may slip into informality or unnecessary strength in places. The content addresses the question but may include minor inaccuracies or unnecessary details.

- Score 3: The response is appropriate and accurate, with a tone that is professional and suited to the context. The content directly and correctly addresses the question without unnecessary deviations or inaccuracies."""""",

scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),

model_parameters={""temperature"": 0},

)

while for conciseness:

conciseness = LLMBasedCustomMetric(

name=""Conciseness"",

definition=""Conciseness in communication refers to the expression of ideas in a clear and straightforward manner, using the fewest possible words without sacrificing clarity or completeness of information. It involves eliminating redundancy, verbosity, and unnecessary details, focusing instead on delivering the essential message efficiently. "",

scoring_rubric=""""""Use the following rubric to assign a score to the answer based on its conciseness:

- Score 1: The answer is overly verbose, containing a significant amount of unnecessary information, repetition, or redundant expressions that do not contribute to the understanding of the topic.

- Score 2: The answer includes some unnecessary details or slightly repetitive information, but the excess does not severely hinder understanding.

- Score 3: The answer is clear, direct, and to the point, with no unnecessary words, details, or repetition."""""",

scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),

model_parameters={""temperature"": 0},

)

We use Tone and Conciseness together with more standard metrics, in particular we will consider the

Answer Correctness (DeterministicAnswerCorrectens and LLMBasedAnswerCorrectness)

Answer Relevance (LLMBasedAnswerRelevance)

Style Consistency (LLMBasedStyleConsistency)

Readability (FleschKincaidReadability)

The next step is to put all the metrics together and specify what field of the dataset should be used to compute the metrics. To do that we can use the SingleModulePipeline

pipeline = SingleModulePipeline(

dataset=dataset,

eval=[

DeterministicAnswerCorrectness().use(

answer=dataset.answer,

ground_truth_answers=dataset.ground_truths,

),

LLMBasedAnswerCorrectness().use(

question=dataset.question,

answer=dataset.answer,

ground_truth_answers=dataset.ground_truths,

),

LLMBasedAnswerRelevance().use(

question=dataset.question, answer=dataset.answer

),

LLMBasedStyleConsistency().use(

answer=dataset.answer, ground_truth_answers=dataset.ground_truths

),

FleschKincaidReadability().use(answer=dataset.answer),

tone.use(

question=dataset.question,

answer=dataset.answer,

ground_truth_answers=dataset.ground_truths,

),

conciseness.use(

question=dataset.question,

answer=dataset.answer,

ground_truth_answers=dataset.ground_truths,

),

],

)

and run all the metrics using the EvaluationManager

eval_manager = EvaluationManager(pipeline)

# The dataset already contains the model output so we just set the evaluation results

eval_manager.evaluation.results = dataset.data

eval_manager.run_metrics() # Note: there is no progress bar, it might take a few minutes

The next step is to train simulated user feedback predictor

datasplit = DataSplit(

X=eval_manager.metrics.to_pandas(),

y=map(lambda x: 1 if x == ""correct"" else 0, dataset[""annotation""]),

split_ratios=SplitRatios(train=0.6, test=0.2, calibration=0.2),

)

# We use the train and calibration sets to train the classifier

predictor = EnsembleMetric(training=datasplit.train, calibration=datasplit.calibration)

This simulated user feedback predictor is able to correctly predict the human feedback in the test split 96.67% of the time.

We can leverage the proposed approach to better understand what is important to the user. Below is the learned importance of every metric by the simulated user feedback predictor.

Looking at the plot, we see that Correctness (including token overlap, which is another measure for correctness) and Relevance to the question are the most important predictors of user preference. But the user also weighs tone and style consistency into the decision. At the same time, we can see that conciseness and readability are not as important. Reviewing this graph provides valuable insight into user preferences, giving a clear indication of what elements are essential and what can be adjusted if compromises need to be made.

Wrapping Up

Collecting user feedback is challenging, yet it is the most important information for developers of large language models (LLMs). By simulating user feedback during offline testing, we significantly reduces the time it takes for feedback to travel from the field back to developers, while maintaining positive user relationships.

In practice, our approach has proven to closely mirror actual human responses, outperforming traditional methods that rely on isolated LLM responses. This strategy allows for the incremental improvement of generative AI applications, fostering continuous refinement and greater congruence with what users expect.

—

Note: We will soon publish a research paper with more details on this methodology. Stay tuned!"
https://towardsdatascience.com/the-definitive-guide-to-structured-data-parsing-with-openai-gpt3-5-0e5ea0e52637,The Definitive Guide to Structured Data Parsing with OpenAI GPT3.5,"Systematically comparing Instructor, Fructose, and Langchain for three complex real-world structured data parsing tasks.

Marie Stephen Leo

·

Follow

Published in

Towards Data Science

·

7 min read

·

1 day ago

--

Parsing structured data from Large Language Models (LLMs) can be frustrating for anything beyond toy problems. Yet, reliably parsing LLM outputs into…"
https://www.globalgovernmentforum.com/challenge-limits-in-the-civil-service-five-minutes-with-10-downing-streets-chief-analyst-laura-gilbert/,Five minutes with 10 Downing Street’s chief analyst Laura Gilbert,"Laura Gilbert, chief analyst and director of data science at the UK’s 10 Downing Street, discusses civil service possibilities and reforms, and inspiration from governments overseas.

This is part of a ‘Five minutes’ series featuring speakers from this week’s Global Government Forum GovernmentDX event (Washington, D.C., April 18-19). During the conference, Gilbert will participate in a session on how government can make the most of analytics and AI.

What drew you to a career in the civil service?

I have a strong need to find meaning in my work. I work very long hours, I work very hard, I need to know it has purpose – and delivering better outcomes for the public whilst protecting taxpayer resources feels important to me.

What advice would you give someone starting out in the civil service?

Don’t accept what you are told about what is possible and not possible. Many more things are possible, even in civil service, than people think.

Read more: Why a tech industry veteran joined the civil service: Five minutes with GovTech Singapore’s Chang Sau Sheong

If you could introduce one civil service reform, what would it be?

Outcomes-based performance management and the ability to exit non-performers efficiently.

Which civil servant – past or present – do you most admire and why?

Dame Emily Lawson, a woman who really knows how to get things done!

Which country’s civil service or which government department or agency are you most inspired by and why?

I think everyone is very impressed by Denmark’s e-government and Estonia’s digital services. They are really trailblazing when it comes to digital services for the public.

Read more: The battle of the data strategies

Can you name one lesson or idea from abroad that’s helped you and your colleagues?

I saw an incredible initiative in Nepal – an AI-based smear test, where you would be assessed and treated in one appointment, in a mobile testing facility. A much better and much cheaper solution than the one we employ!

What is your dream holiday destination?

Langkawi. I’ve been once. They have six different types of flying animal (that don’t usually fly), which I love – flying snakes, flying frogs, flying fish, flying squirrels, flying lizards and flying lemurs (colugos). Also king cobras, tarantulas, monitor lizards, civets… it’s a wildlife paradise.

What was your first car?

A seriously tinny black Fiat Panda. It weighed very little though so you could really take off at traffic lights."
https://towardsdatascience.com/leveraging-python-pint-units-handler-package-part-1-716a13e96b59,Leveraging Python Pint Units Handler Package — Part 1,"If you work in the engineering or science fields, or even if you are someone involved in supply chain operations, environmental sustainability, or whatever field that uses physical quantities like time, mass, and length, you have faced situations where you need to operate and manipulate…"
https://towardsdatascience.com/the-limitations-and-advantages-of-retrieval-augmented-generation-rag-9ec9b4ae3729,The Practical Limitations and Advantages of Retrieval Augmented Generation (RAG),"The Value of RAG

Imagine RAG as highly intelligent librarian who can sift through a digital library in seconds to answer your questions. Sometimes the librarian finds relevant and useful information to answer your questions , but other times they miss the mark.

Let’s explore situations in which RAG excels and those in which it falls short. In a future work, I will explore a series of approaches that can be used individually or in combination to improve RAGs capabilities — which will support better responses when used with a language model.

Where RAG Falls Short

Even the most intelligent librarian has their challenges , some of which include the ability to reason iteratively, ensuring that they are retrieving the most useful documents, and ensure that the information they are sourcing from is relevant and unbiased.

Piecing Together the Puzzle with Iterative Reasoning: One of the key limitations of current RAG is its lack of iterative reasoning capabilities. RAG is unable to fully understand whether the data that is being retrieved is the most relevant information the language model needs to effectively solve the problem.

For example, if you were to pose a question such as “What does the impact of new environmental regulations passed in 2024 have on my latest white paper?” a RAG-enabled system would attempt to retrieve the data most semantically similar to the query. It might return the top X documents that have information on new policies, but are they the relevant policies for the specific paper the user is referencing?

As humans, we would approach this problem with reasoning skills. We would first read the white paper to understand its content and then determine what type of environmental policies best apply. Then based on that knowledge we would perform a search for those white papers. This iterative reasoning process — understanding the problem, formulating a more targeted search strategy, and then retrieving the most useful information — is a capability that current RAG implementations lack.

Organization Matters: The performance and effectiveness of RAG is heavily dependent on the organization and structure of the underlying data it is accessing. The ability for the retrieval algorithm to identify and surface the most useful documents is greatly influenced by how that information is cataloged and stored as well as how semantically similar the query is to the data retrieved.

In our library analogy, imagine a scenario where 500 books on various subjects are simply placed haphazardly on a single shelf, without any categorization or tagging. Trying to find the most relevant resources to answer a specific query would be a feat. You may stumble across some potentially useful books, but have no reliable way to assess which ones contain the most pertinent information. If those same 500 books were organized by genre, with clear metadata and subject tags, the retrieval process becomes significantly more efficient and effective. Rather than blindly scanning the entire shelf, the RAG implementation could quickly zero in on the most relevant section(s).

The same principles apply to how data is stored and indexed for RAG implementations in real-world applications. If the underlying datasets lack coherent organization, categorization, and metadata, the retrieval algorithms will struggle to identify the most valuable information. Ensuring data is properly structured, cataloged, and accessible is a critical.

The Good, the Bad, and the Biased : The quality of the data retrieved by a RAG implementation is only as good as the data it has access to. If the information in the underlying source systems, be it databases, online file storage, or other data repositories, contains outdated, incomplete, or biased content, the RAG implementation will have no way to discern this. It will simply retrieve and pass along this flawed information to the language model responsible for generating the final output.

Where RAG Models Shine

Accessing Domain Specific and Confidential Information: One of the key advantages of RAG is the ability to leverage domain-specific and even confidential information that may not be included in a language model’s standard training data. This can be particularly beneficial for organizations working on proprietary, cutting-edge research and projects. For example, if a company is conducting groundbreaking research in quantum computing that has not yet been publicly released, a RAG implementation could be granted access to these internal data sources. This would allow the language model to access specialized knowledge to engage in discussions about the company’s latest developments, without needing to be trained on that confidential information.

However, exposing sensitive, internal data to externally hosted language models (such as GPT, LLAMA, etc.) is not risk free. Organizations must exercise due diligence to ensure proper data security measures are in place to protect their intellectual property and confidential information.

Bringing the Latest News to Your Conversation: One of the key advantages of RAG is its ability to provide language models with access to the most up-to-date information, going beyond the fixed cutoff date of the language model’s original training data.If a language model were to rely solely on its inherent knowledge, its information would be limited to what was available at the time it was trained.

RAG implementations, on the other hand, can be integrated with live data sources such as the internet, constantly updating databases, news feeds, etc. This allows the language model to utilize current information when generating responses.

Conclusion

Retrieval Augmented Generation (RAG) is a powerful technique that can enhance language models by providing access to a wealth of information beyond their initial training. However, it is important to be aware of the limitations of RAG, such as the need for iterative reasoning, the importance of well organized data, and the potential for biased or outdated information. In a future work, I will explore a series of approaches to improve the capabilities of RAG — enhancing the quality of responses generated by a language model."
https://www.simplilearn.com/data-analytics-books-article,Best Data Analytics Books 2024: Must-Read Books,"Top Data Analytics Books of 2024

Books for data analysts are great ways for professionals who aspire to work in data analysis to learn about subjects, developments, and useful skills.

Here is a collection of the best data analytics books, from fundamentals to specifics, such as big data, AI, statistical programming languages, etc.

Storytelling with Data: A Data Visualization Guide for Business Professionals - Cole Nussbaum Knaflic, 2015

Cole Nussbaummer Knaflic, the CEO and founder of Storytelling With Data, wrote this remarkable data analyst book.

SWD is a book that emphasizes the importance of data storytelling in data analysis. Instead of just placing charts on report pages, data analysts should carefully choose the right chart and create a compelling story to engage their audience.

This piece is one of the must-read data analytics books for beginners, and it provides six useful steps for data storytelling.

Big Data: A Revolution That Will Transform How We Live, Work, and Think - Viktor Mayer-Schönberger, 2013

Viktor Mayer and Schönberger, domain experts, discuss the impact of big data on our world. Their book also focuses on the potential positive or negative changes in big data.

This book offers a good understanding of data analytics and its impact on various industries. It prepares readers for the big data revolution that is about to come. The book digs into the broader consequences of big data on societal aspects. It highlights the potential risks associated with digital technology. The book also provides a theoretical overview of big data's importance in various life stages.

Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython - Wes McKinney, 2011

The author of the Pandas library's comprehensive book Python for Data Analysis teaches learners the fundamentals of using Python for data manipulation, processing, cleaning, and crunching. Real-world case studies are covered, along with an introduction to data science tools and instructions on how to use Matplotlib to build useful visualizations. Other techniques include loading, cleaning, manipulating, combining, and reshaping data.

Naked Statistics: Stripping the Dread from the Data - Charles Wheelan, 2012

The field of statistics is rapidly evolving into a ""sexy"" discipline, with applications in various fields such as politics, game shows, and medical research. Charles Wheelan's book, Naked Statistics, focuses on the intuition behind statistical analysis, explaining key concepts like inference, correlation, and regression analysis. The book also highlights how biased parties can manipulate data and how creative researchers use natural experiment data to tackle complex questions. It is a valuable resource for those who missed Stats 101.

Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking - Tom Fawcett, 2013

This book, written by Foster Provost and Tom Fawcett, introduces the fundamental concepts of data science and data-analytic thinking. This data analytics book enables readers to extract valuable knowledge and business value from data. It educates readers on how to use data science techniques to help business decision-making and how to think analytically about data.

Business UnIntelligence: Insight and Innovation Beyond Analytics and Big Data - Barry Devlin, 2013

This book examines business intelligence's past, present, and future while stressing the advantages and disadvantages of conventional methods. Dr. Devlin discusses how big data and analytics have revolutionized business intelligence today, highlighting tried-and-true methods and providing insights into how people, processes, and information interact to create competitive advantage and propel company success. Additionally, he suggests new frameworks and models for companies to enhance their future.

The Hundred-page Machine Learning Book - Andriy Burkov, 2019

This book offers a succinct introduction to machine learning in just 140 pages, making it appropriate for readers with no prior programming or statistical knowledge. Neural networks, cluster analysis, and supervised and unsupervised learning are among the important ideas covered. The book is short enough to read in one sitting, and the companion wiki provides resources and suggestions for further reading.

Artificial Intelligence: A Guide for Thinking Humans - Melanie Mitchell, 2019

Melanie Mitchell, a computer scientist, wrote this book to help us explore the historical background and people behind artificial intelligence. The book specifically draws attention to difficult ideas like neural networks, computer vision models, and NLP. It helps readers who do not require a thorough understanding of AI understand how AI affects data analytics.

Developing Analytic Talent: Becoming a Data Scientist - Vincent Granville, 2014

With his background in big data, business analytics, and predictive modeling, Granville provides helpful information in his handbook on data science and data scientists. The book discusses the significance of key information for data scientists in big data organizations. It is divided into three sections that address technological applications, case studies, tutorials, career opportunities, and the relationship between data science and other fields.

Educating decision-makers about specialized solutions and their applications also aids in the development of stronger analytics teams. Granville's more than two decades of industrial experience offer quick suggestions for those wishing to build a data science firm.

Learning R: A Step-by-Step Function Guide to Data Analysis - Richard Cotton, 2013

This book offers a step-by-step introduction to the R language, making it an invaluable tool for non-technical learners. It covers environments, looping constructions, packages, and data structures. The book then covers the data analysis processes, including loading, cleaning, and converting data. The second section is a priceless resource for individuals unfamiliar with programming languages, as it offers further insight into exploratory analysis and modeling.

Weapons of Math Destruction - Cathy O'Neil, 2016

Cathy O'Neil's book on data bias highlights the importance of using big data responsibly. It also discusses the consequences of machines making decisions about our lives and how algorithms often reinforce discrimination. Despite disagreements, the insights are crucial for those new to data science, ensuring future data is used for the benefit of all, not just the privileged.

Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data, 2014

Big Data analytics offers deeper insights and supports businesses by integrating real-time data feeds and queries. This book, by EMC Education Services, introduces key techniques and tools for Big Data analytics, guiding readers from basic methods to advanced methods like classification, regression analysis, clustering time series, and text analysis. It is suitable for business analysts, database professionals, and college graduates interested in data science or data analysis as a career field.

Too Big to Ignore: The Business Case for Big Data - Phil Simon, 2013

Phil Simon's book Too Big to Ignore: The Business Case for Big Data explores businesses' and local governments' use of big data. It features case studies and quotes from professionals worldwide, providing valuable insights on turning data into intelligence and making it actionable.

The Elements of Statistical Learning - Trevor Hastie, 2001

This book thoroughly introduces statistical ideas in various industries, including marketing, biology, finance, and medicine. It employs color pictures for examples and prioritizes concepts over mathematical formulas. Classification trees, neural networks, support vector machines, boosting, and other subjects related to supervised and unsupervised learning are covered in this book, which is an invaluable tool for statisticians and data mining players.

Numsense! Data Science for the Layman: No Math Added - Kenneth Soo, 2017

This book offers a comprehensive introduction to data science, suitable for non-technical individuals. It provides clear language and visual explanations for algorithms, avoiding complex math. It is valuable for data scientists and beginners as a refresher for communicating work to business partners. The book's algorithm explanations are useful for field communication.

Head First Data Analysis: A Learner's Guide to Big Numbers, Statistics, and Good Decisions - Michael Milton, 2009

Head First Data Analysis is a book that teaches how to manage and analyze various types of data, including product development, marketing, sales, and entrepreneurship. It provides a unique approach to learning how to convert raw data into a vital business tool. The book uses the latest research in cognitive science and learning theory to create a visually rich format that caters to the brain's workings, making it an efficient way to convert raw data into a valuable business tool.

SQL QuickStart Guide: The Simplified Beginner's Guide to Managing, Analyzing, and Manipulating Data With SQL - Walter Shields, 2015

This book includes a thorough introduction to Structured Query Language (SQL), digital resources such as workbooks and reference guides, and an example database and SQL browser software. It addresses subjects like relational database communication, database structures, important SQL queries, and marketing SQL expertise to prospective employers. The book also offers suggestions on marketing newly acquired SQL abilities to possible employers.

Microsoft Excel Data Analysis and Business Modeling - Wayne L. Winston, 2004

Wayne Winston, a renowned consultant and business professor, has been teaching clients in the corporate sector and MBA students how to use Microsoft Excel for data analysis, modeling, and decision-making for over a decade. This practical guide offers real-world examples and learn-by-doing exercises to enhance data analysis and modeling expertise. The book is available as a searchable eBook and CD file for download."
https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61,Advanced Retriever Techniques to Improve Your RAGs,"Master Advanced Information Retrieval: Cutting-edge Techniques to Optimize the Selection of Relevant Documents with Langchain to Create Excellent RAGs

Damian Gil

·

Follow

Published in

Towards Data Science

·

18 min read

·

7 hours ago

--

Content Table

· Introduction

· Vectore Store Creation

· Method: Naive Retriever

· Method: Parent Document Retriever

· Method: Self Query Retriever ∘ Query Constructor

∘ Query Translater

· Method: Contextual Compression Retriever (Reranking)

· Conclusion

Introduction

Let’s briefly remember what the 3 acronyms that make up the word RAG mean:

Retrieval: The main objective of a RAG is to collect the most relevant documents/chunks regarding the query.

Augmented: Create a well-structured prompt so that when the call is made to the LLM, it knows perfectly what its purpose is, what the context is and how it should respond.

Generation: This is where the LLM comes into play. When the model is given good context (provided by the “Retrieval” step) and has clear instructions (provided by the “Augmented” step), it will generate high-value responses for the user.

As we can see, the generation of the response to a user’s query (If we apply a RAG for the purpose of Q&A), depends directly on how well we have built the “Augmented” and especially the “Retrieval”.

In this article we are going to focus exclusively on the “Retrieval” part. In this important process of returning the most relevant documents, the concept of vector store appears.

To create these retrievals, we will use the Langchain library.

The vectore store is nothing more than a vector database, which stores documents in vector format. This vector representation comes from the use of transformers. I’m not saying something you don’t know at the moment.

It is clear that the more robust and complete this vector store is, the better retriever we can run. We already know that the creation of this database is an art in itself. Depending on the size of the chunks or the embedding model we use, our RAG will be better or worse.

I make a clarification here:

In this post we are NOT going to discuss how to create this vector store.

In this post we are going to discuss some of the techniques used to retrieve relevant documents.

Since a picture is worth a thousand words, I suggest you take a look at the following:

Therefore, I reiterate that in this post we are going to deeply study one of the many important steps in creating a good RAG tool. The “Retrieve” step is key since it directly improves the context that the LLM has when generating a response.

The methods we will study are:

Naive Retriever

Parent Document Retriever

Self-Query Retriever

Contextual Compression Retriever (Reranking)

You can find the project with the notebooks here. And you can also take a look at my github:

damiangilgonzalez1995 - Overview

Passionate about data, I transitioned from physics to data science. Worked at Telefonica, HP, and now CTO at…

github.com

Vectore Store Creation

To expose these methods, a practical use case will be carried out to improve the explanation. Therefore, we are going to create a RAG about reviews of the John Wick movies.

So that the reader can follow each step of this post, they can access the repository that I have created. In it you will find the code for each of the methods, in addition to the documents used to create the vector store. The jupyter notebook in charge of this task can be found in the git repository, and is the file called “0_create_vectore_db.ipynb”.

In relation to the data source of our RAG, there are 4 csv’s each corresponding to the reviews obtained for each of the films in the John Wick saga. The files contain the following information:

As you can see, the “Review” field will be the target of our retriever. The other fields being important to store as metadata:

Movie_Title

Review_Date

Review_Title

Review_Url

Author

Rating

To read and convert each row of our files into the “Document” format, we execute the following code:

from langchain_community.document_loaders.csv_loader import CSVLoader

from datetime import datetime, timedelta

documents = []

for i in range(1, 4):

loader = CSVLoader(

encoding=""utf8"",

file_path=f""data/john_wick_{i}.csv"",

metadata_columns=[""Review_Date"", ""Review_Title"", ""Review_Url"", ""Author"", ""Rating""]

)

movie_docs = loader.load()

for doc in movie_docs:

# We add metadate about the number of the movi

doc.metadata[""Movie_Title""] = f""John Wick {i}""

# convert ""Rating"" to an `int`, if no rating is provided - None

doc.metadata[""Rating""] = int(doc.metadata[""Rating""]) if doc.metadata[""Rating""] else 5

documents.extend(movie_docs)

We already have our documents in “Document” format:

print(documents[0])

Document(page_content="": 0\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity."", metadata={'source': 'data/john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 4, 8, 11, 49, 47, 92560)})

We only have to create a vector database (Vectore Store) locally. For this, I have used Chroma. Also keep in mind that it is necessary to use an embedding model, which will transform our documents into vector format for storage. Everything mentioned can be seen in the following piece of code:

from langchain_community.vectorstores import Chroma

from langchain_openai import OpenAIEmbeddings

import os

from dotenv import load_dotenv

load_dotenv()

os.environ[""OPENAI_API_KEY""] = os.getenv('OPENAI_KEY')

embeddings = OpenAIEmbeddings(model=""text-embedding-3-small"")

db = Chroma.from_documents(documents=documents, embedding=embeddings, collection_name=""doc_jonhWick"", persist_directory=""./jonhWick_db"")

This will create a database on our premises called “JonhWick_db”. This will be the database that our RAG will use and from where our retriever will obtain the most relevant documents regarding the user’s queries.

Now is the time to present the different methods for creating a retriever.

Method: Naive Retriever

Code in 1_naive_retriever.ipynb file.

This method is the simplest, in fact its name indicates it. We use this adjective to identify this method for the simple reason that when entering the query into our database, we hope (naively) that it will return the most relevant documents/chunks.

Basically what happens is that we encode the user query with the same transformer with which we created the vector store. Once its vector representation is obtained, we calculate the similarity by calculating the cosine, the distance, etc.

And we collect the top K documents closest/similar to the query.

The flow of this type of retriever can be seen in the following image:

Keeping the scheme in mind, let’s see how all this looks in the code. We read the database:

from langchain_community.vectorstores import Chroma

from langchain_openai import OpenAIEmbeddings

import os

from dotenv import load_dotenv

load_dotenv()

os.environ[""OPENAI_API_KEY""] = os.getenv('OPENAI_KEY')

embeddings = OpenAIEmbeddings(model=""text-embedding-3-small"")

vectordb= Chroma(persist_directory=""./jonhWick_db"",

embedding_function=embeddings,

collection_name=""doc_jonhWick"")pyth

And we create our retriever. We can configure the similarity calculation method, in addition to other parameters.

Retriever

# Specifying top k

naive_retriever = vectordb.as_retriever(search_kwargs={ ""k"" : 10})

# Similarity score threshold retrieval

# naive_retriever = db.as_retriever(search_kwargs={""score_threshold"": 0.8}, search_type=""similarity_score_threshold"")

# Maximum marginal relevance retrieval

# naive_retriever = db.as_retriever(search_type=""mmr"")

Actually, we have already created our “Naive Retriever”, but to see how it works, we will create the complete RAG that we remember is composed of the following components:

R (Retrieval): Done

A (Augmented): Not yet

G (Generation): Not yet

Augmented & Generation

from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import ChatOpenAI

# Augmented

TEMPLATE = """"""\

You are happy assistant. Use the context provided below to answer the question.

If you do not know the answer, or are unsure, say you don't know.

Query:

{question}

Context:

{context}

""""""

rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)

# Generation

chat_model = ChatOpenAI()

We already have the 3 components of our RAG. All that remains is to assemble them, and for this we will use the langchain chains to create a RAG.

I don’t know if you know the language created by langchain for creating chains in a more efficient way. This language is known as LCEL (LangChain Expression Language). If you are new to this way of creating chains in langchain, I leave you a very good tutorial here:

Finally, we create our RAG using Langchain’s own chain creation language (LCEL):

from langchain_core.runnables import RunnablePassthrough, RunnableParallel

from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser

setup_and_retrieval = RunnableParallel({""question"": RunnablePassthrough(), ""context"": naive_retriever })

output_parser = StrOutputParser()

naive_retrieval_chain = setup_and_retrieval

| rag_prompt

| chat_model

| output_parser

naive_retrieval_chain.invoke( ""Did people generally like John Wick?"")

# response: 'Yes, people generally liked John Wick.'

This is the simplest way to create a chain for a RAG. In the Jupyter notebook you can find the same chain but more robust. Since I don’t want us to get lost on this topic now, I have only shown the simplest form. Also so that we understand what is happening in the code above, I have created this very clarifying diagram:

Great, we’re done creating our Naive RAG. Let’s move on to the next method.

Method: Parent Document Retriever

Code in 2_parent_document_retriever.ipynb file.

Imagine that we have created a RAG to recognize possible diseases by introducing some of their symptoms in the consultation. In the event that we have a Naive RAG, we may collect a series of possible diseases that only coincide in one or two symptoms, leaving our tool in a bit of a bad place.

This is an ideal case to use Parent Doc Retriever. And the type of technique consists of cutting large chunks (parent chunk) into even smaller pieces (child chunk). By having small chunks, the information they contain is more concentrated and therefore, its informative value is not diluted between paragraphs of text.

There is a small problem in all this:

If we want to be precise in searching for the most relevant documents, we need to break our documents into small chunks.

But it is also very important to provide good context to the LLM, which is achieved by providing larger chunks.

What has been said can be seen in the following image:

It seems that there is no way out of the problem, since when we increase the precision, the context is reduced, and vice versa. This is when this method appears that will solve our lives.

The main idea is to further chop the large chunks (Parent chunks/documents) into smaller chunks (Child Chunks/documents). Once this is done, perform the search for the most relevant top K documents with the child chunks, and return the parents chunks to which the top K child document belongs.

We already have the main idea, now let’s get it down to earth. The best way to explain it is step by step:

Obtain the documents and create the large chunks (Parent chunks)

Perform a split of each of the parent chunks for the growth of the child chunks.

Save the child chunks (Vector Representation) in the Vector Store.

Save the parent chunks in memory (We do not need to create their vector representation).

What has been said can be seen in the following image:

This may seem very complex to create, since we have to create a new database with the small chunks, save the parent chunks in memory. Additionally, know which parent chunk each child chunk belongs to. Thank goodness Langchain exists and the way to build it is super simple.

Surely you have come to the conclusion that it is necessary to create a new vector store for this method. Furthermore, in the case of reviews of the John Wick movies, such as the data source with CSV files, it is not necessary to perform the first split (parent chunks). This is because we can consider each row of our csv files to be a chunk in itself.

Overall, let’s visualize the following image that reflects how this method works:

Going to code it is represented as follows:

from langchain.retrievers import ParentDocumentRetriever

from langchain.storage import InMemoryStore

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_openai import OpenAIEmbeddings

from langchain_community.vectorstores import Chroma

# documents = Read csv files. Check jupyter notebook for more details

parent_docs = documents

# Embedding Model

embeddings = OpenAIEmbeddings(model=""text-embedding-3-small"")

# Splitters

child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)

# We don't need a parent splitter because the data cames from CSV file, and each row is a parent doc.

# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)

# Stores

store = InMemoryStore()

vectorstore = Chroma(embedding_function=embeddings, collection_name=""fullDoc"", persist_directory=""./JohnWick_db_parentsRD"")

parent_document_retriever = ParentDocumentRetriever(

vectorstore=vectorstore,

docstore=store,

child_splitter=child_splitter,

# parent_splitter =parent_splitter

)

Something intuitive about what happens here is that the number of chunks in the vector store (number of child chunks) should be much higher than the number of documents stored in memory (parent chunks). With the following code we can check it:

print(f""Number of parent chunks is: {len(list(store.yield_keys()))}"")

print(f""Number of child chunks is: {len(parent_document_retriever.vectorstore.get()['ids'])}"")

'''

Number of parent chunks is: 75

Number of child chunks is: 3701

'''

Great, we would already have our Parent Document Retriever, we just need to create our RAG based on this retriever and that would be it. It would be done exactly the same as in the previous method. I attach the code for creating the chain in langchain. To see more details, take a look at the jupyter notebook.

setup_and_retrieval = RunnableParallel({""question"": RunnablePassthrough(), ""context"": parent_document_retriever })

output_parser = StrOutputParser()

parent_retrieval_chain = setup_and_retrieval | rag_prompt | chat_model | output_parser

Note that it is exactly the same as in the previous case, only with the small difference that in the “setup_and_retrieval” variable, we configure that we want to use our “parent_document_retriever”, instead of the “naive_retriever”.

Method: Self Query Retriever

Code in 3_self_query_retriever.ipynb file.

This is possibly one of the most optimal methods to improve the efficiency of our retriever.

Its main feature is that it is capable of performing searches in the vector store, applying filters based on the metadata.

We know that when we apply a “Naive retrieval”, we are calculating the similarity of all the chunks of the vector database with the query. The more chunks the vector store has, the more similarity calculations will have to be done. Now, imagine being able to do a prior filter based on the metadata, and after selecting the chunks that meet the conditions imposed in relation to the metadata, calculate similarities. This can drastically reduce computational and time cost.

Let’s look at a use case to fully understand when to apply this type of retreival.

Let’s imagine that we have stored in our vector database a large number of experiences and leisure offers (Ex: surf classes, zip line, gastronomic route, etc.). The description of the experience is what we have encoded, using our embedding model. Additionally, each offer has 3 key values or metadata: Date, price and place.

Let’s imagine that a user is looking for an experience of this style: An experience in nature, that is for the whole family and safe. Furthermore, the price must be less than $50 and the place is California.

Something is clear here

WE DO NOT WANT YOU TO RETURN US ACTIVITY/EXPERIENCES THAT DO NOT MEET THE PRICE OR PLACE THAT THE USER REQUESTS.

Therefore, it does not make sense to calculate similarities with chunks/experiences that do not comply with the metadata filter.

This case is ideal for applying Self Query Retriever. What this type of retriever allows us is to perform a first filter through the metadata, and then perform the similarity calculation between the chunks that meet the metadata requirements and the user input.

This technique can be summarized in two very specific steps:

Query Constructor

Query Translater

Query Constructor

The objective of the step called “Query Constructor” is to create the appropriate query and filters according to the user input.

Who is in charge of applying the corresponding filters and how do you know what they are?

For this we are going to use an LLM. This LLM will have to be able to decide which filters to apply and when. We will also have to explain beforehand what the metadata is and what each of them means. In short, the prompt must contain 3 key points:

Context: Personality, how you should act, output format, etc.

Metadata: Information about available metadata.

Query: The user’s query/input/question.

The output generated by the LLM cannot be directly entered into the database. Therefore, the so-called “Query Translater” is needed.

Query Translater

This is a module in charge of translating the output of the LLM (Query Constructor) into the appropriate format to perform the query. Depending on the vector database you use, you will have to use one or the other. In my case I used Chroma db, therefore, I need a translator focused on this database. Luckily, Langchain has specific database translators for almost all of them.

As you may have already noticed, I am a big fan of diagrams. Let’s look at the following which provides quite a bit of clarity to the matter:

Regarding the previous image, we see that everything begins with the user’s query. We create the prompt that contains the 3 key fields and is introduced to the LLM that generates a response with two key fields: “Query” and “Filter”. This is fed into the query translator which translates these two fields into the correct format needed by Chroma DB. Performs the query and returns the most relevant documents based on the user’s initial question.

Something to emphasize is that the query entered by the user does not have to be the same as the one entered into the database. In the diagram shown, it can be seen that the LLM, taking into account the available metadata and the user’s question, detects that it can create a filter with the “Rating” metadata. It also creates a new query based on the user’s query.

Let’s look at all this in code. As I have explained, it is very important to provide the LLM with a detailed description of the metadata available in the vector store. This translates into the following piece of code:

from langchain.chains.query_constructor.base import AttributeInfo

from langchain.retrievers.self_query.base import SelfQueryRetriever

from langchain_openai import ChatOpenAI

from langchain.retrievers.self_query.chroma import ChromaTranslator

metadata_field_info = [

AttributeInfo(

name=""Movie_Title"",

description=""The title of the movie"",

type=""string"",

),

AttributeInfo(

name=""Review_Date"",

description=""The date of the review"",

type=""string"",

),

AttributeInfo(

name=""Review_Title"",

description=""The title of the review"",

type=""string"",

),

AttributeInfo(

name=""Review_Url"",

description=""The URL of the review"",

type=""string"",

),

AttributeInfo(

name=""Author"",

description=""The author of the review"",

type=""string"",

),

AttributeInfo(

name=""Rating"",

description=""A 1 to 10 rating for the movie"",

type=""integer"",

)

]

To define our retrieval we must define the following points:

The LLM to use

The embedding model to be used

The vector basis that is accessed

A description of what information can be found in the

documents of this vector base.

The metadata description

The Query translator you want to use

Let’s see what it looks like in code:

document_content_desription = ""A review of the Jonh Wick movie.""

embeddings = OpenAIEmbeddings(model=""text-embedding-3-small"")

chat_model = ChatOpenAI()

self_query_retriever = SelfQueryRetriever.from_llm(

llm=ChatOpenAI(temperature=0),

vectorstore =vectordb,

document_contents = document_content_desription,

metadata_field_info =metadata_field_info,

verbose = True,

structured_query_translator = ChromaTranslator()

)

Let’s see with a very clear example how we have greatly improved our RAG by using this type of retriever. First we use a naive retriever and then a self query retriever.

Question = ""Make a summary of the reviews that talk about John Wick 3 and have a score higher than 7""

response = naive_retrieval_chain.invoke(Question)

print(response)

'''

I don't know the answer.

'''

------------------------------------------------------------------------

response = self_retrieval_chain.invoke(Question)

print(response)

'''

John Wick: Chapter 3 - Parabellum is quite literally

about consequences, dealing with the fallout of John's...

'''

As we can see, there is a notable improvement.

Method: Contextual Compression Retriever (Reranking)

Code in 4_contextual_compression_retriever(reranking).ipynb file.

Context Windows: The more documents we obtain from the vectore store, the more information the LLM will have to give a good answer.

Recall: The more documents are retrieved from the vector store, the probability of obtaining irrelevant chunks is greater and therefore, the recall decreases (Not a good thing).

There seems to be no solution for this problem. When we increase one of the metrics, the other seems destined to decrease. Are we sure about that?

This is when this technique, compression retriever, is presented, focusing on the reranking technique. Let’s say that this technique consists of two very different steps:

Step 1: Get a good amount of relevant docs based on the input/question. Normally we set the most relevant K.

Step 2: Recalculate which of these documents are really relevant. discarding the other documents that are not really useful (Compression).

For the first step, what is known as Bi-Encoder is used, which is nothing more than what we usually use to make a basic RAG. Vectorize our documents. vectorize the query and calculate the similarity with any metric of our choice.

The second step is something different from what we are used to seeing. This recalculation/reranking is executed by the reranking model or cross-encoder.

These models expect two documents/texts as input, returning a similarity score between the pair.

If one of these two inputs is the query and the other is a chunk, we can calculate the similarity between the two.

These two methods can be displayed as follows:

You will have realized that the two methods in the end provide the same result, a metric that reflects the similarity between two texts. And this is totally true, but there is a key feature:

The result returned by the cross encoder is much more reliable than with the Bi-encoder

Okay, it works better, then, because we don’t use it directly with all chunks, instead of just the top K chunks. Because it would be terribly expensive in time and money/computation. For this reason, we make a first filter of the chunks closest in similarity to the query, reducing the use of the reranking model to only K times.

A good question would be where to find the Cross-Encoder models? We are lucky that there are open source models that we can find in HuggingFace, but for the practical case of this post we are going to use the model made available by the company Cohere.

Cohere | The leading AI platform for enterprise

Cohere provides industry-leading large language models (LLMs) and RAG capabilities tailored to meet the needs of…

cohere.com

To better understand the architecture of this method, let’s look at a visual example.

The image shows the steps:

1º) We obtain the query, which we encode in its vector form with a transformer and we introduce it into the vector base.

2º) Collect the documents most similar to the query from our database. We can use any retriever method.

3º) Next we use the Cohere cross-encoder model. In the example in the image, this model will be used a total of 4 times. Remember that the input of this model will be the query and a document/chunk, to collect the similarity of these two texts.

4º) The 4 calls have been made to this model in the previous step and 4 new values (between 0 and 1) of the similarity between the query and each of the documents have been obtained. As can be seen, the chunk number 1 obtained in the previous steps, after the reranking, is now in 4th place.

5º) We add the first 3 chunks most relevant to the context.

Returning again to the computational cost and time, if the cross-encoders were applied directly, think that with each new query, the similarity of the query with each of the documents should be calculated. Something that is not optimal at all.

On the other hand, using Bi-Encoders, the vector representation of the documents is the same for each new query.

We then have a much superior method that is expensive to execute, and on the other hand, another method that works well but does not have a large computational cost with each new query. All this ends with the conclusion of unifying these two methods for a better RAG. And this is known as the Contextual Compression with reranking method.

Let’s move on to the code part. Let’s remember that this method uses a retreiver, which in our case will be a Naive Retriever:

naive_retriever = vectordb.as_retriever(search_kwargs={ ""k"" : 10})

Thanks to Langchain and its integration with Cohere, we only have to import the module that will execute the call to the Cohere cross-encoder model:

from langchain_cohere import CohereRerank

os.environ[""COHERE_API_KEY""] = ""YOUR API KEY FROM COHERE""

compressor = CohereRerank(top_n=3)

Finally, we create our Contextual Compression Retriever with Langchain:

from langchain.retrievers.contextual_compression import ContextualCompressionRetriever

compression_retriever = ContextualCompressionRetriever(

base_compressor=compressor,

base_retriever=naive_retriever

)

As simple as that. Let’s see a comparison between a Naive Retriever and a Reranking Retriever:

As we see, Naive returns us the top 10 chunks/documents. After performing the reranking and obtaining the 3 most relevant documents/chunks, there are noticeable changes. Notice how document number 16, which is in third position in relation to its relevance in the first retriever, becomes first position when performing the reranking.

Conclusion

We have seen that depending on the characteristics of the case where we want to apply a RAG, we will want to use one method or another. Furthermore, there may be the case in which one does not know which retriever method to use. For this, there are different libraries to evaluate your rags.

There are several tools for this purpose. Some of those options that I personally recommend are the combination of RAGAS and LangSmith.

Evaluating RAG pipelines with Ragas + LangSmith

Editor's Note: This post was written in collaboration with the Ragas team. One of the things we think and talk about a…

blog.langchain.dev

I highly recommend following, learning and watching the videos of these people who are really what inspired me to make this article.

AI Makerspace

Learn how to build, ship, and share production Large Language Model applications with us!

www.youtube.com

Thank you for reading!

If you find my work useful, you can subscribe to get an email every time that I publish a new article.

If you’d like, follow me on Linkedin!"
https://www.forbes.com/sites/rachelwells/2024/04/15/5-free-online-data-analysis-courses-in-2024/,5 Free Online Data Analysis Courses In 2024,"For the next four years, big data analytics is expected to be one of the core drivers of economic growth, according to projections by the World Economic Forum's Future Of Jobs report 2023. Analytical thinking leads the way in being the number one skill on the rise, increasing in demand this year, as per the findings of the same report, while data analysis appears sixth in LinkedIn top 10 Most In-Demand Skills of 2024 report.

Organizations have many priorities which make data analytics a worthwhile skill to learn, with AI (artificial intelligence), big data, and a strong focus on data-driven decision-making and business strategy being a few of them.

This is further evidenced by the fact that the job demand for data scientists is predicted to continue soaring until 2032 at least, to as much as 35%, according to the U.S. Bureau of Labor Statistics—that's much faster than the average job growth rate in North America.

Thanks to the demand, data analysis careers pay exceptionally well, leading to six figure incomes, with data scientists making as much as over $119,000 per year on average, as evidenced by current average salary figures on Salary.com

MORE FROM FORBES3 High-Paying Remote Data Entry Jobs In 2024

Why Work In Data Analysis?

As if all of the above are not enough, here are two more excellent reasons to pursue data analytics as a career:

First, data analysis is a highly transferable skill, and is a function that is needed in every industry globally, similar to careers such as project management, which also shares the same characteristic of global demand and transferability across industries. You could work in a wide range of sectors, from healthcare, to finance, to e-commerce, to marketing, and of course, technology.

Second, now is a great time to learn data analysis, because of the acceleration and advancement of AI (artificial intelligence), which provides a helpful boost in augmenting your work and ensuring accuracy. Your work within data can come at a critical time when an organization is seeking to prepare its data for use in its internal AI tools and processes.

Free Online Courses On Data Analytics

Whether you're seeking to reskill or upskill, you may not have much spare cash to invest in a course or to pursue a degree within this field. However, there may be some free courses that can help:

1. MIT's How To Process, Analyze And Visualize Data

OpenCourseWare is a an online platform that hosts courses from many of the world's leading universities, including Ivy League schools, for free, using a Creative Commons Licence. One such course, from the world-renowned MIT (Massachusetts Institute of Technology) has lectures and course materials on How To Process, Analyze, And Visualize Data.

2. Google Advanced Data Analytics Professional Certificate

This free online course, provided through Coursera, comes with an extra advantage—a professional certificate. Google career certificates are widely recognized by employers around the world, and undertaking one is an excellent way to fast-track your career development and be invited to job interviews. The advanced version teaches Python, machine learning, Jupyter Notebook, and Tableau software, amongst other skills, all of which you can add to your resume's skills section, thus improving your chances of being hired.

MORE FROM FORBESHow To List Strengths And Skills On Your Resume In 2024

3. Google Data Analytics Professional Certificate

This course is aimed at guiding you through data analysis at the beginner level, and is a great introduction and starting point for further studies, such as the advanced course mentioned above. As with all Google Career certificates offered via Coursera, you have the option to apply for financial aid to cover the cost of your course, subject to eligibility.

4. HarvardX's Data Science Professional Certificate

Harvard University has partnered with edX to provide free courses to learners on the edX platform. Although you need to pay for the entire training with the certificate, if you click into each module (course) within the certificate program on edX, it does provide you with the option to learn the course materials for free. To get the certificate, you would need to upgrade.

5. AWS Data Analytics

Amazon Web Services (AWS) provides a range of online certifications and ongoing professional development training, tailored to specific technical career paths. Once of these learning paths is their data analytics training. They provide this through free virtual training webinars, and individual courses that you can study on their own or as part of their data analytics learning plan, such as Data Analytics Fundamentals, or Best Practices for Data Warehousing.

Overall, choosing to pursue a career in data analysis is a decision you will not regret. If you commit to ongoing professional development and exploring new training opportunities, you too, can experience the lucrative salaries, remote work opportunities, flexibility, and diverse opportunities that others within this profession are already enjoying."
https://towardsdatascience.com/meet-the-nicegui-your-soon-to-be-favorite-python-ui-library-fb69f14bb0ac,Meet the NiceGUI: Your Soon-to-be Favorite Python UI Library,"Meet NiceGUI, a simple Python-based UI framework that works smoothly with your web browser or as a desktop app. Whether you’re making small web apps, dashboards, or playing with robotics projects, NiceGUI makes it easy with its easy interface and many features.

The goal of this post is to convince you to try it out by listing the pros and cons of this library by showing you how you can build and deploy a NiceGUI app. (This is not a sponsored post, I just like the library 🙃)

Streamlit vs. NiceGUI: Why Switch?

While Streamlit is good for making interactive apps, it can be tricky to handle events and states, especially for bigger projects. NiceGUI is different. It lets you control states and interactions directly, without needing extra steps or hacky workarounds.

Simple State Management

NiceGUI makes managing states easy. Unlike Streamlit, which can reset states unexpectedly, NiceGUI keeps things steady, whether it’s the starting state or changes made by users. You can use callbacks to handle user interactions in event-based manner without getting annoyed by a full page refresh and losing state data.

Lots of Features

NiceGUI has many cool features:

Buttons, switches, sliders, inputs, and more for interaction.

Easy ways to arrange things on the screen.

Charts, tables, and even 3D scenes for visuals.

Integration with data visualization libraries like Matplotlib or Plotly.

Customize colors and styles easily.

Tools to help with coding and testing.

Main devs are always available to answer questions and are very receptive to feedback on their GitHub space.

Build on top of popular frameworks: FastAPI, Vue3, Tailwind, Quasar.

Their whole site is made with the NiceGUI library: https://nicegui.io/documentation

Limitations

While NiceGUI is great, it’s worth noting that its smaller community size might be a little limiting. It also has a slightly longer learning curve compared to more popular frameworks like Streamlit. It’s preferable to get familiar with CSS and Tailwind CSS to make the most of the library’s features. Also, knowledge of FastAPI, Vue and Quasar can provide you with greater flexibility and extend what you can implement.

Hands-On

Now, lets explore some features of NiceGUI and then build and deploy a demo app.

Basic app

First install NiceGUI:

pip install nicegui[highcharts]

Lets start from an example from the main documentation:

# https://nicegui.io/documentation/section_data_elements

from nicegui import ui

from random import random

chart = ui.highchart({

'title': False,

'chart': {'type': 'bar'},

'xAxis': {'categories': ['A', 'B']},

'series': [

{'name': 'Alpha', 'data': [0.1, 0.2]},

{'name': 'Beta', 'data': [0.3, 0.4]},

],

}).classes('w-full h-64')

def update():

chart.options['series'][0]['data'][0] = random()

chart.update()

ui.button('Update', on_click=update)

ui.run()

Here, the UI module is what will allow you to create a UI element.

In this example, first we create a highchart element, we assign to it the tailwind classes w-full and h-64. w-full will make it use the whole screen horizontally in a responsive manner and h-64 specifies the height.

When we click on the button, a callback function is triggered. This callback will update the data used for the chart and then re-renders it in a fluid manner.

You can also change the callback to add new bars:

def update():

chart.options[""xAxis""][""categories""].append(random.choice(string.ascii_uppercase))

for series in chart.options['series']:

series[""data""].append(random.random())

chart.update()

Also, notice that refreshing the page does not make you lose your data! You can’t do that with some other Python UI libraries. The reason why it works that way here is that data is shared among all users, but there are lots of ways to keep data user-specific like the app.storage.user object or app.storage.browser (when wrapped around a @ui.page decorator).

But, what if you want to update the UI on a recurrent timer? easy ! Just change the button element to ui.timer

ui.timer(5, callback=lambda: (update(), ui.notify(""Data Updated"")))

Now, let us build a demo app that lets users pick a category then allows them to generate a random Chuck Norris Fact.

First, here is the main code:

import requests # Importing the requests library to make HTTP requests

from nicegui import ui # Importing UI components from the NiceGUI library

from nicegui_app.header import add_head_html # Importing a function to add HTML head content

# List of categories for Chuck Norris facts

CATEGORIES = [

""animal"",

""career"",

""celebrity"",

""dev"",

""fashion"",

""food"",

""money"",

""movie"",

""music"",

""science"",

""sport"",

""travel"",

]

# Class to handle Chuck Norris facts

class Fact:

def __init__(self):

self.fact = None # Initialize the fact attribute to None

# Method to update the fact based on a given category

def update_fact(self, category):

url = f""https://api.chucknorris.io/jokes/random?category={category}"" # URL to Chuck Norris API

for i in range(10): # Try up to 10 times to fetch a valid fact

result = requests.get(url) # Make a GET request to the Chuck Norris API

if result.status_code == 200: # If the request is successful

result_json = result.json() # Parse the JSON response

if self.fact != result_json[""value""]: # If the fetched fact is different from the current one

self.fact = result_json[""value""] # Update the fact attribute

break # Exit the loop

# Function to generate the Chuck Norris fact UI

def chuck():

add_head_html() # Add HTML head content for the NiceGUI app

default_value = CATEGORIES[0] # Default category for Chuck Norris facts

fact = Fact() # Create an instance of the Fact class

fact.update_fact(default_value) # Update the fact using the default category

# Create a grid layout with 12 columns

with ui.grid(columns=12).classes(""w-full""):

# Column for category selection

with ui.column().classes(""col-span-4 sm:col-span-2 space-x-0""):

ui.label(""Pick a fact category:"") # Display a label for category selection

# Radio button group for selecting categories

category = ui.radio(

CATEGORIES,

value=default_value,

on_change=lambda _: fact.update_fact(category.value), # Update the fact when the category changes

).classes(""w-full"")

# Button to regenerate the fact for the selected category

ui.button(

""⟳ Re-Generate"", on_click=lambda _: fact.update_fact(category.value)

)

# Column for displaying the Chuck Norris fact

with ui.column().classes(

""flex col-span-8 sm:col-span-10 w-full justify-center mx-auto max-w-screen-md""

):

# Label to display the Chuck Norris fact, bound to the fact attribute of the Fact instance

ui.label().bind_text_from(fact, ""fact"").classes(

""text-lg sm:text-3xl text-gray-800 bg-gray-100 rounded-lg shadow-lg p-6""

)

Now let us go through it step by step:

First, we make the necessary imports and define the possible categories.

Then, we define the class that will store and update our random fact:

class Fact:

def __init__(self):

self.fact = None # Initialize the fact attribute to None

# Method to update the fact based on a given category

def update_fact(self, category):

url = f""https://api.chucknorris.io/jokes/random?category={category}"" # URL to Chuck Norris API

for i in range(10): # Try up to 10 times to fetch a valid fact

result = requests.get(url) # Make a GET request to the Chuck Norris API

if result.status_code == 200: # If the request is successful

result_json = result.json() # Parse the JSON response

if self.fact != result_json[""value""]: # If the fetched fact is different from the current one

self.fact = result_json[""value""] # Update the fact attribute

break # Exit the loop

This class stores the fact in the attribute “fact” and has a method update_fact that calls the Chuck Norris facts api. https://api.chucknorris.io

Next, we define our page in the “chuck” function. NiceGUI adopts a modular approach that lets you define your app over multiple modules and python files.

We define an instance of our data class fact = Fact() This is a specific instance to each user. Next, we init the fact using the update_fact method.

Now, we start defining our UI elements.

We define a grid with two columns:

A first column that has our category options and generate button. This one has the following tailwind classes: col-span-4 sm:col-span-2. It means that for very small screens it will use up 4/12 of the screen, otherwise it will use up 2/12. This makes the design work in mobile phones too.

A second column where we will display the fact.

For the first column:

A radio menu ui.radio.

A button to generate a random fact.

Both elements, when clicked or changed will use a callback that calls fact.update_fact

For the second column:

We have a ui.label that binds its value to fact.fact so each time this variable changes, it will update the display automatically.

The label has the following tailwind classes: text-lg sm:text-3xl This makes it so the text is smaller on small screens.

This gives you the following app:

Neat! right?

Deployment

Deploying such app is easy! Using CloudRun for example. You just have to create a Dockerfile and then run the following gcloud instructions:

PROJECT_ID=$(gcloud config get-value project)

REPO=""demo""

LOCATION=""europe-west1""

IMAGE=""nicegui_app""

SERVICE_NAME=""nicegui-app""

VERSION=""0.0.1""

GAR_TAG=$LOCATION-docker.pkg.dev/$PROJECT_ID/$REPO/$IMAGE:$VERSION

# Create repository

gcloud artifacts repositories create $REPO --repository-format=docker \

--location=$LOCATION --description=""Docker repository"" \

--project=$PROJECT_ID || true # If fails because already exist then its fine

# Build image

gcloud builds submit --tag $GAR_TAG

# Deploy Cloud run

gcloud run deploy $SERVICE_NAME --image=$GAR_TAG --max-instances=1 --min-instances=0 --port=8080 \

--allow-unauthenticated --region=europe-west1 --memory=0.5Gi --cpu=1 -q --no-cpu-throttling --session-affinity

This builds the docker image using cloud build and then deploys it to CloudRun.

The only key options here are: “ — no-cpu-throttling — session-affinity” This allows the same user to be routed the same container when possible and keeps the CPU alive between requests. You can try it out here: https://nicegui-app-dfmj3maizq-ew.a.run.app/

In Conclusion

NiceGUI is a great choice if you want to make user interfaces quickly and easily with Python. It will help you build powerful python apps where you retain full control of the internal state and that you can test and deploy easily. Hopefully, it can allow you to express your creativity in your data science projects without being limited by tools.

What was shown here is just a small fraction of what you can do with NiceGUI. You can learn more by following the links below.

Resources:"
https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-interview-questions,Top 45 Machine Learning Interview Questions (2024),"Companies are striving to make information and services more accessible to people by adopting new-age technologies like artificial intelligence (AI) and machine learning. One can witness the growing adoption of these technologies in industrial sectors like banking, finance, retail, manufacturing, healthcare, and more.

Data scientists, artificial intelligence engineers, machine learning engineers, and data analysts are some of the in-demand organizational roles that are embracing AI. If you aspire to apply for these types of jobs, it is crucial to know the kind of machine learning interview questions that recruiters and hiring managers may ask.

This article takes you through some of the machine learning interview questions and answers, that you’re likely to encounter on your way to achieving your dream job.

Top Machine Learning Interview Questions

Let's start with some commonly asked machine learning interview questions and answers.

1. What Are the Different Types of Machine Learning?

There are three types of machine learning:

Supervised Learning

In supervised machine learning, a model makes predictions or decisions based on past or labeled data. Labeled data refers to sets of data that are given tags or labels, and thus made more meaningful.

Unsupervised Learning

In unsupervised learning, we don't have labeled data. A model can identify patterns, anomalies, and relationships in the input data.

Reinforcement Learning

Using reinforcement learning, the model can learn based on the rewards it received for its previous action.

Consider an environment where an agent is working. The agent is given a target to achieve. Every time the agent takes some action toward the target, it is given positive feedback. And, if the action taken is going away from the goal, the agent is given negative feedback.

Also Read: Supervised and Unsupervised Learning in Machine Learning

2. What is Overfitting, and How Can You Avoid It?

The Overfitting is a situation that occurs when a model learns the training set too well, taking up random fluctuations in the training data as concepts. These impact the model’s ability to generalize and don’t apply to new data.

When a model is given the training data, it shows 100 percent accuracy—technically a slight loss. But, when we use the test data, there may be an error and low efficiency. This condition is known as overfitting.

There are multiple ways of avoiding overfitting, such as:

Regularization. It involves a cost term for the features involved with the objective function

Making a simple model. With lesser variables and parameters, the variance can be reduced

Cross-validation methods like k-folds can also be used

If some model parameters are likely to cause overfitting, techniques for regularization like LASSO can be used that penalize these parameters

Also Read: Overfitting and Underfitting in Machine Learning

3. What is ‘training Set’ and ‘test Set’ in a Machine Learning Model? How Much Data Will You Allocate for Your Training, Validation, and Test Sets?

There is a three-step process followed to create a model:

Train the model

Test the model

Deploy the model

Training Set Test Set

The training set is examples given to the model to analyze and learn

70% of the total data is typically taken as the training dataset

This is labeled data used to train the model

The test set is used to test the accuracy of the hypothesis generated by the model

Remaining 30% is taken as testing dataset

We test without labeled data and then verify results with labels

Consider a case where you have labeled data for 1,000 records. One way to train the model is to expose all 1,000 records during the training process. Then you take a small set of the same data to test the model, which would give good results in this case.

But, this is not an accurate way of testing. So, we set aside a portion of that data called the ‘test set’ before starting the training process. The remaining data is called the ‘training set’ that we use for training the model. The training set passes through the model multiple times until the accuracy is high, and errors are minimized.

Now, we pass the test data to check if the model can accurately predict the values and determine if training is effective. If you get errors, you either need to change your model or retrain it with more data.

Regarding the question of how to split the data into a training set and test set, there is no fixed rule, and the ratio can vary based on individual preferences.

4. How Do You Handle Missing or Corrupted Data in a Dataset?

One of the easiest ways to handle missing or corrupted data is to drop those rows or columns or replace them entirely with some other value.

There are two useful methods in Pandas:

IsNull() and dropna() will help to find the columns/rows with missing data and drop them

Fillna() will replace the wrong values with a placeholder value

5. How Can You Choose a Classifier Based on a Training Set Data Size?

When the training set is small, a model that has a right bias and low variance seems to work better because they are less likely to overfit.

For example, Naive Bayes works best when the training set is large. Models with low bias and high variance tend to perform better as they work fine with complex relationships.

6. Explain the Confusion Matrix with Respect to Machine Learning Algorithms.

A confusion matrix (or error matrix) is a specific table that is used to measure the performance of an algorithm. It is mostly used in supervised learning; in unsupervised learning, it’s called the matching matrix.

The confusion matrix has two parameters:

Actual

Predicted

It also has identical sets of features in both of these dimensions.

Consider a confusion matrix (binary matrix) shown below:

Here,

For actual values:

Total Yes = 12+1 = 13

Total No = 3+9 = 12

Similarly, for predicted values:

Total Yes = 12+3 = 15

Total No = 1+9 = 10

For a model to be accurate, the values across the diagonals should be high. The total sum of all the values in the matrix equals the total observations in the test data set.

For the above matrix, total observations = 12+3+1+9 = 25

Now, accuracy = sum of the values across the diagonal/total dataset

= (12+9) / 25

= 21 / 25

= 84%

7. What Is a False Positive and False Negative and How Are They Significant?

False positives are those cases that wrongly get classified as True but are False.

False negatives are those cases that wrongly get classified as False but are True.

In the term ‘False Positive,’ the word ‘Positive’ refers to the ‘Yes’ row of the predicted value in the confusion matrix. The complete term indicates that the system has predicted it as a positive, but the actual value is negative.

So, looking at the confusion matrix, we get:

False-positive = 3

True positive = 12

Similarly, in the term ‘False Negative,’ the word ‘Negative’ refers to the ‘No’ row of the predicted value in the confusion matrix. And the complete term indicates that the system has predicted it as negative, but the actual value is positive.

So, looking at the confusion matrix, we get:

False Negative = 1

True Negative = 9

8. What Are the Three Stages of Building a Model in Machine Learning?

The three stages of building a machine learning model are:

Model Building

Choose a suitable algorithm for the model and train it according to the requirement

Model Testing

Check the accuracy of the model through the test data

Applying the Model

Make the required changes after testing and use the final model for real-time projects

Here, it’s important to remember that once in a while, the model needs to be checked to make sure it’s working correctly. It should be modified to make sure that it is up-to-date.

9. What is Deep Learning?

The Deep learning is a subset of machine learning that involves systems that think and learn like humans using artificial neural networks. The term ‘deep’ comes from the fact that you can have several layers of neural networks.

One of the primary differences between machine learning and deep learning is that feature engineering is done manually in machine learning. In the case of deep learning, the model consisting of neural networks will automatically determine which features to use (and which not to use).

This is a commonly asked question asked in both Machine Learning Interviews as well as Deep Learning Interview Questions

10. What Are the Differences Between Machine Learning and Deep Learning?

Learn more: Difference Between AI,ML and Deep Learning

Machine Learning Deep Learning

Enables machines to take decisions on their own, based on past data

It needs only a small amount of data for training

Works well on the low-end system, so you don't need large machines

Most features need to be identified in advance and manually coded

The problem is divided into two parts and solved individually and then combined

Enables machines to take decisions with the help of artificial neural networks

It needs a large amount of training data

Needs high-end machines because it requires a lot of computing power

The machine learns the features from the data it is provided

The problem is solved in an end-to-end manner

11. What Are the Applications of Supervised Machine Learning in Modern Businesses?

Applications of supervised machine learning include:

Email Spam Detection

Here we train the model using historical data that consists of emails categorized as spam or not spam. This labeled information is fed as input to the model.

Healthcare Diagnosis

By providing images regarding a disease, a model can be trained to detect if a person is suffering from the disease or not.

Sentiment Analysis

This refers to the process of using algorithms to mine documents and determine whether they’re positive, neutral, or negative in sentiment.

Fraud Detection

By training the model to identify suspicious patterns, we can detect instances of possible fraud.

Related Interview Questions and Answers

AI | Data Science

12. What is Semi-supervised Machine Learning?

Supervised learning uses data that is completely labeled, whereas unsupervised learning uses no training data.

In the case of semi-supervised learning, the training data contains a small amount of labeled data and a large amount of unlabeled data.

13. What Are Unsupervised Machine Learning Techniques?

There are two techniques used in unsupervised learning: clustering and association.

Clustering

Clustering problems involve data to be divided into subsets. These subsets, also called clusters, contain data that are similar to each other. Different clusters reveal different details about the objects, unlike classification or regression.

Association

In an association problem, we identify patterns of associations between different variables or items.

For example, an e-commerce website can suggest other items for you to buy, based on the prior purchases that you have made, spending habits, items in your wishlist, other customers’ purchase habits, and so on.

14. What is the Difference Between Supervised and Unsupervised Machine Learning?

Supervised learning - This model learns from the labeled data and makes a future prediction as output

Unsupervised learning - This model uses unlabeled input data and allows the algorithm to act on that information without guidance.

15. What is the Difference Between Inductive Machine Learning and Deductive Machine Learning?

Inductive Learning Deductive Learning

It observes instances based on defined principles to draw a conclusion

Example: Explaining to a child to keep away from the fire by showing a video where fire causes damage

It concludes experiences

Example: Allow the child to play with fire. If he or she gets burned, they will learn that it is dangerous and will refrain from making the same mistake again

16. Compare K-means and KNN Algorithms.

K-means KNN

K-Means is unsupervised

K-Means is a clustering algorithm

The points in each cluster are similar to each other, and each cluster is different from its neighboring clusters

KNN is supervised in nature

KNN is a classification algorithm

It classifies an unlabeled observation based on its K (can be any number) surrounding neighbors

17. What Is ‘naive’ in the Naive Bayes Classifier?

The classifier is called ‘naive’ because it makes assumptions that may or may not turn out to be correct.

The algorithm assumes that the presence of one feature of a class is not related to the presence of any other feature (absolute independence of features), given the class variable.

For instance, a fruit may be considered to be a cherry if it is red in color and round in shape, regardless of other features. This assumption may or may not be right (as an apple also matches the description).

18. Explain How a System Can Play a Game of Chess Using Reinforcement Learning.

Reinforcement learning has an environment and an agent. The agent performs some actions to achieve a specific goal. Every time the agent performs a task that is taking it towards the goal, it is rewarded. And, every time it takes a step that goes against that goal or in the reverse direction, it is penalized.

Earlier, chess programs had to determine the best moves after much research on numerous factors. Building a machine designed to play such games would require many rules to be specified.

With reinforced learning, we don’t have to deal with this problem as the learning agent learns by playing the game. It will make a move (decision), check if it’s the right move (feedback), and keep the outcomes in memory for the next step it takes (learning). There is a reward for every correct decision the system takes and punishment for the wrong one.

19. How Will You Know Which Machine Learning Algorithm to Choose for Your Classification Problem?

While there is no fixed rule to choose an algorithm for a classification problem, you can follow these guidelines:

If accuracy is a concern, test different algorithms and cross-validate them

If the training dataset is small, use models that have low variance and high bias

If the training dataset is large, use models that have high variance and little bias

20. How is Amazon Able to Recommend Other Things to Buy? How Does the Recommendation Engine Work?

Once a user buys something from Amazon, Amazon stores that purchase data for future reference and finds products that are most likely also to be bought, it is possible because of the Association algorithm, which can identify patterns in a given dataset.

21. When Will You Use Classification over Regression?

Classification is used when your target is categorical, while regression is used when your target variable is continuous. Both classification and regression belong to the category of supervised machine learning algorithms.

Examples of classification problems include:

Predicting yes or no

Estimating gender

Breed of an animal

Type of color

Examples of regression problems include:

Estimating sales and price of a product

Predicting the score of a team

Predicting the amount of rainfall

22. How Do You Design an Email Spam Filter?

Building a spam filter involves the following process:

The email spam filter will be fed with thousands of emails

Each of these emails already has a label: ‘spam’ or ‘not spam.’

The supervised machine learning algorithm will then determine which type of emails are being marked as spam based on spam words like the lottery, free offer, no money, full refund, etc.

The next time an email is about to hit your inbox, the spam filter will use statistical analysis and algorithms like Decision Trees and SVM to determine how likely the email is spam

If the likelihood is high, it will label it as spam, and the email won’t hit your inbox

Based on the accuracy of each model, we will use the algorithm with the highest accuracy after testing all the models

23. What is a Random Forest?

A ‘random forest’ is a supervised machine learning algorithm that is generally used for classification problems. It operates by constructing multiple decision trees during the training phase. The random forest chooses the decision of the majority of the trees as the final decision.

24. Considering a Long List of Machine Learning Algorithms, given a Data Set, How Do You Decide Which One to Use?

There is no master algorithm for all situations. Choosing an algorithm depends on the following questions:

How much data do you have, and is it continuous or categorical?

Is the problem related to classification, association, clustering, or regression?

Predefined variables (labeled), unlabeled, or mix?

What is the goal?

Based on the above questions, the following algorithms can be used:

25. What is Bias and Variance in a Machine Learning Model?

Bias

Bias in a machine learning model occurs when the predicted values are further from the actual values. Low bias indicates a model where the prediction values are very close to the actual ones.

Underfitting: High bias can cause an algorithm to miss the relevant relations between features and target outputs.

Variance

Variance refers to the amount the target model will change when trained with different training data. For a good model, the variance should be minimized.

Overfitting: High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs.

26. What is the Trade-off Between Bias and Variance?

The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, variance, and a bit of irreducible error due to noise in the underlying dataset.

Necessarily, if you make the model more complex and add more variables, you’ll lose bias but gain variance. To get the optimally-reduced amount of error, you’ll have to trade off bias and variance. Neither high bias nor high variance is desired.

High bias and low variance algorithms train models that are consistent, but inaccurate on average.

High variance and low bias algorithms train models that are accurate but inconsistent.

27. Define Precision and Recall.

Precision

Precision is the ratio of several events you can correctly recall to the total number of events you recall (mix of correct and wrong recalls).

Precision = (True Positive) / (True Positive + False Positive)

Recall

A recall is the ratio of the number of events you can recall the number of total events.

Recall = (True Positive) / (True Positive + False Negative)

28. What is a Decision Tree Classification?

A decision tree builds classification (or regression) models as a tree structure, with datasets broken up into ever-smaller subsets while developing the decision tree, literally in a tree-like way with branches and nodes. Decision trees can handle both categorical and numerical data.

29. What is Pruning in Decision Trees, and How Is It Done?

Pruning is a technique in machine learning that reduces the size of decision trees. It reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.

Pruning can occur in:

Top-down fashion. It will traverse nodes and trim subtrees starting at the root

Bottom-up fashion. It will begin at the leaf nodes

There is a popular pruning algorithm called reduced error pruning, in which:

Starting at the leaves, each node is replaced with its most popular class

If the prediction accuracy is not affected, the change is kept

There is an advantage of simplicity and speed

30. Briefly Explain Logistic Regression.

Logistic regression is a classification algorithm used to predict a binary outcome for a given set of independent variables.

The output of logistic regression is either a 0 or 1 with a threshold value of generally 0.5. Any value above 0.5 is considered as 1, and any point below 0.5 is considered as 0.

31. Explain the K Nearest Neighbor Algorithm.

K nearest neighbor algorithm is a classification algorithm that works in a way that a new data point is assigned to a neighboring group to which it is most similar.

In K nearest neighbors, K can be an integer greater than 1. So, for every new data point, we want to classify, we compute to which neighboring group it is closest.

Let us classify an object using the following example. Consider there are three clusters:

Football

Basketball

Tennis ball

Let the new data point to be classified is a black ball. We use KNN to classify it. Assume K = 5 (initially).

Next, we find the K (five) nearest data points, as shown.

Observe that all five selected points do not belong to the same cluster. There are three tennis balls and one each of basketball and football.

When multiple classes are involved, we prefer the majority. Here the majority is with the tennis ball, so the new data point is assigned to this cluster.

32. What is a Recommendation System?

Anyone who has used Spotify or shopped at Amazon will recognize a recommendation system: It’s an information filtering system that predicts what a user might want to hear or see based on choice patterns provided by the user.

33. What is Kernel SVM?

Kernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods are a class of algorithms for pattern analysis, and the most common one is the kernel SVM.

34. What Are Some Methods of Reducing Dimensionality?

You can reduce dimensionality by combining features with feature engineering, removing collinear features, or using algorithmic dimensionality reduction.

Now that you have gone through these machine learning interview questions, you must have got an idea of your strengths and weaknesses in this domain.

35. What is Principal Component Analysis?

Principal Component Analysis or PCA is a multivariate statistical technique that is used for analyzing quantitative data. The objective of PCA is to reduce higher dimensional data to lower dimensions, remove noise, and extract crucial information such as features and attributes from large amounts of data.

36. What do you understand by the F1 score?

The F1 score is a metric that combines both Precision and Recall. It is also the weighted average of precision and recall.

The F1 score can be calculated using the below formula:

F1 = 2 * (P * R) / (P + R)

The F1 score is one when both Precision and Recall scores are one.

37. What do you understand by Type I vs Type II error?

Type I Error: Type I error occurs when the null hypothesis is true and we reject it.

Type II Error: Type II error occurs when the null hypothesis is false and we accept it.

38. Explain Correlation and Covariance?

Correlation: Correlation tells us how strongly two random variables are related to each other. It takes values between -1 to +1.

Formula to calculate Correlation:

Covariance: Covariance tells us the direction of the linear relationship between two random variables. It can take any value between - ∞ and + ∞.

Formula to calculate Covariance:

39. What are Support Vectors in SVM?

Support Vectors are data points that are nearest to the hyperplane. It influences the position and orientation of the hyperplane. Removing the support vectors will alter the position of the hyperplane. The support vectors help us build our support vector machine model.

40. What is Ensemble learning?

Ensemble learning is a combination of the results obtained from multiple machine learning models to increase the accuracy for improved decision-making.

Example: A Random Forest with 100 trees can provide much better results than using just one decision tree.

41. What is Cross-Validation?

Cross-Validation in Machine Learning is a statistical resampling technique that uses different parts of the dataset to train and test a machine learning algorithm on different iterations. The aim of cross-validation is to test the model’s ability to predict a new set of data that was not used to train the model. Cross-validation avoids the overfitting of data.

K-Fold Cross Validation is the most popular resampling technique that divides the whole dataset into K sets of equal sizes.

42. What are the different methods to split a tree in a decision tree algorithm?

Variance: Splitting the nodes of a decision tree using the variance is done when the target variable is continuous.

Information Gain: Splitting the nodes of a decision tree using Information Gain is preferred when the target variable is categorical.

Gini Impurity: Splitting the nodes of a decision tree using Gini Impurity is followed when the target variable is categorical.

43. How does the Support Vector Machine algorithm handle self-learning?

The SVM algorithm has a learning rate and expansion rate which takes care of self-learning. The learning rate compensates or penalizes the hyperplanes for making all the incorrect moves while the expansion rate handles finding the maximum separation area between different classes.

44. What are the assumptions you need to take before starting with linear regression?

There are primarily 5 assumptions for a Linear Regression model:

Multivariate normality

No auto-correlation

Homoscedasticity

Linear relationship

No or little multicollinearity

45. What is the difference between Lasso and Ridge regression?

Lasso(also known as L1) and Ridge(also known as L2) regression are two popular regularization techniques that are used to avoid overfitting of data. These methods are used to penalize the coefficients to find the optimum solution and reduce complexity. The Lasso regression works by penalizing the sum of the absolute values of the coefficients. In Ridge or L2 regression, the penalty function is determined by the sum of the squares of the coefficients.

Looking forward to a successful career in AI and Machine learning. Enrol in our Artificial Intelligence Course in collaboration with Caltech University now.

Become Part of the Machine Learning Talent Pool

With technology ramping up, jobs in the field of data science and AI will continue to be in demand. Candidates who upgrade their skills and become well-versed in these emerging technologies can find many job opportunities with impressive salaries. Looking forward to becoming a Machine Learning Engineer? Enroll in Simplilearn's Caltech Post Graduate Program in AI & ML and get certified today. Based on your experience level, you may be asked to demonstrate your skills in machine learning, additionally, but this depends mostly on the role you’re pursuing. These machine learning interview questions and answers will prepare you to clear your interview on the first attempt!

Apart from the above mentioned interview questions, it is also important to have a fair understanding of frequently asked Data Science interview questions.

Considering this trend, Simplilearn offers Caltech Post Graduate Program in AI & ML certification course to help you gain a firm hold of machine learning concepts. This course is well-suited for those at the intermediate level, including:

Analytics managers

Business analysts

Information architects

Developers looking to become data scientists

Graduates seeking a career in data science and machine learning

Facing the machine learning interview questions would become much easier after you complete this course."
https://towardsdatascience.com/the-math-behind-deep-cnn-alexnet-738d858e5a2f,The Math Behind Deep CNN — AlexNet,"1: Introduction

AlexNet is a pioneering deep learning network that rose to prominence after winning the ImageNet Large Scale Visual Recognition Challenge in 2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet significantly lowered the top-5 error rate to 15.3% from the previous best of 26.2%, setting a new benchmark for the field. This achievement highlighted the effectiveness of CNNs that use ReLU activations, GPU acceleration, and dropout regularization to manage complex image classification tasks across large datasets.

The model comprises several layers that have become standard in most deep-learning CNNs today. These include convolutional layers, max-pooling, dropout, fully connected layers, and a softmax output layer. The model’s success demonstrated the practicality of deeper network architectures through creative approaches to design and training.

In this article, we will break down the sophisticated design and mathematical principles that underpin AlexNet. We’ll also review AlexNet’s training procedures and optimization techniques, and we will build it from scratch using PyTorch.

2: Overview of AlexNet Architecture

2.1: General Layer Structure

AlexNet’s architecture cleverly extracts features through a hierarchical layering system where each layer builds on the previous layers’ outputs to refine the feature extraction process. Here’s a detailed breakdown of its layers and functions:

Input Image

The model processes input images resized to 227x227 pixels. Each image has three channels (Red, Green, and Blue), reflecting standard RGB encoding.

Layer Configuration

It consists of eight primary layers that learn weights, five of which are convolutional, and the remaining three are fully connected. Between these layers, activation functions, normalization, pooling, and dropout are strategically applied to improve learning efficacy and combat overfitting.

Convolutional Layers

The initial layer uses 96 kernels (filters) sized 11x11x3, which convolve with the input image using a stride of 4 pixels. This large stride size helps reduce the output spatial volume size significantly, making the network computationally efficient right from the first layer.

Outputs from the first layer undergo normalization and max-pooling before reaching the second convolutional layer, which consists of 256 kernels each of size 5x5x48. The use of 48 feature maps each corresponds to separate filtered outputs from the previous layer, allowing this layer to mix features effectively.

The third convolutional layer does not follow with pooling or normalization, which typically helps to maintain the feature map’s richness derived from previous layers. It includes 384 kernels of size 3x3x256, directly connected to the outputs of the second layer, enhancing the network’s ability to capture complex features.

The fourth convolutional layer mirrors the third layer’s configuration but uses 384 kernels of size 3x3x192, enhancing the depth of the network without altering the layer’s spatial dimensions.

The final convolutional layer employs 256 kernels of size 3x3x192 and is followed by a max-pooling layer, which helps to reduce dimensionality and provides rotational and positional invariance to the features being learned.

Fully Connected Layers

The first fully connected layer is a dense layer with 4096 neurons. It takes the flattened output from the preceding convolutional layers (transformed into a 1D vector) and projects it onto a high-dimensional space to learn non-linear combinations of the features.

The second fully connected layer also features 4096 neurons and includes dropout regularization. Dropout helps prevent overfitting by randomly setting a fraction of input units to zero during training, which encourages the network to learn more robust features that are not reliant on any small set of neurons.

The final fully connected layer comprises 1000 neurons, each corresponding to a class of the ImageNet challenge. This layer is essential for class prediction, and it typically utilizes a softmax function to derive the classification probabilities.

2.2: Output Layer and Softmax Classification

The final layer in AlexNet is a softmax regression layer which outputs a distribution over the 1000 class labels by applying the softmax function to the logits of the third fully connected layer.

The softmax function is given by:

​​where zi​ are the logits or the raw prediction scores for each class from the final fully connected layer.

This layer essentially converts the scores into probabilities by comparing the exponentiated score of each class with the sum of exponentiated scores for all classes, highlighting the most probable class.

The softmax layer not only outputs these probabilities but also forms the basis for the cross-entropy loss during training, which measures the difference between the predicted probability distribution and the actual distribution (the true labels).

3: In-depth Analysis of AlexNet Components

3.1: ReLU Nonlinearity

The Rectified Linear Unit (ReLU) has become a standard activation function for deep neural networks, especially CNNs like AlexNet. Its simplicity allows models to train faster and converge more effectively compared to networks using sigmoid or tanh functions.

The mathematical representation of ReLU is straightforward:

This function outputs x if x is positive; otherwise, it outputs zero.

Graphically, it looks like a ramp function that increases linearly for all positive inputs and is zero for negative inputs.

Advantages Of Sigmoid Over Tanh

ReLU has several advantages over traditional activation functions such as sigmoid:

and hyperbolic tangent:

ReLU helps neural networks converge faster by addressing the vanishing gradient problem. This problem occurs with sigmoid and tanh functions where gradients become very small (approach zero) as inputs become large, in either positive or negative direction. This small gradient slows down the training significantly as it provides very little update to the weights during backpropagation. In contrast, the gradient of the ReLU function is either 0 (for negative inputs) or 1 (for positive inputs), which simplifies and speeds up gradient descent.

It promotes sparsity of the activation. Since it outputs zero for half of its input domain, it inherently produces sparse data representations. Sparse representations seem to be more beneficial than dense representations (as typically produced by sigmoid or tanh functions), particularly in large-scale image recognition tasks where the inherent data dimensionality is very high but the informative part is relatively low.

Moreover, ReLU involves simpler mathematical operations. For any input value, this activation function requires a single max operation, whereas sigmoid and tanh involve more complex exponential functions, which are computationally more expensive. This simplicity of ReLU leads to much faster computational performance, especially beneficial when training deep neural networks on large datasets.

Because the negative part of ReLU’s function is zeroed out, it avoids the problem of outputs that do not change in a non-linear fashion as seen with sigmoid or tanh functions. This characteristic allows the network to model the data more cleanly and avoid potential pitfalls in training dynamics.

3.2: Training on Multiple GPUs

AlexNet was one of the pioneering convolutional neural networks to leverage parallel GPU training, managing its deep and computation-heavy architecture. The network operates on two GPUs simultaneously, a core part of its design that greatly improves its performance and practicality.

Layer-wise Distribution

AlexNet’s layers are distributed between two GPUs. Each GPU processes half of the neuron activations (kernels) in the convolutional layers. Specifically, the kernels in the third layer receive inputs from all kernel maps of the second layer, whereas the fourth and fifth layers only receive inputs from kernel maps located on the same GPU.

Communication Across GPUs

The GPUs need to communicate at specific layers crucial for combining their outputs for further processing. This inter-GPU communication is essential for integrating the results of parallel computations.

Selective Connectivity

Not every layer in AlexNet is connected across both GPUs. This selective connectivity reduces the amount of data transferred between GPUs, cutting down on communication overhead and enhancing computation efficiency.

This strategy of dividing not just the dataset but also the network model across two GPUs enables AlexNet to handle more parameters and larger input sizes than if it were running on a single GPU. The extra processing power allows AlexNet to handle its 60 million parameters and the extensive computations required for training deep networks on large-scale image classification tasks efficiently.

Training with larger batch sizes is more feasible with multiple GPUs. Larger batches provide more stable gradient estimates during training, which is vital for efficiently training deep networks. While not directly a result of using multiple GPUs, the ability to train with larger batch sizes and more rapid iteration times helps combat overfitting. The network experiences a more diverse set of data in a shorter amount of time, which enhances its ability to generalize from the training data to unseen data.

3.3: Local Response Normalization

Local Response Normalization (LRN) in AlexNet is a normalization strategy that plays a crucial role in the network’s ability to perform well in image classification tasks. This technique is applied to the output of the ReLU non-linearity activation function.

LRN aims to encourage lateral inhibition, a biological process where activated neurons suppress the activity of neighboring neurons in the same layer. This mechanism works under the “winner-takes-all” principle, where neurons showing relatively high activity suppress the less active neurons around them. This dynamic allows the most significant features relative to their local neighborhood to be enhanced while suppressing the lesser ones.

The LRN layer computes a normalized output for each neuron by performing a sort of lateral inhibition by damping the responses of neurons when their locally adjacent neurons exhibit high activity.

Given a neuron’s activity ax, yi​ at position (x, y) in the feature map i, the response-normalized activity bx, yi​ is given by:​​

where:

ax, yi​ is the activity of a neuron computed by applying kernel i at position (x, y) and then applying the ReLU function.

N is the total number of kernels in the layer.

The sum runs over n neighboring kernel maps at the same spatial position, and N is the total number of kernels.

k, α, β are hyperparameters whose values are predetermined (in AlexNet, typically n=5, k=2, α=10e−4, β=0.75).

bx, yi​ is the normalized response of the neuron.

Local Response Normalization (LRN) serves to implement a form of local inhibition among adjacent neurons, which is inspired by the concept of lateral inhibition found in biological neurons. This inhibition plays a vital role in several key areas:

Activity Regulation

LRN prevents any single feature map from overwhelming the response of the network by penalizing larger activations that lack support from their surroundings. This squaring and summing of neighboring activations ensures no single feature disproportionately influences the output, enhancing the model’s ability to generalize across various inputs.

Contrast Normalization

By emphasizing patterns that stand out relative to their neighbors, LRN functions similarly to contrast normalization in visual processing. This feature highlights critical local features in an image more effectively, aiding in the visual differentiation process.

Error Rate Reduction

Incorporating LRN in AlexNet has helped reduce the top-1 and top-5 error rates in the ImageNet classification tasks. It manages the high activity levels of neurons, thereby improving the overall robustness of the network.

3.4: Overlapping Pooling

Overlapping pooling is a technique used in convolutional neural networks (CNNs) to reduce the spatial dimensions of the input data, simplify the computations, and help control overfitting. It modifies the standard non-overlapping (traditional) max-pooling by allowing the pooling windows to overlap.

Traditional Max Pooling

In traditional max pooling, the input image or feature map is divided into distinct, non-overlapping regions, each corresponding to the size of the pooling filter, often 2x2. For each of these regions, the maximum pixel value is determined and output to the next layer. This process reduces the data dimensions by selecting the most prominent features from non-overlapping neighborhoods.

For example, assuming a pooling size (z) of 2x2 and a stride (s) of 2 pixels, the filter moves 2 pixels across and 2 pixels down the input field. The stride of 2 ensures there is no overlap between the regions processed by the filter.

Overlapping Pooling in AlexNet

Overlapping pooling, used by AlexNet, involves setting the stride smaller than the pool size. This approach allows the pooling regions to overlap, meaning the same pixel may be included in multiple pooling operations. It increases the density of the feature mapping and helps retain more information through the layers.

For example, using a pooling size of 3x3 and a stride of 2 pixels. This configuration means that while the pooling filter is larger (3x3), it moves by only 2 pixels each time it slides over the image or feature map. As a result, adjacent pooling regions share a column or row of pixels that gets processed multiple times, enhancing feature integration.

3.5: Fully Connected Layers and Dropout

In the architecture of AlexNet, after several stages of convolutional and pooling layers, the high-level reasoning in the network is done by fully connected layers. Fully connected layers play a crucial role in transitioning from the extraction of feature maps in the convolutional layers to the final classification.

A fully connected (FC) layer takes all neurons in the previous layer (whether they are the output of another fully connected layer, or a flattened output from a pooling or convolutional layer) and connects each of these neurons to every neuron it contains. In AlexNet, there are three fully connected layers following the convolutional and pooling layers.

The first two fully connected layers in AlexNet have 4096 neurons each. These layers are instrumental in integrating the localized, filtered features that the prior layers have identified into global, high-level patterns that can represent complex dependencies in the inputs. The final fully connected layer effectively acts as a classifier: with one neuron for each class label (1000 for ImageNet), it outputs the network’s prediction for the input image’s category.

Each neuron in these layers applies a ReLU (Rectified Linear Unit) activation function except for the output layer, which uses a softmax function to map the output logits (the raw prediction scores for each class) to a probabilistic distribution over the classes.

The output from the final pooling or convolutional layer typically undergoes flattening before being fed into the fully connected layers. This process transforms the 2D feature maps into 1D feature vectors, making them suitable for processing via traditional neural network techniques. The final layer’s softmax function then classifies the input image by assigning probabilities to each class label based on the feature combinations learned through the network.

3.6: Dropout

Dropout is a regularization technique used to prevent overfitting in neural networks, particularly effective in large networks like AlexNet. Overfitting occurs when a model learns patterns specific to the training data, but which do not generalize to new data.

In AlexNet, dropout is applied to the outputs of the first two fully connected layers. Each neuron in these layers has a probability p (commonly set to 0.5, i.e., 50%) of being “dropped,” meaning it is temporarily removed from the network along with all its incoming and outgoing connections.

If you want to dive deep into Dropout’s math and code, I highly recommend you take a look at section 3.4 of my previous article:

4: Training Process and Optimization

4.1: Stochastic Gradient Descent Parameters

In AlexNet, Stochastic Gradient Descent (SGD) is employed to optimize the network during training. This method updates the network’s weights based on the error gradient of the loss function, where the effective tuning of parameters such as batch size, momentum, and weight decay is critical for the model’s performance and convergence. In today’s article, we will use a Pytorch implementation of SGD, and we will cover a high-level view of this popular optimization technique. If you are interested in a low-level view, scraping its math, and building the optimizer from scratch, take a look at this article:

Let’s cover now the main components of SGD and the settings used in AlexNet:

Batch Size

The batch size, which is the number of training examples used to calculate the loss function’s gradient for one update of the model’s weights, is set to 128 in AlexNet. This size strikes a balance between computational efficiency — since larger batches require more memory and computation — and the accuracy of error estimates, which benefit from averaging across more examples.

The choice of a batch size of 128 helps stabilize the gradient estimates, making the updates smoother and more reliable. While larger batches provide a clearer signal for each update by reducing noise in the gradient calculations, they also require more computational resources and may sometimes generalize less effectively from training data to new situations.

Momentum

Momentum in SGD helps accelerate the updates in the correct direction and smoothens the path taken by the optimizer. It modifies the update rule by incorporating a fraction of the previous update vector. In AlexNet, the momentum value is 0.9, implying that 90% of the previous update vector contributes to the current update. This high level of momentum speeds up convergence towards the loss function’s minimum, which is particularly useful when dealing with small but consistent gradients.

Using momentum ensures that updates not only move in the right direction but also build up speed along surfaces of the loss function’s topology that have consistent gradients. This aspect is crucial for escaping from any potential shallow local minima or saddle points more effectively.

Weight Decay

Weight decay acts as a regularization term that penalizes large weights by adding a portion of the weight values to the loss function. AlexNet sets this parameter at 0.0005 to keep the weights from becoming too large, which could lead to overfitting given the network’s large number of parameters.

Weight decay is essential in complex models like AlexNet, which are prone to overfitting due to their high capacity. By penalizing the magnitude of the weights, weight decay ensures that the model does not rely too heavily on a small number of high-weight features, promoting a more generalized model.

The update rule for AlexNet’s weights can be described as follows:

Here:

vt​ is the momentum-enhanced update vector from the previous step.

μ (0.9 for AlexNet) is the momentum factor, enhancing the influence of the previous update.

ϵ is the learning rate, determining the size of the update steps.

∇L represents the gradient of the loss function for the weights.

λ (0.0005 for AlexNet) is the weight decay factor, mitigating the risk of overfitting by penalizing large weights.

w denotes the weights themselves.

These settings help ensure that the network not only learns efficiently but also achieves robust performance on both seen and unseen data, optimizing the speed and accuracy of training while maintaining the ability to generalize well.

4.2: Initialization

Proper initialization of weights and biases and the careful adjustment of the learning rate are critical to training deep neural networks. These factors influence the rate at which the network converges and its overall performance on both training and validation datasets.

Weights Initialization

In AlexNet, the weights for the convolutional layers are initialized from a zero-mean Gaussian distribution with a standard deviation of 0.01. This narrow standard deviation prevents any single neuron from initially overwhelming the output, ensuring a uniform scale of weight initialization.

Similarly, weights in the fully connected layers are initialized from a Gaussian distribution. Special attention is given to the variance of this distribution to keep the output variance consistent across layers, which is crucial for maintaining the stability of deeper networks.

To get a better understanding of this process let’s build the initialization for AlexNet from scratch in Python:

import numpy as np

def initialize_weights(layer_shapes):

weights = []

for shape in layer_shapes:

if len(shape) == 4: # This is a conv layer: (out_channels, in_channels, filter_height, filter_width)

std_dev = 0.01 # Standard deviation for conv layers

fan_in = np.prod(shape[1:]) # product of in_channels, filter_height, filter_width

elif len(shape) == 2: # This is a fully connected layer: (out_features, in_features)

# He initialization: std_dev = sqrt(2. / fan_in)

fan_in = shape[1] # number of input features

std_dev = np.sqrt(2. / fan_in) # Recommended to maintain variance for ReLU

else:

raise ValueError(""Invalid layer shape: must be 4D (conv) or 2D (fc)"")

# Gaussian initialization

weight = np.random.normal(loc=0, scale=std_dev, size=shape)

weights.append(weight)

return weights

# Example usage:

layer_shapes = [

(96, 3, 11, 11), # Conv1 Layer: 96 filters, 3 input channels, 11x11 filter size

(256, 96, 5, 5), # Conv2 Layer: 256 filters, 96 input channels, 5x5 filter size

(384, 256, 3, 3), # Conv3 Layer: 384 filters, 256 input channels, 3x3 filter size

(384, 384, 3, 3), # Conv4 Layer: 384 filters, 384 input channels, 3x3 filter size

(256, 384, 3, 3), # Conv5 Layer: 256 filters, 384 input channels, 3x3 filter size

(4096, 256*6*6), # FC1 Layer: 4096 output features, (256*6*6) input features

(4096, 4096), # FC2 Layer: 4096 output features, 4096 input features

(1000, 4096) # FC3 (output) Layer: 1000 classes, 4096 input features

]

initialized_weights = initialize_weights(layer_shapes)

for idx, weight in enumerate(initialized_weights):

print(f""Layer {idx+1} weights shape: {weight.shape} mean: {np.mean(weight):.5f} std dev: {np.std(weight):.5f}"")

The initialize_weights function takes a list of tuples describing the dimensions of each layer's weights. Convolutional layers expect four dimensions (number of filters, input channels, filter height, filter width), while fully connected layers expect two dimensions (output features, input features).

In the convolutional layers standard deviation is fixed at 0.01, aligned with the original AlexNet configuration to prevent overwhelming outputs by any single neuron.

Fully connected layers use He initialization (good practice for layers using ReLU activation) where the standard deviation is adjusted to sqrt(2/fan_in) to keep the output variance consistent, promoting stable learning in deep networks.

For each layer defined in layer_shapes, weights are initialized from a Gaussian (normal) distribution centered at zero with a calculated

Biases Initialization

Biases in some convolutional layers are set to 1, particularly in layers followed by ReLU activations. This initialization pushes the neuron outputs into the positive range of the ReLU function, ensuring they are active from the beginning of training. Biases in other layers are initialized at 0 to start from a neutral output.

Like in certain convolutional layers, biases in fully connected layers are also set to 1. This strategy helps to prevent dead neurons at the start of training by ensuring that neurons are initially in the positive phase of activation.

4.3: Strategy for Adjusting the Learning Rate

AlexNet begins with an initial learning rate of 0.01. This rate is high enough to allow significant updates to the weights, facilitating rapid initial progress without being so high as to risk the divergence of the learning process.

The learning rate is decreased by a factor of 10 at predetermined points during the training. This approach is known as “step decay.” In AlexNet, these adjustments typically occur when the validation error rate stops decreasing significantly. Reducing the learning rate at these points helps refine the weight adjustments, promoting better convergence.

Starting with a higher learning rate helps the model overcome potential local minima more effectively. As the network begins to stabilize, reducing the learning rate helps it settle into broad, flat minima that are generally better for generalization to new data.

As training progresses, lowering the learning rate allows for finer weight adjustments. This gradual refinement helps the model to not only fit the training data better but also improves its performance on validation data, ensuring the model is not just memorizing the training examples but genuinely learning to generalize from them.

5: Building AlexNet in Python

In this section, we detail the step-by-step process to recreate AlexNet in Python using PyTorch, providing insights into the class architecture, its initial setup, training procedures, and evaluation techniques.

I suggest you keep this Jupyter Notebook open and accessible, as it contains all the code we will be covering today:

5.1: AlexNet Class

Let’s start with building the AlexNet main class:

# PyTorch for creating and training the neural network

import torch

import torch.nn as nn

import torch.optim as optim

from torch.utils.data.dataset import random_split

# platform for getting the operating system

import platform

# torchvision for loading and transforming the dataset

import torchvision

import torchvision.transforms as transforms

# ReduceLROnPlateau for adjusting the learning rate

from torch.optim.lr_scheduler import ReduceLROnPlateau

# numpy for numerical operations

import numpy as np

# matplotlib for plotting

import matplotlib.pyplot as plt

class AlexNet(nn.Module):

def __init__(self, num_classes=1000):

super(AlexNet, self).__init__()

self.features = nn.Sequential(

nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

nn.Conv2d(64, 192, kernel_size=5, padding=2),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

nn.Conv2d(192, 384, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.Conv2d(384, 256, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.Conv2d(256, 256, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

)

self.avgpool = nn.AdaptiveAvgPool2d((6, 6))

self.classifier = nn.Sequential(

nn.Dropout(),

nn.Linear(256 * 6 * 6, 4096),

nn.ReLU(inplace=True),

nn.Dropout(),

nn.Linear(4096, 4096),

nn.ReLU(inplace=True),

nn.Linear(4096, num_classes),

)

def forward(self, x):

x = self.features(x)

x = self.avgpool(x)

x = torch.flatten(x, 1)

x = self.classifier(x)

return x

Initializationclass AlexNet(nn.Module)

class AlexNet(nn.Module):

def __init__(self, num_classes=1000):

super(AlexNet, self).__init__()

The AlexNet class inherits from nn.Module, a base class for all neural network modules in PyTorch. Any new network architecture in PyTorch is created by subclassing nn.Module.

The initialization method defines how the AlexNet object should be constructed when instantiated. It optionally takes a parameter num_classes to allow for flexibility in the number of output classes, defaulting to 1000, which is typical for ImageNet tasks.

Feature Layers

self.features = nn.Sequential(

nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

nn.Conv2d(64, 192, kernel_size=5, padding=2),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

nn.Conv2d(192, 384, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.Conv2d(384, 256, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.Conv2d(256, 256, kernel_size=3, padding=1),

nn.ReLU(inplace=True),

nn.MaxPool2d(kernel_size=3, stride=2),

)

Here is where the convolutional layers of AlexNet are defined. The nn.Sequential container wraps a sequence of layers, and data passes through these layers in the order they are added.

nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)

The first layer is a 2D convolutional layer (nn.Conv2d) with 3 input channels (RGB image), and 64 output channels (feature maps), with a kernel size of 11x11, a stride of 4, and padding of 2 on each side. This layer processes the input image and begins the feature extraction.

nn.ReLU(inplace=True)

Then, we pass the ReLU activation function which introduces non-linearity, allowing the model to learn complex patterns. The inplace=True parameter helps to save memory by modifying the input directly.

nn.MaxPool2d(kernel_size=3, stride=2)

The max-pooling layer reduces the spatial dimensions of the input feature maps, making the model more robust to the position of features in the input images. It uses a window of size 3x3 and a stride of 2.

Additional nn.Conv2d and nn.MaxPool2d layers follow, which further refine and compact the feature representation. Each convolutional layer typically increases the number of feature maps while reducing their dimensionality through pooling, a pattern that helps in abstracting from the spatial input to features that progressively encapsulate more semantic information.

Adaptive Pooling and Classifier

self.avgpool = nn.AdaptiveAvgPool2d((6, 6))

self.avgpool adaptively pools the feature maps to a fixed size of 6x6, which is necessary for matching the input size requirement of the fully connected layers, allowing the network to handle various input dimensions.

self.classifier = nn.Sequential(

nn.Dropout(),

nn.Linear(256 * 6 * 6, 4096),

nn.ReLU(inplace=True),

nn.Dropout(),

nn.Linear(4096, 4096),

nn.ReLU(inplace=True),

nn.Linear(4096, num_classes),

)

Here, we define another sequential container named classifier, which contains the fully connected layers of the network. These layers are responsible for making the final classification based on the abstract features extracted by the convolutional layers.

nn.Dropout() randomly zeroes some of the elements of the input tensor with a probability of 0.5 for each forward call, which helps prevent overfitting.

nn.Linear(256 * 6 * 6, 4096) reshapes the flattened feature maps from the adaptive pooling layer into a vector of size 4096. It connects every input to every output with learned weights.

Finally, nn.ReLU and nn.Dropout calls further refine the learning pathway, providing non-linear activation points and regularization respectively. The final nn.Linear layer reduces the dimension from 4096 to num_classes, outputting the raw scores for each class.

Forward Method

def forward(self, x):

x = self.features(x)

x = self.avgpool(x)

x = torch.flatten(x, 1)

x = self.classifier(x)

return x

The forward method dictates the execution of the forward pass of the network:

x = self.features(x) processes the input through the convolutional layers for initial feature extraction.

x = self.avgpool(x) applies adaptive pooling to the features to standardize their size.

x = torch.flatten(x, 1) flattens the output to a vector, preparing it for classification.

x = self.classifier(x) runs the flattened vector through the classifier to generate predictions for each class.

5.2: Early Stopping Class

The EarlyStopping class is used during the training of machine learning models to halt the training process when the validation loss ceases to improve. This approach is instrumental in preventing overfitting and conserving computational resources by stopping the training at the optimal time.

class EarlyStopping:

""""""

Early stopping to stop the training when the loss does not improve after

Args:

-----

patience (int): Number of epochs to wait before stopping the training.

verbose (bool): If True, prints a message for each epoch where the loss

does not improve.

delta (float): Minimum change in the monitored quantity to qualify as an improvement.

""""""

def __init__(self, patience=7, verbose=False, delta=0):

self.patience = patience

self.verbose = verbose

self.counter = 0

self.best_score = None

self.early_stop = False

self.delta = delta

def __call__(self, val_loss):

""""""

Args:

-----

val_loss (float): The validation loss to check if the model performance improved.

Returns:

--------

bool: True if the loss did not improve, False if it improved.

""""""

score = -val_loss

if self.best_score is None:

self.best_score = score

elif score < self.best_score + self.delta:

self.counter += 1

if self.counter >= self.patience:

self.early_stop = True

else:

self.best_score = score

self.counter = 0

Initialization

def __init__(self, patience=7, verbose=False, delta=0):

self.patience = patience

self.verbose = verbose

self.counter = 0

self.best_score = None

self.early_stop = False

self.delta = delta

The EarlyStopping class is initialized with several parameters that configure its operation:

patience determines the number of epochs to wait for an improvement in the validation loss before stopping the training. It is set by default to 7, allowing some leeway for the model to overcome potential plateaus in the loss landscape.

verbose controls the output of the class; if set to True, it will print a message for each epoch where the loss does not improve, providing clear feedback during training.

delta sets the threshold for what constitutes an improvement in the loss, aiding in fine-tuning the sensitivity of the early stopping mechanism.

Callable Method

def __call__(self, val_loss):

score = -val_loss

if self.best_score is None:

self.best_score = score

elif score < self.best_score + self.delta:

self.counter += 1

if self.counter >= self.patience:

self.early_stop = True

else:

self.best_score = score

self.counter = 0

The __call__ method allows the EarlyStopping instance to be used as a function, which simplifies its integration into a training loop. It assesses whether the model's performance has improved based on the validation loss from the current epoch.

The method first converts the validation loss into a score that should be maximized; this is done by negating the loss (score = -val_loss), as a lower loss is better. If this is the first evaluation (self.best_score is None), the method sets the current score as the initial best_score.

If the current score is less than self.best_score plus a small delta, indicating no significant improvement, the counter is incremented. This counter tracks how many epochs have passed without improvement. If the counter reaches the patience threshold, it triggers the early_stop flag, indicating that training should be halted.

Conversely, if the current score shows an improvement, the method updates self.best_score with the new score and resets the counter to zero, reflecting the new baseline for future improvements.

This mechanism ensures that the training process is only stopped after a specified number of epochs without meaningful improvement, thereby optimizing the training phase and preventing premature cessation that could lead to underfitting models. By adjusting patience and delta, users can calibrate how sensitive the early stopping is to changes in training performance, allowing them to tailor it to specific scenarios and datasets. This customization is crucial for achieving the best possible model given the computational resources and time available.

5.3: Trainer Class

The Trainer class incorporates the entire training workflow, which includes iterating over epochs, managing the training loop, handling backpropagation, and implementing early stopping protocols to optimize training efficiency and efficacy.

class Trainer:

""""""

Trainer class to train the model.

Args:

-----

model (nn.Module): Neural network model.

criterion (torch.nn.modules.loss): Loss function.

optimizer (torch.optim): Optimizer.

device (torch.device): Device to run the model on.

patience (int): Number of epochs to wait before stopping the training.

""""""

def __init__(self, model, criterion, optimizer, device, patience=7):

self.model = model

self.criterion = criterion

self.optimizer = optimizer

self.device = device

self.early_stopping = EarlyStopping(patience=patience)

self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)

self.train_losses = []

self.val_losses = []

self.gradient_norms = []

def train(self, train_loader, val_loader, epochs):

""""""

Train the model.

Args:

-----

train_loader (torch.utils.data.DataLoader): DataLoader for training dataset.

val_loader (torch.utils.data.DataLoader): DataLoader for validation dataset.

epochs (int): Number of epochs to train the model.

""""""

for epoch in range(epochs):

self.model.train()

for images, labels in train_loader:

images, labels = images.to(self.device), labels.to(self.device)

self.optimizer.zero_grad()

outputs = self.model(images)

loss = self.criterion(outputs, labels)

loss.backward()

self.optimizer.step()

self.train_losses.append(loss.item())

val_loss = self.evaluate(val_loader)

self.val_losses.append(val_loss)

self.scheduler.step(val_loss)

self.early_stopping(val_loss)

# Log the training and validation loss

print(f'Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

if self.early_stopping.early_stop:

print(""Early stopping"")

break

def evaluate(self, test_loader):

""""""

Evaluate the model on the test dataset.

Args:

-----

test_loader (torch.utils.data.DataLoader): DataLoader for test dataset.

Returns:

--------

float: Average loss on the test dataset.

""""""

self.model.eval()

total_loss = 0

with torch.no_grad():

for images, labels in test_loader:

images, labels = images.to(self.device), labels.to(self.device)

outputs = self.model(images)

loss = self.criterion(outputs, labels)

total_loss += loss.item()

return total_loss / len(test_loader)

def accuracy(self, test_loader):

""""""

Calculate the accuracy of the model on the test dataset.

Args:

-----

test_loader (torch.utils.data.DataLoader): DataLoader for test dataset.

Returns:

--------

float: Accuracy of the model on the test dataset.

""""""

self.model.eval()

correct = 0

total = 0

with torch.no_grad():

for images, labels in test_loader:

images, labels = images.to(self.device), labels.to(self.device)

outputs = self.model(images)

_, predicted = torch.max(outputs.data, 1)

total += labels.size(0)

correct += (predicted == labels).sum().item()

return correct / total

def plot_losses(self, window_size=100):

# Compute moving averages

train_losses_smooth = self.moving_average(self.train_losses, window_size)

val_losses_smooth = self.moving_average(self.val_losses, window_size)

# Plot

plt.plot(train_losses_smooth, label='Train Loss')

plt.plot(val_losses_smooth, label='Validation Loss')

plt.legend()

plt.grid()

plt.title('Losses')

def moving_average(self, data, window_size):

return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

Initialization

def __init__(self, model, criterion, optimizer, device, patience=7):

self.model = model

self.criterion = criterion

self.optimizer = optimizer

self.device = device

self.early_stopping = EarlyStopping(patience=patience)

self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)

self.train_losses = []

self.val_losses = []

self.gradient_norms = []

The Trainer class is initialized with the neural network model, the loss function, the optimizer, and the device (CPU or GPU) on which the model will run. This setup ensures that all model computations are directed to the appropriate hardware.

It also configures early stopping and learning rate reduction strategies:

EarlyStopping: Monitors validation loss and stops training if there hasn’t been an improvement for a given number of epochs (patience).

ReduceLROnPlateau: Reduces the learning rate when the validation loss stops improving, which helps in fine-tuning the model by taking smaller steps in the weight space.

Here, train_losses and val_losses collect the loss per epoch for training and validation phases, respectively, allowing for performance tracking and later analysis. gradient_norms could be used to store the norms of the gradients, useful for debugging and ensuring that gradients are neither vanishing nor exploding.

Training Method

def train(self, train_loader, val_loader, epochs):

for epoch in range(epochs):

self.model.train()

for images, labels in train_loader:

images, labels = images.to(self.device), labels.to(self.device)

self.optimizer.zero_grad()

outputs = self.model(images)

loss = self.criterion(outputs, labels)

loss.backward()

self.optimizer.step()

self.train_losses.append(loss.item())

val_loss = self.evaluate(val_loader)

self.val_losses.append(val_loss)

self.scheduler.step(val_loss)

self.early_stopping(val_loss)

# Log the training and validation loss

print(f'Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

if self.early_stopping.early_stop:

print(""Early stopping"")

break

The train method orchestrates the model training over a specified number of epochs. It processes batches of data, performs backpropagation to update model weights, and evaluates model performance using the validation set at the end of each epoch.

After each epoch, it logs the training and validation losses and updates the learning rate if necessary. The loop may break early if the early stopping condition is triggered, which is checked after evaluating the validation loss.

Evaluation and Accuracy Methods

def evaluate(self, test_loader):

self.model.eval()

total_loss = 0

with torch.no_grad():

for images, labels in test_loader:

images, labels = images.to(self.device), labels.to(self.device)

outputs = self.model(images)

loss = self.criterion(outputs, labels)

total_loss += loss.item()

return total_loss / len(test_loader)

def accuracy(self, test_loader):

self.model.eval()

correct = 0

total = 0

with torch.no_grad():

for images, labels in test_loader:

images, labels = images.to(self.device), labels.to(self.device)

outputs = self.model(images)

_, predicted = torch.max(outputs.data, 1)

total += labels.size(0)

correct += (predicted == labels).sum().item()

return correct / total

The evaluate method assesses the model’s performance on a given dataset (typically the validation or test set) and returns the average loss. This method sets the model to evaluation mode, iterates through the dataset, computes the loss for each batch, and calculates the average loss across all batches.

accuracy calculates the accuracy of the model on a given dataset by comparing the predicted labels with the actual labels. This method processes the dataset in evaluation mode, uses the model’s predictions to compute the number of correct predictions, and returns the accuracy percentage.

Utility Methods for Visualization

def plot_losses(self, window_size=100):

# Compute moving averages

train_losses_smooth = self.moving_average(self.train_losses, window_size)

val_losses_smooth = self.moving_average(self.val_losses, window_size)

# Plot

plt.plot(train_losses_smooth, label='Train Loss')

plt.plot(val_losses_smooth, label='Validation Loss')

plt.legend()

plt.grid()

plt.title('Losses')

def moving_average(self, data, window_size):

return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

This method visualizes the training and validation losses, smoothed over a specified window of epochs to highlight trends more clearly, such as reductions in loss over time or potential points where the model began to overfit.

5.4: Data Preprocessing

To effectively train the AlexNet model, proper data preprocessing is necessary to conform to the input requirements of the model, specifically, the dimension and normalization standards that AlexNet was originally designed with.

Transform

transform = transforms.Compose([

transforms.Resize((224, 224)), # Resize the images to 224x224 for AlexNet compatibility

transforms.ToTensor(), # Convert images to PyTorch tensors

transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize the tensors

])

transforms.Resize((224, 224)) adjusts the size of the images to 224x224 pixels, matching the input size required by the AlexNet model, ensuring that all input images are of the same size.

transforms.ToTensor() converts the images from a PIL format or a NumPy array to a PyTorch tensor, an essential step as PyTorch models expect inputs in tensor format.

transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) normalizes the image tensors; this specific normalization adjusts the mean and standard deviation for all three channels (RGB) to 0.5, effectively scaling pixel values to the range [-1, 1]. This step is crucial as it standardizes the inputs, facilitating the model's learning process.

Loading Dataset

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,

download=True, transform=transform)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,

download=True, transform=transform)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

Here, we load the CIFAR-10 dataset for both training and testing. You might wonder why we didn’t choose the ImageNet dataset, which is known for its extensive use in training models that compete in the ImageNet challenge. The reason is practical: ImageNet requires significant computational resources and lengthy training times, which I wouldn’t recommend attempting on a standard laptop. Instead, we opt for the CIFAR-10 dataset, which includes 60,000 32x32 color images distributed across 10 different classes, with 6,000 images per class.

Disclaimer: The CIFAR-10 dataset is open source and available for use under the MIT License. This license allows for wide freedom in use, including commercial applications.

Split and Data Loader

train_split = 0.8

train_size = int(train_split * len(trainset))

val_size = len(trainset) - train_size

train_dataset, val_dataset = random_split(trainset, [train_size, val_size])

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)

test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

The training data is split to set aside 80% for training and 20% for validation. This practice is common to tune the model on unseen data, enhancing its ability to generalize well.

DataLoader objects are created for the training, validation, and test datasets with a batch size of 64. Shuffling is enabled for the training data to ensure randomness, which helps the model learn more effectively by reducing the chance of learning spurious patterns from the order of the data.

Data Visualization

dataiter = iter(train_loader)

images, labels = next(dataiter)

def imshow(img):

img = img / 2 + 0.5 # unnormalize

npimg = img.numpy()

plt.imshow(np.transpose(npimg, (1, 2, 0)))

plt.show()

imshow(torchvision.utils.make_grid(images[:5]))

print(' '.join('%5s' % classes[labels[j]] for j in range(5)))

First, we need to unnormalize the image (img = img / 2 + 0.5). Here imshow converts it from a tensor to a NumPy array, and changes the order of dimensions to fit what matplotlib.pyplot.imshow() expects.

Then, we display the first 5 images in the dataset:

5.5: Model Training and Evaluation

Finally, we set up the training environment for an AlexNet model, executing the training process, and evaluating the model’s performance on a test dataset using PyTorch.

But first, we need to ensure the best computational resource (CPU or GPU) to use, which maximizes performance efficiency.

# Check the system's operating system

if platform.system() == 'Darwin': # Darwin stands for macOS

try:

device = torch.device('cuda')

_ = torch.zeros(1).to(device) # This will raise an error if CUDA is not available

except:

device = torch.device('mps' if torch.backends.mps.is_built() else 'cpu')

else:

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

Here, we identify whether the system is macOS (‘Darwin’) and tries to configure CUDA for use. If CUDA is unavailable, which is common on macOS without NVIDIA GPUs, it opts for MPS (Apple’s Metal Performance Shaders) if available, or CPU otherwise.

On operating systems other than macOS, it directly attempts to utilize CUDA and defaults to CPU if CUDA isn’t available.

Model, Loss Function, and Optimizer Initialization

Next, we initialize the AlexNet model, specifying the computational device, and set up the loss function and optimizer:

model = AlexNet(num_classes=10).to(device)

criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

An instance of AlexNet is created with 10 classes, and it is immediately transferred to the determined device (GPU or CPU). This ensures all computations for the model are performed on the specified device.

The CrossEntropyLoss function is used for training, which is typical for multi-class classification problems.

The SGD (Stochastic Gradient Descent) optimizer is initialized with the model’s parameters, a learning rate of 0.01, and a momentum of 0.9. These are standard values to start with for many vision-based tasks.

Training the Model

The model undergoes training over a specified number of epochs, handling data in batches, calculating loss, performing backpropagation, and applying early stopping based on the validation loss:

trainer = Trainer(model, criterion, optimizer, device, patience=7)

trainer.train(train_loader, val_loader, epochs=50)

The train method trains the model for 50 epochs using the training and validation data loaders. This method meticulously processes batches from the data loaders, computes the loss, performs backpropagation to update weights, and evaluates the model periodically using the validation dataset to implement early stopping if no improvement is observed in the validation loss.

Model Evaluation

After training, the model’s performance is assessed on the test set using:

test_loss = trainer.evaluate(test_loader)

print(f'Test Loss: {test_loss:.4f}')

accuracy = trainer.accuracy(test_loader)

print(f'Test Accuracy: {accuracy:.2%}')

Finally, the training and validation losses are visualized to monitor the model’s learning progress:

trainer.plot_losses(window_size=3)

This line calls the plot_losses method to visualize the training and validation loss. The losses are smoothed over a window (3 data points in this case) to better visualize trends without noise. By running this code you should expect the following loss:

As shown in the graph above, the model training stopped after 21 epochs because we set the patience parameter to 7, and the validation loss didn’t improve after the 14th epoch. Keep in mind, that this setup is meant for educational purposes, so the goal isn’t to outperform AlexNet.

You’re encouraged to tweak the setup by increasing the number of epochs or the patience to see if the validation loss might drop further. Also, there are several changes and updates you could apply to enhance AlexNet’s performance. Although we won’t cover these adjustments in this article due to our 30-minute limit, you can explore a variety of advanced techniques that could refine the model’s performance.

For those interested in further experimentation, try adjusting parameters like the learning rate, tweaking the network architecture, or using more advanced regularization methods. You can explore more optimization and fine-tuning techniques in this article:

6: Conclusion

AlexNet has been a pivotal model in the evolution of neural network design and training techniques, marking a significant milestone in the field of deep learning. Its innovative use of ReLU activations, overlapping pooling, and GPU-accelerated training dramatically improved the efficiency and effectiveness of neural networks, setting new standards for model architecture.

The introduction of dropout and data augmentation strategies by AlexNet addressed overfitting and improved the generalization capabilities of neural networks, making them more robust and versatile across various tasks. These techniques have become foundational in modern deep-learning frameworks, influencing a wide array of subsequent innovations.

Additional Resources

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems. http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf

LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. https://doi.org/10.1038/nature14539"
https://towardsdatascience.com/ai-mapping-using-neural-networks-to-identify-house-numbers-12f194a95d75,AI Mapping: Using Neural Networks to Identify House Numbers,Let Our Actions Inspire | Data Scientist & Deep Learning Engineer at Intel | All ideas and opinions expressed are my own | Find my work at johnlenehan.eu
https://www.datasciencecentral.com/using-window-functions-for-advanced-data-analysis/,Using window functions for advanced data analysis,"Window functions are an advanced feature of SQL that provides powerful tools for detailed data analysis and manipulation without grouping data into single output rows, which is common in aggregate functions. These functions operate on a set of rows and return a value for each row based on the calculation against the set.

In this article, we delve into window functions in SQL Server. You will learn how to apply various window functions, including moving averages, ranking, and cumulative sums, to achieve comprehensive analytics on data sets.

You will also see how to partition and filter data using the window functions.

Finally, you will study some best practices and pitfalls to avoid when working with the Window functions. These are the types of things that are covered during the more advanced SQL workshops that are available online.

Note: We will use the Microsoft Pubs database as an example to execute various window function queries.

Understanding window functions

Window functions are used for calculations across sets of rows related to the current row. Unlike standard aggregate functions, window functions do not collapse rows and allow us to perform calculations across rows related to the current row. This capability is crucial for running totals, moving averages, and cumulative statistics, which are invaluable for time-series data analysis, financial data, inventory management, and more.

With window functions, you can specify a “window” of rows related to the current row over which SQL Server performs a calculation. You can define this window using clauses like OVER, PARTITION BY, and ORDER BY.

Basic syntax

The basic syntax for a window function is:

Each part of the syntax has a specific purpose:

{function_name}(): This is the window function you want to apply. SQL supports various window functions such as SUM(), AVG(), COUNT(), RANK(), ROW_NUMBER(), and more. These functions can compute values over a specified range of rows.

OVER: This keyword defines the window over which the SQL server executes the function. It signifies the start of the window specification.

PARTITION BY: Divides the data into partitions (or groups) to which the function is applied. If you don’t include the PARTITION BY clause, all the rows will be treated as a single partition.

ORDER BY: Defines the order of data within each partition.

Practical scenarios using window functions

Let’s explore some practical scenarios using window functions on the Microsoft Pubs database. We will look into calculating moving averages, ranking, and cumulative sums.

Calculating moving averages for sales quantities

Moving averages smooth out data series and are commonly used to understand trends.

Let’s calculate a moving average for the sales quantities in the sales table of the Pubs database.

USE pubs

SELECT ord_num, ord_date, qty,

AVG(qty) OVER (ORDER BY qty ROWS UNBOUNDED PRECEDING) AS MovingAvgQty

FROM sales;

Output:

In the above query, we use the AVG window functions to calculate the moving average for the qty column. The ROWS UNBOUNDED PRECEDING means we want to calculate the moving average of all the previous rows up to the current ones.

You can also calculate the moving average for a specific number of previous rows.

For example, the following script returns the moving average for the previous two rows and the current row. Notice here that we cast the qty column to a floating type to have a precise average value.

USE pubs

SELECT ord_num, ord_date, qty,

AVG(CAST(qty AS float)) OVER (ORDER BY qty ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS MovingAvgQty

FROM sales;

Output:

Ranking Sales Data by Price

Ranking can help in comparing items, like listing products by sales price.

Let’s see an example where we rank the total sales price for each sale. We will first join the sales and titles tables. Next, we will calculate the total sale price for each record by multiplying qty with the price column of the corresponding tables). Finally, we will use the RANK function to rank all the records in descending order of the total sale price. This will give you information about which sale made the most money.

SELECT

S.ord_num,

S.ord_date,

S.qty,

T.title,

S.qty * T.price AS TotalSalePrice,

RANK() OVER (ORDER BY S.qty * T.price DESC) AS SalesRank

FROM

sales S

JOIN

titles T ON S.title_id = T.title_id;

Output:

Cumulative Sums of Total Sales Price

Cumulative sums are useful for running totals, which can be essential for inventory or account balance tracking.

For example, let’s calculate the cumulative total sale prices for all the rows in the sales table. As we did previously, we will join the sales and titles column to calculate the total sale price for each row.

Next, you can use the SUM window function to calculate the cumulative sales by ordering the results using the ord_date column. This will return you the cumulative sales by date.

USE pubs;

SELECT

S.ord_num,

S.ord_date,

S.qty,

T.title,

S.qty * T.price AS TotalSalePrice,

SUM(S.qty * T.price) OVER (ORDER BY s.ord_date ROWS UNBOUNDED PRECEDING) AS CumulativeSales

FROM

sales S

JOIN

titles T ON S.title_id = T.title_id;

Output:

Partitioning and Filtering with Window Functions

You can partition and filter records in a window function using the PARTITION BY and the CASE statement.

Partitioning with PARTITION BY Clause

You can use the PARTITION BY clause in conjunction with window functions. This allows you to apply window functions separately for each partition.

For example, the following query returns cumulative prices for various title types in the titles table.

USE pubs;

SELECT

title_id,

title,

type,

price,

SUM(price) OVER (PARTITION BY type ORDER BY price ROWS UNBOUNDED PRECEDING) AS CumulativePriceByType

FROM

titles

Output:

In the above output, cumulative prices are calculated separately for each title type.

Filtering with CASE Statement

You can use the CASE statement inside a window function to filter the records before applying the window function.

For example, you can use the following query containing the CASE statement if you only want to include titles in the cumulative sum where the price is greater than $10:

Output:

Best Practices and Common Pitfalls When Using Window Functions

Let’s now discuss some of the best practices and common pitfalls to avoid when using window functions in SQL Server.

Best practices

Indexing for Performance: Ensure columns used in ORDER BY and PARTITION BY are indexed to improve query performance, especially with large datasets.

Use PARTITION BY Judiciously: Use PARTITION BY thoughtfully. Overpartitioning, especially by columns with high cardinality, can reduce performance. Balance meaningful data segmentation with efficiency.

Limit Window Frames: Use specific boundaries like ROWS or RANGE to limit window sizes instead of defaulting to UNBOUNDED PRECEDING, which improves performance by reducing the number of rows processed.

Common pitfalls

Ignoring NULL Values: Window functions include NULL values by default. To ensure accuracy, exclude or handle NULLs as necessary.

Forgetting to Order Data: Omitting ORDER BY can yield incorrect results since the order of rows affects calculations like running totals or moving averages.

Performance Issues: Be mindful of potential performance issues with large datasets or complex queries. Check execution plans to identify and mitigate bottlenecks.

Conclusion

Window functions in SQL Server are indispensable tools for anyone seeking to perform sophisticated data analysis without the constraints of traditional aggregate functions. Their ability to operate over a set of rows and dynamically compute values makes them essential for various applications—from financial modelling and time-series analysis to inventory management.

In this article, you saw how SQL window functions work with the help of different practical scenarios. You also learned how to partition and filter data using window functions and what are the best practices and pitfalls to avoid when using window functions."
https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5,Quantizing the AI Colossi,"In recent years, a powerful alliance has been forged between the transformer neural network architecture and the formulation of various problems as self-supervised sequence prediction tasks. This union has enabled researchers to train large foundation models of unprecedented sizes using massive troves of unlabeled sequential data, and these models have shown uncanny emergent capabilities that closely mimic human-level intelligence in several domains. With newfound heights of practical utility unleashed, artificial intelligence (AI) was catapulted into mainstream life and conversation, and few today are unaware that the once fictional realm of silicone-based intelligence has now become very tangible and real.

However, intrinsically linked to the explosive growth in AI capabilities has been the rapid inflation of model sizes well into the hundreds of billions (and in some cases trillions) of parameters. A powerful new technology was delivered to the world, but it could be only be served using massive hardware clusters. Echoing the challenges from earlier eras of AI, the temptation to possess these powers on consumer hardware or edge devices was tremendous, and motivation to compress these pretrained behemoths took effect immediately, triggering catalytic flows of funding and talent into the study of model compression, and reanimating several well-pedigreed techniques including pruning, quantization, knowledge distillation, and parameter-efficient fine-tuning.

In part one of the Streamlining Giants series, we began our discussion on democratizing the power of large language models (LLMs) through model compression by exploring the rich legacy of research in neural network pruning, from its inception through its recent applications in LLMs containing tens or hundreds of billions of parameters. Along the way, we discovered that these large models can be compressed substantially with minimal drops in performance and marked gains in computational burden through either the unstructured or structured removal of the least important parameters from the network. We also saw that while pruning produces compact models that can operate in resource-constrained environments, the process itself traditionally required calculation of gradient information and/or retraining of the model to recover performance. This meant that the method was historically only accessible for those with the computational resources needed to train the original model, which in the case of LLMs would mean millions of dollars. While this originally placed the means of compressing models through pruning outside of the reach of those who needed it most, we saw that recent studies have proposed highly accessible methods using low-rank gradients or even just forward-pass information. Further, with the retraining of large models facilitated by simultaneous advances in parameter-efficient fine-tuning methods, pruning can now be performed using consumer hardware.

In this installment, we investigate an orthogonal approach to model compression: Quantization seeks to improve the computational efficiency and memory requirements of models by reducing the precision of the numbers being stored and operated on by the network, which may include the weights, activations, or both. While quantization can refer to any drop in precision, for example from 32-bit to 16-bit floating point, it also often involves a transition into the integer space, which offers accelerated operation and deployment on consumer hardware. As we will see, quantization is an extremely powerful method for compressing LLMs, offering significant reductions in computational overhead and hardware requirements with only minor or even non-existent drops in performance, making it the most widely employed model compression technique in today’s world of large models. Further, by varying the levels of numeric precision, we can tune the accuracy/efficiency tradeoff for our use case.

Along this journey, we will see that quantization works in harmony with the pruning techniques we encountered previously, as well as with knowledge distillation and parameter-efficient fine-tuning methods which we have yet to explore, providing us a glimpse into the upcoming topics of investigation in the Streamlining Giants series. There is a popular adage which states that “there is no such thing as free lunch,” but as we saw in our investigation into pruning: when it comes to model compression, sometimes there is. Similar to pruning, quantization acts as a form of regularization which is known to make neural networks more robust and generalizable, meaning that judicious applications of these techniques can often simultaneously compresses a model and improve its performance. In this article, we will survey the literature and see several examples of “free lunch” compression. By the end, even the skeptical reader should find themselves disabused of the notion that network quantization inherently suggests a degradation in quality. After reviewing the research, we will explore the tools for applying these techniques in our own work using open-source software. Now, let us dig in to the exciting field of neural network quantization.

Note: For those who want to skip the lesson and get straight to the implementation guide for accelerating their workflows, click here.

To ground our investigation into quantization, it is important to reflect on exactly what we mean by “quantizing” numbers. So far we’ve discussed that through quantization, we take a set of high-precision values and map them to a lower precision in such a way that best preserves their relationships, but we have not zoomed into the mechanics of this operation. Unsurprisingly, we find there are nuances and design choices to be made concerning how we remap values into the quantized space, which vary depending on use case. In this section, we will seek to understand the knobs and levers which guide the quantization process, so that we can better understand the research and equip ourselves to bring educated decision making into our deployments.

Bit Width

Throughout our discussion on quantization, we will refer to the bit widths of the quantized values, which represents the number of bits available to express the value. A bit can only store a binary value of 0 or 1, but sets of bits can have their combinations interpreted as incremental integers. For instance, having 2 bits allows for 4 total combinations ({0, 0}, {0, 1}, {1, 0}, {1, 1}) which can represent integers in the range [0, 3]. As we add N bits, we get 2 to the power of N possible combinations, so an 8-bit integer can represent 256 numbers. While unsigned integers will count from zero to the maximum value, signed integers will place zero at the center of the range by interpreting the first bit as the +/- sign. Therefore, an unsigned 8-bit integer has a range of [0, 255], and a signed 8-bit integer spans from [-128, 127].

This fundamental knowledge of how bits represent information will help us to contextualize the numeric spaces that the floating point values get mapped to in the techniques we study, as when we hear that a network layer is quantized to 4 bits, we understand that the destination space has 2 to the power of 4 (16) discrete values. In quantization, these values do not necessarily represent integer values for the quantized weights, and often refer to the indices of the quantization levels — the “buckets” into which the values of the input distribution are mapped. Each index corresponds to a codeword that represents a specific quantized value within the predefined numeric space. Together, these codewords form a codebook, and the values obtained from the codebook can be either floating point or integer values, depending on the type of arithmetic to be performed. The thresholds that define the buckets depend on the chosen quantization function, as we will see. Note that codeword and codebook are general terms, and that in most cases the codeword will be the same as the value returned from the codebook.

Floating-Point, Fixed-Point, and Integer-Only Quantization

Now that we understand bit widths, we should take a moment to touch on the distinctions between floating-point, fixed-point, and integer-only quantization, so that we are clear on their meaning. While representing integers with binary bits is straightforward, operating on numbers with fractional components is a bit more complex. Both floating-point and fixed-point data types have been designed to do this, and selecting between them depends on both on the deployment hardware and desired accuracy-efficiency tradeoff, as not all hardware supports floating-point operations, and fixed-point arithmetic can offer more power efficiency at the cost of reduced numeric range and precision.

Floating-point numbers allocate their bits to represent three pieces of information: the sign, the exponent, and the mantissa, which enables efficient bitwise operations on their representative values. The number of bits in the exponent define the magnitude of the numeric range, and the number of mantissa bits define the level of precision. As one example, the IEEE 754 standard for a 32-bit floating point (FP32) gives the first bit to the sign, 8 bits to the exponent, and the remaining 23 bits to the mantissa. Floating-point values are “floating” because they store an exponent for each individual number, allowing the position of the radix point to “float,” akin to how scientific notation moves the decimal in base 10, but different in that computers operate in base 2 (binary). This flexibility enables precise representation of a wide range of values, especially near zero, which underscores the importance of normalization in various applications.

In contrast, “fixed” point precision does not use a dynamic scaling factor, and instead allocates bits into sign, integer, and fractional (often still referred to as mantissa) components. While this means higher efficiency and power-saving operations, the dynamic range and precision will suffer. To understand this, imagine that you want to represent a number which is as close to zero as possible. In order to do so, you would carry the decimal place out as far as you could. Floating-points are free to use increasingly negative exponents to push the decimal further to the left and provide extra resolution in this situation, but the fixed-point value is stuck with the precision offered by a fixed number of fractional bits.

Integers can be considered an extreme case of fixed-point where no bits are given to the fractional component. In fact, fixed-point bits can be operated on directly as if they were an integer, and the result can be rescaled with software to achieve the correct fixed-point result. Since integer arithmetic is more power-efficient on hardware, neural network quantization research favors integer-only quantization, converting the original float values into integers, rather than the fixed-point floats, because their calculations will ultimately be equivalent, but the integer-only math can be performed more efficiently with less power. This is particularly important for deployment on battery-powered devices, which also often contain hardware that only supports integer arithmetic.

Uniform Quantization

To quantize a set of numbers, we must first define a quantization function Q(r), where r is the real number (weight or activation) to be quantized. The most common quantization function is shown below:

In this formula, Z represents an integer zero-point, and S is the scaling factor. In symmetrical quantization, Z is simply set to zero, and cancels out of the equation, while for asymmetrical quantization, Z is used to offset the zero point, allowing for focusing more of the quantization range on either the positive or negative side of the input distribution. This asymmetry can be extremely useful in certain cases, for example when quantizing post-ReLU activation signals, which contain only positive numbers. The Int(·) function assigns a scaled continuous value to an integer, typically through rounding, but in some cases following more complex procedures, as we will encounter later.

Choosing the correct scaling factor (S) is non-trivial, and requires careful consideration of the distribution of values to be quantized. Because the quantized output space has a finite range of values (or quantization levels) to map the inputs to, a clipping range [α, β] must be established that provides a good fit for the incoming value distribution. The chosen clipping range must strike a balance between not over-clamping extreme input values and not oversaturating the quantization levels by allocating too many bits to the long tails. For now, we consider uniform quantization, where the bucketing thresholds, or quantization steps, are evenly spaced. The calculation of the scaling factor is as follows:

The shapes of trained parameter distributions can vary widely between networks and are influenced by a number of factors. The activation signals generated by those weights are even more dynamic and unpredictable, making any assumptions about the correct clipping ranges difficult. This is why we must calibrate the clipping range based on our model and data. For best accuracy, practitioners may choose to calibrate the clipping range for activations online during inference, known as dynamic quantization. As one might expect, this comes with extra computational overhead, and is therefore by far less popular than static quantization, where the clipping range is calibrated ahead of time, and fixed during inference.

Dequantization

Here we establish the reverse uniform quantization operation which decodes the quantized values back into the original numeric space, albeit imperfectly, since the rounding operation is non-reversible. We can decode our approximate values using the following formula:

Non-Uniform Quantization

The astute reader will probably have noticed that enacting uniformly-spaced bucketing thresholds on an input distribution that is any shape other than uniform will lead to some bits being far more saturated than others, and that adjusting these widths to focus more bits in the denser regions of the distribution would more faithfully capture the nuances of the input signal. This concept has been investigated in the study of non-uniform quantization, and has indeed shown benefits in signal fidelity; however, the hardware-optimized calculations made possible by uniform quantization has made it the de-facto neural network quantization method. The equation below describes the non-uniform quantization process:

Many works in non-uniform quantization refer to learning centroids, which represent the centers of clusters in the input distribution to which the surrounding values are mapped through the quantization process. To think of this another way, in uniform quantization, where the thresholds are evenly spaced on the input distribution, the centroids are simply the values directly in between the bucketing thresholds.

Mixed-Precision Quantization

As we saw with pruning, a trained neural network’s performance is more sensitive to changes in some layers and submodules than others, and by measuring these sensitivities, entire pieces of neural networks can be removed without significantly affecting error. Intuitively, the same is true for varying levels of quantization, with some network components capable of being remapped to much lower bit widths than their counterparts. The most fundamental example of this we already mentioned: the use of 16-bit floats in less-sensitive network operations to substantially reduce memory footprint during training, but mixed-precision quantization can refer to any combination of different quantization levels throughout a network.

Related to the concept of mixed-precision quantization is the granularity of quantization, which might be layer-wise, group-wise, channel-wise, or sub-channel-wise, and describes the scale at which distinct sets of quantization parameters are calibrated. Intuitively, computational overhead increases with granularity, representing an accuracy/efficiency trade-off. For example, in convolutional neural networks (CNNs), channel-wise granularity is often the weapon of choice, since sub-channel-wise (i.e. filter-wise) quantization would be too complex.

Scalar vs. Vector Quantization

While the majority of research in quantization has historically focused on quantizing individual values within the matrices, it is possible to learn multidimensional centroids as well. This means that matrices can be split into vectors, and then each of those vectors can be given a codeword that points to their closest centroid, creating the possibility of recovering entire pieces of the matrix from single codebook lookups, effectively storing a set of numbers into a single value, and greatly increasing compression levels. This is known as Vector Quantization, and the advantages it offers has been attracting increasing interest. “Vector Quantization” typically refers to splitting the matrices into column vectors, but these vectors can be further split into sub-vectors in a practice known as Product Quantization, which generalizes both vector and scalar quantization at its extremes. The idea is that the assembly of centroid vectors returned from the codebook using the relatively small structure of stored codewords will faithfully recreate the original, larger matrix. We will see that this has indeed proven to be a very powerful model compression technique.

Compensating for the Effects of Quantization

It makes sense that we cannot simply round all of the weights in a neural network to various resolutions and expect that things still work properly, so we must come up with a plan for how to compensate for the perturbations caused by the quantization process. As we learned above, it is possible to train or fine-tune models under simulated quantization in order to drastically increase the amount of quantization that can be performed without affecting performance in a technique called Quantization-Aware Training (QAT), which also allows for learning the quantization parameters during training. However, performing QAT requires having the hardware and data necessary to train the model, which is often not possible, particularly for very large models like today’s LLMs. To address this issue, Post-Training Quantization (PTQ) techniques aim to avoid training and require only a small amount of unlabeled data to calibrate the quantization function, and Zero-Shot Quantization (ZSQ) explores the ideal “data-free” scenario which requires no data for calibration.

We will see each these techniques highlighted in more detail as we journey through the literature, so let us now board our temporal tour bus and travel back to the end of the last century, when researchers were being similarly tantalized by the power of neural networks which exceeded their hardware limitations, and first started to consider how we might hope to deploy these complex models on mobile hardware.

Quantization in the Post-AlexNet Era

In 2012, the authors of AlexNet capitalized on a serendipitous confluence of major developments in data availability and computational hardware to eclipse the performance of previous state-of-the-art approaches in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). Two major elements helped to make this historic success possible: 1) the efforts of Fei-Fei Li and her team at Princeton, who provided the world’s first large-scale curated image dataset, and 2) a fortuitous coincidence that the well-funded advancements in Graphic Processing Unit (GPU) technology, which were being driven by a healthy stream of gaming industry revenue, happened to produce hardware that offers the same type of parallelized computation needed to accelerate the matrix operations in deep learning.

Given these auspicious circumstances, Alex Krizhevsky and his team trained a sizeable Convolutional Neural Network (CNN) of 62.3 million parameters and blew past the competitors with a >10% lead in accuracy over the runner up, marking a watershed PR moment for neural network research, and kicking off an enduring period of intense interest and funding that is often referred to as the “deep learning revolution.” However, the rejuvenated neural nets were quickly met by their old foe: hardware limitations. Despite the profound benefits of GPU-accelerated training, the authors of AlexNet conceded that hardware constraints imposed a limiting factor on the success of their approach, and that the results would likely improve with better hardware. The research community was quick to recognize the unaddressed potential of model compression to address these limitations, and rose to action. The revelations made during this CNN-focused period about the varying sensitivity levels among different types of network components would provide a strong base for the future investigations into transformers.

In fact, the echoing desires to deploy neural networks at the edge were already being heard before AlexNet shook the world. Vanhoucke et al.’s seminal 2011 work explored the acceleration of neural nets on x86 CPUs. Written at a time when the AI community was at a crossroads — debating whether to invest in GPUs or to extract more performance from traditional CPUs — their paper offered a pivotal guide on optimizing neural network operations on Intel and AMD CPUs. Predating the era of GPU dominance ushered in by AlexNet, Vanhoucke et al. showcased the untapped potential of CPUs through meticulous optimizations, including the adoption of fixed-point and integer arithmetic complemented by SIMD instructions and memory alignment techniques. Using these optimizations, the authors achieved significant performance gains, and laid the groundwork for upcoming research into the efficient training and deployment of neural networks on CPU hardware.

After the success of AlexNet, CNNs became the new soil in which a rapid-growing crop of quantization research would grow. Researchers grappled with the nuances of quantizing different types of network layers, with their varying levels of sensitivity and advantages to be offered. For instance, most of the FLOPs in CNNs occur in the convolutional layers, so quantizing them offers the best gains in speed; however, these layers also house the parameters most crucial to feature extraction, rendering them particularly sensitive to alteration. Fully-connected layers, on the other hand, tend to be much easier to compress, but doing so is advantageous mostly in terms of storage size rather than the latency, as these layers contribute less to the overall computational graph.

In addition to the distinction between techniques providing storage-only vs. full efficiency gains, there is also a distinction in the latter group between techniques which seek to accelerate both training and inference vs. those that only seek to accelerate inference. The concept of QAT was born during this period, and while many techniques opt to use simulated quantization during training, others stuck closer to the roots of network quantization and explored the use of fixed-point or integer-only arithmetic during both training and inference to pursue end-to-end neural network development at the edge.

As two early examples of these diverse approaches to CNN compression through quantization, Denton et al.’s 2014 “Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation” method improves computational efficiency by applying matrix factorization as either a PTQ or QAT procedure in the convolutional layers, while Gong et al.’s 2014 “Compressing Deep Convolutional Networks using Vector Quantization” focuses instead on achieving storage size optimization by compressing the fully-connected layers using a variety of vector quantization methods in a PTQ setting, remarking on the distinct superiority of product quantization.

In this section, we will watch the field of quantization come into focus during the era of CNN dominance unleashed by AlexNet. In this formative period of growth, we will see QAT, mixed-precision quantization, PTQ, and extreme quantization down to 1 or 2 bits become well-defined areas of research, setting the stage for our exploration into the maturation of these techniques in today’s era of large models.

It was during the post-AlexNet era that QAT truly took form as a distinct area of quantization research. In former eras, nearly all work in quantization used the training process to optimize weight discretization, since the networks in question were relatively small. Even after the AI growth spurt triggered by GPU-accelerated training, the models were still trainable using a reasonable amount of resources, and concerns about avoiding the necessity of retraining quantized networks were mostly motivated by mobile deployment and data access/privacy concerns, rather than training resources. Nonetheless, the value of PTQ was becoming clear during this era, and the distinction between the two fields materialized. Given the reasonable training costs, the bulk of quantization research in the CNN era stuck to the roots of QAT-based approaches. In this section, we review the development of QAT approaches during the CNN era.

We start in late 2014, when Courbariaux et al. observed that “multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks,” and investigated specifically reducing the precision of these operations for efficiency gains, since their cost scales quadratically in relation to bit width, whereas the costs of the other operators in the multiplier-accumulator (MAC) operations (the adder and accumulator) only scale linearly, and are therefore comparably inexpensive. Notably, their study showed that “the use of half precision floating point format has little to no impact on the training of neural networks.” Further, the authors found that “very low precision is sufficient not just for running trained networks but also for training them,” although at this point in time, “very low precision” is referring to 10-bit multiplications, an emblem of how rapidly this field has shifted bits, as we will see.

In 2015, Gupta et al. from IBM published “Deep Learning with Limited Numerical Precision,” introducing a pioneering approach to training deep neural networks in 16-bit fixed-point arithmetic using stochastic rounding — where the probability of rounding a number to either of its nearest quantization points is proportional to their proximity. This rounding method outperforms the round-to-nearest approach by introducing noise which drives the expected value (bias) of quantization error to zero. Unlike conventional QAT methods that often rely on full-precision floating-point operations during training, Gupta et al.’s strategy involves executing all training computations in lower precision. The use of fixed point arithmetic allows using faster and more power and space-efficient compute units, and the authors explore hardware co-design by demonstrating a novel energy-efficient hardware accelerator. Importantly, the use of stochastic rounding ensures that even small gradient values contribute to the training process, thereby diminishing the reliance on gradient approximation methods like the Straight-Through Estimator (STE).

Han et al.’s 2015 “Deep Compression” is a foundational dictionary-based (i.e. codebook) method for achieving extraordinary levels of compression in neural networks without sacrificing performance using a hybrid quantization and pruning approach. Motivated to bring the breakthroughs in computer vision into mobile applications, the authors experimented on the workhorse CNNs of the day, achieving compression rates of 35x and 49x on AlexNet and VGG-16 respectively, with no loss in accuracy, by using a three-stage pipeline which sequentially prunes, quantizes, and finally applies Huffman coding (a form of lossless data compression) to the network weights. First, the unimportant weights are identified and pruned from the model using the seminal unstructured approach of Han et al.’s prior 2015 work “Learning both Weights and Connections for Efficient Neural Networks” to achieve between 9x and 13x compression with no increase in error. Second, the remaining weights are quantized from 32 down to 5 bits, followed by a round of retraining to recover performance, then the approach finishes with Huffman coding of the quantized weights for an additional 20–30% reduction in storage size.

The results from “Deep Compression” are staggering, with the method compressing CNNs to less than 1/35 their original size while demonstrating equal or superior accuracy compared with their baseline references. However, it is important to remember that both the AlexNet and VGG architectures studied are intentionally overparameterized to maximize performance, and they both have contain parameter-dense yet relatively insensitive fully-connected layers and the end which can be compressed heavily. While the method is primarily focused on enhancing the storage footprint of the model, the authors remark that smaller storage also means accelerated operation, as there are less weights to store and fetch, and therefore reduced memory bandwidth requirements, particularly when the model size is reduced enough to be stored on-chip in the Static Random Access Memory (SRAM) rather than being bounced back and forth between Dynamic RAM (DRAM) and SRAM. Further, the authors introduce the concept of the Efficient Inference Engine (EIE), a hardware accelerator designed to leverage the sparsity resulting from pruning, which would be the subject of their forthcoming publication.

In early 2017, the Incremental Network Quantization (INQ) approach from Zhou et al. surpassed the levels of compression seen in Deep Compression. The authors also use a combination of pruning and quantization, but no Huffman coding, to achieve 53x compression on AlexNet with no loss in top-1 accuracy, and a whopping 89x compression with only minimal loss (<1.5%). Their strategy incrementally quantizes portions of the network weights, retrains the remaining full-precision weights to compensate of the induced error, and iterates until all weights are quantized. The weights are constrained be either zero or powers of 2 (think negative powers). As the authors explain, the advantage of this strategy “is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA.” To do this, they employ a variable-length encoding, which uses one bit to indicate a zero value, and the remaining bits together indicate the codeword that indexes the possible quantized values for a given bit width and scaling factor, the latter of which they set to be the maximum absolute layer-wise weight magnitude. Their approach slightly exceeds baseline FP32 AlexNet performance with 5-bit precision, and shows comparable accuracy down to 3-bits, as seen in the chart below. Code for INQ is available on GitHub.

In late 2017, Jacob et al. from Google focused on enabling efficient integer-only inference at the edge on mobile device CPUs. The authors state that using intentionally overparameterized models like AlexNet and VGG to benchmark model compression techniques creates an easy target, so they opt instead to test their approach using MobileNets. Since these compact models are already designed to maximize parameter efficiency, there is less “dead weight” to be easily compressed, and their parameters are more sensitive to perturbation. Considered the formative work in QAT, Jacob et al.’s approach quantizes both weights and activations to 8 bit integers, and the biases (which require more precision) to 32 bit integers. Their method uses floating-point arithmetic during training, serving as an early example of using simulated quantization for QAT. The authors avoid a quantization scheme requiring look-up tables because these tend to be less performant than pure arithmetic on SIMD hardware, and opt instead for an affine transformation of the weights to the integer space. To complement their QAT method, the authors co-design a framework for converting and running the resulting trained model on integer-only hardware, and go a step further than many previous works by proving their efficiency gains on actual edge hardware.

So far, the techniques we’ve seen quantize the layers of the model to a uniform level of precision, which is the optimal condition for hardware acceleration, particularly at the edge on low-power hardware. However, as we learned in our previous exploration into pruning, some network layers are less sensitive to alteration than others, and can therefore be compressed more aggressively without affecting performance. Thus, mixed-precision quantization approaches use varying levels of compression on network components (typically at layer-wise granularity) based on their sensitivity to achieve even smaller memory footprints and reduce the data transfer and power costs of operating models at the edge. In the next section, we will see the familiar themes of sensitivity analysis inform the variable assignment of numeric precision across neural network components to maximize model compression through quantization.

The astute reader will notice that the majority of works in this section on quantization during the CNN era belong to the QAT category, since the models studied during this period were easy enough to fine-tune in a quantized setting. However, even before the imminent explosion in neural network sizes, researchers were already keen on the practical advantages of PTQ, which promised to liberate those developing quantized models from the need to gain access to the original training data (which may be impossible in many cases), as well as save the time and resources required for retraining. Thus, a timely wave of interest in PTQ research caught steam around 2019, laying a welcome groundwork for the focus on large language models yet to come.

Krishnamoorthi led this charge in mid-2018 with a seminal white paper on CNN quantization. Their approach uses channel-wise asymmetric uniform quantization of weights and layer-wise quantization of activations to a fixed precision of 8 bits while maintaining accuracy within 2% of baseline. The author observes that quantizing only the weights of a network to 8-bit can be an easy way of compressing storage size, but in order to enable efficient inference, activations must also be quantized, which requires using calibration data to calculate the dynamic ranges of activations throughout the network layers to discover the appropriate layer-wise quantization parameters. In the chart below, the author provides a comparison of the effects of per-layer and per-channel weight quantization schemes on various CNNs. Notice that the larger, overparameterized CNNs towards the right are much more receptive to the lower granularity of per-layer quantization parameters than the efficiency-minded MobileNets (left).

In October 2018, Banner et al.’s “Post training 4-bit quantization of convolutional networks for rapid-deployment” sought to expand the usability of PTQ to below 8bits of precision. Their approach efficiently achieves 4-bit data-free mixed-precision PTQ with tolerable performance degradation. To do this, the authors exploit the knowledge that neural network distributions tend to be bell-shaped around a mean in order to tune their quantization scheme in a way which minimizes the mean-squared quantization error at the tensor level, thereby avoiding the need for retraining. To allow for better knowledge transfer into the quantized space, the authors 1) use their proposed analytical clipping for integer quantization (ACIQ) technique to clamp activation tensor outliers according to an optimal saturation point which reduces rounding error in the more densely populated region of the spectrum, 2) determine optimal per-channel bit allocations analytically, finding that the optimal quantization step size for a given channel “is proportional to the 2/3-power of its range,” and 3) propose a simple bias-correction method to compensate for the biases introduced into the weights after quantization by incorporating the expected changes in their channel-wise mean and variance into the quantization parameters.

The ACIQ approach requires statistical analysis of network activations on a small calibration set, so while it does not require access to the training data, it is essential to ensure that the calibration set is representative of the distributions that will be encountered during runtime, otherwise there is risk of overfitting quantization parameters to the wrong distribution. Also, note that the use of channel-wise bit width creates a lot of concerns for practical application, as both hardware and software must be catered to support mixed-precision at the channel level, or otherwise running the quantized network may be inefficient or impossible. Nonetheless, the formulation of a closed-form analytical solution to directly calculate the optimal bit-widths for network components marks an important milestone in quantization research. Further, their closed-form PTQ solution for bias correction parameters, as well as the efficient absorption of these parameters into existing calculations, marks another significant contribution. Code for Banner et al.’s approach is available on GitHub.

Nagel et al.’s 2019 “Data-Free Quantization Through Weight Equalization and Bias Correction” (DFQ) introduced a groundbreaking approach to data-free PTQ, enabling deep networks to be efficiently quantized down to 8 bits without the need for calibration data, fine-tuning, or hyperparameter tuning. The authors adapt the weights through scaling to make them “more amenable to quantization,” propose a method for correcting the biases introduced by quantization, and mention that their approach could be employed as a complementary pre-processing step to QAT. Unlike the Krishnamoorthi and Banner et al. approaches above, which require storing quantization parameters for each channel, DFQ requires storing only a single scale and offset value for each layer’s weight tensor by determining the values that maximize the per-channel precision across that layer. DFQ exploits the scale equivariance of the ReLU activation function and keeps the overall math equivalent by absorbing the scaling and induced bias into the next layer. The authors demonstrate that the need for calibration data to quantize activations can be avoided by using the batch normalization statistics preserved in the model from its training to estimate the layer-wise expected quantization error, and compensate for the bias introduced into layer activations through quantization by subtracting this expected error from the layer bias parameters. Although the authors do not explicitly use the term, DFQ can be seen as a formative work in zero-shot quantization (ZSQ), as it is a PTQ approach which requires no calibration data.

Choukroun et al.’s 2019 OMSE method finds the kernel-wise quantization parameters which minimize the mean squared error (MSE) between the quantized and original weight/activation tensors, marking the first PTQ approach to achieve 4-bit quantization with minimal loss in accuracy (3% degradation on top-1 ImageNet classification). The authors opt to use symmetrical uniform quantization for highest efficiency, saving the additional calculations introduced by using offsets. Since the relationship between quantization-induced MSE and scaling factor for a given kernel is non-convex, the authors use a line search method to discover the optimum values. To circumvent the need for using mixed-precision to preserve representation power in the sensitive network layers, the authors propose to represent these “key” layers using multiple low-precision tensors, but they warn that the complexity of this approach necessitates using it only for small tensors, which in the case of CNNs works out fine, since the most sensitive components are the convolutional kernels, but it would not scale well for architectures with large salient components.

In early 2020, the authors of the HAWQ papers published ZeroQ: a data-free PTQ approach which beat the previous state-of-the-art ZSQ benchmark set by DFQ. Their approach achieves mixed-precision quantization through a novel Pareto frontier based method which automatically determines the optimal mixed-precision setting with no manual searching. Rather than requiring access to training or calibration data, ZeroQ generates a synthetic dataset tailored to match the statistics in the batch normalization layers, called “distilled data,” and then uses the activations generated by this data to calibrate quantization parameters and to perform layer-wise sensitivity analysis. These sensitivity values feed the Pareto frontier selection process, which finds the optimal setting for a given model size or desired level of accuracy. The authors call out the fact that most work in PTQ typically only benchmarks image classification accuracy without considering more complex tasks, so they also prove their method preserves performance on the more challenging object detection task. ZeroQ is open-sourced and extremely compute efficient, offering a low barrier for entry on network quantization.

Extreme quantization refers to compression at ≤2 bits, meaning either ternary (2-bit), or binary (1-bit). The ability to effectively compress models down to these levels of precision comes with obvious benefits, as the models become 16–32 times smaller than their FP32 counterparts, allowing for deployment on smaller edge devices using far less power consumption, and saving valuable on-chip real estate for optimizing computation speed. Unsurprisingly, reducing the precision of neural networks this low comes with equally extreme challenges, due to the loss of representational power. However, the costly multiply-accumulate (MAC) operations in network computation can be entirely replaced in both binary and ternary networks by far more energy-efficient addition/subtraction and bit shift operations, making the potential gains radical, and galvanizing researchers to tackle the challenges involved. In this section, we observe the compelling results which arose from the developing field of extreme network quantization during the CNN era, and discover why it has since become such a highly magnetic and frequently cited field: the extraordinary gains in efficiency offered by low-bit networks are hard to ignore. Moreover, we will see that binary nets can be ensembled to use an equivalent number of bits as a fixed-point network to exceed its performance while maintaining all the benefits of binary arithmetic. First, let us rewind to 2014, so that we can build our understanding of low-precision networks bit-by-bit.

As a pioneering work in extreme quantization, the 2014 study by Hwang & Sung from Seoul National University titled “Fixed-Point Feedforward Deep Neural Network Design Using Weights +1, 0, and -1” is a QAT approach which obtains ternary (2-bit) weights and 3-bit activation signal with negligible performance loss. In contrast, the authors observe that the biases must be allotted a higher precision of 8 bits in order to preserve performance. The quantization thresholds are initially chosen to minimize MSE between the original and quantized tensors, and then an exhaustive search is performed in each layer one-by-one in order to tune these initial proposals to their optimal values which minimize the network output error. Finally, the quantized weights are fine-tuned using a fixed-point backpropagation scheme that is modified to handle the quantization. While the authors set a foundational precedent in the field of extreme neural network quantization, their use of exhaustive search is a testament to the smaller model sizes of the day, and more scalable solutions would become necessary for the growing models sizes in the coming years.

In 2015, Courbariaux et al. presented the “BinaryConnect” algorithm. As the name suggests, their method produces a binarized network, in which the weights are constrained to be either -1 or 1. The authors remark that noisy weights, i.e. weights that are discretized using a stochastic rounding scheme, “are quite compatible with Stochastic Gradient Descent (SGD),” as we saw earlier in Gupta et al.’s 2015 work on stochastic rounding. Like the approach of Hwang & Sung above, the authors apply the gradients from the loss generated by the quantized weights to update the full-precision weights (which are stored separately), and the quantized weights are derived from the current state of the full-precision weights for each forward pass. The authors demonstrate that both deterministic binarization (simply taking the sign of the weight) and stochastic binarization (using weight magnitude to derive probability) both work well as regularization mechanisms (similarly to dropout) in the networks studied, demonstrating slower convergence curves with lower final validation error compared with the non-regularized full-precision baseline. While these results are very exciting, it is important to consider the fact that neither the CIFAR-10 dataset nor the CNN being trained in their study are particularly complex by today’s standards, so it is not clear at this point if these results would hold up using deeper networks or more challenging tasks. The code for BinaryConnect is available on GitHub.

Later in 2015, Lin et al.’s “Neural Networks with Few Multiplications” extended the work and codebase of BinaryConnect in a fork of the original repo to include a ternary variant of their stochastic binarization approach called “TernaryConnect,” and also introduced a quantized back propagation (QBP) scheme, where network activations are quantized to integer powers of two so that the expensive multiplication operations in the backwards passes can be replaced by efficient bit shift operations, further increasing training efficiency, and ticking another checkbox on the list of network operations which can be binarized.

Li et al.’ 2016 “Ternary Weight Networks” (TWN) is a QAT approach which trains ternary networks from scratch. Their quantization scheme seeks to minimize Euclidean distance between the quantized and raw weights using learned layer-wise scaling factors and ternarization thresholds set to 3/4 the average magnitude per weight tensor to approximate the full-precision weights as closely as possible. The authors observe that ternary networks “show better expressive capabilities than binary precision counterparts,” and demonstrate this concept using the example of binary vs ternary 3x3 convolution filters, which can take on 512 and 19683 possible unique templates, respectively. Their experiments prove that this additional expressive power is beneficial on various tasks, including MNIST, CIFAR-10, ImageNet, and the Pascal VOC object detection task. In the results below, we can see that this additional expressive ability of ternary networks is particularly beneficial in the more challenging ImageNet and Pascal VOC tasks, a possible signal that the optimism we were experiencing from the very promising binarization results we saw above on less complex models and tasks may not hold up as the complexity grows.

In late 2016, Hou, Yao, and Kwok published “Loss-aware Binarization of Deep Networks” (LAB), which filled an empty research lane left by previous methods which had not optimized the binarization process based on its impact on the cost function directly. To binarize the network in a way which minimizes the cost function, the authors solve a proximal Newton algorithm by using the second-order gradient information captured in the Adam optimizer to efficiently extract a diagonal Hessian approximation, rather than computing the Hessian directly. The authors show that their method is “more robust to wide and deep networks,” and also extend their investigation into NLP tasks using Recurrent-Neural Networks (RNNs). Then in early 2018, Hou & Kwok extended their LAB algorithm to produce higher-precision networks in “Loss-Aware Weight Quantization of Deep Networks” (LAQ, or LAT in the ternary case), showing that the proposed method improved results further over the binarization offered by LAB.

In 2017, Dong et al. proposed the stochastic quantization algorithm: a QAT approach to extreme quantization in which only a portion of the network elements/filters (inversely proportional in size to the quantization error) are quantized during each training step, and updated separately from the full-precision weights. As training progresses, eventually all weights are quantized, and the resulting low-bit network maintains significantly better accuracy than equivalent BWN and TWN models.

The 2017 Incremental Network Quantization (INQ) paper we saw earlier in the QAT of CNNs section, which surpassed the previous state-of-the-art in model compression set by Deep Compression in 2015, also investigated the viability of their approach in creating ternary networks. In the chart below, we can see that their approach is notably superior to TWN in ResNet-18 trained on the ImageNet classification task, reducing the error by over 4%. Looking above, we can also see that it beats the 36.18% top-1 error rate of the stochastic-quantized TWN (SQ-TWN) by over 2%.

Later in 2017 Lin et al.’s “Towards Accurate Binary Convolutional Neural Network” (ABC-Net) paper sought to overcome the lack of representational power in binary networks by combining multiple sets of binary weights or activations to more faithfully represent the high-precision values, showing that by using 3–5 weight bases and 5 binary activations, the accuracy degradation on ImageNet can be reduced to 5% from baseline. The authors point out that while this requires the use of more bits, the scheme is preferable to using higher-bit fixed-point representation because it still avoids the need for the more complex arithmetic operators as the bitwise math is still done in binary. Their work marks the first time that binary neural networks reached comparable performance on ImageNet to full precision baselines, but their solution increases baseline BNN complexity by O(k * l), where k is the number of weight bases and l is the number of activation bases used, so there is a notable loss in efficiency.

Zhu et al.’s 2018 “Binary Ensemble Neural Network: More Bits per Network, or More Networks per Bit?” (BENN) paper argued that the limitations of BNNs are not solvable through further optimization of the binarization process, since they are rooted in the lack of representational ability of the binary space. The authors aimed to reduce the prediction variance and improve the robustness to noise when using binary networks, achieving this by creating ensembles of multiple BNNs using boosting or bagging. Their experiments demonstrate that the statistical properties of the ensembled classifiers improve, and the performance improves drastically as a result. The added complexity of executing these ensembles is only O(k), more efficient than ABC-Net by a factor of l, and outperforms it significantly on ImageNet. Moreover, because the ensemble can be parallelized, the solution can have O(1) added complexity, and run just as fast as the baseline BNN.

In this section, we’ve witnessed the explosion of interest in extreme neural network quantization following the advent of large CNNs which began after the resounding success of AlexNet in 2012. The tantalizing urge to deploy the newfound heights of modeling capabilities in this era on edge devices with low-power hardware was irresistible, as this is where many practical applications of deep learning exist. Note that most of the methods in this section train low-bit networks from scratch without quantizing pretrained weights, which is more easily facilitated in the fully quantized approaches by their extraordinary efficiency. Later, we will see that this impressive period of foliation in extreme quantization methodology would mature into a fertile bed of soil in which future breakthroughs would be made.

For now, we close our chapter on the CNN era, as now we have seen the formation of several distinct areas of quantization research flourish after the success of AlexNet brought massive influxes of talent and funding into the deep learning field. We’ve seen QAT approaches achieve impressive levels of compression without losing performance by fine-tuning the quantized weights, mixed-precision techniques achieve new levels of compression by incorporating sensitivity analysis into the quantization process, PTQ approaches closely match baseline performance in 8 or even 4 bits of precision with no retraining (and in the case of ZSQ, without using any calibration data), and finally, we saw the rise of extreme quantization research. Now, we shift our focus from the CNN era to the wave of research interest in NLP born from the success of the transformer architecture in 2017.

Now that we are familiar with the functional details and history of quantization, we can proceed in our discussion of quantizing the large language models (LLMs) of today. As we saw with pruning, the transition into the world of large models comes with diminishing hopes of employing compression techniques that require model training, such as QAT, for anyone but the largest outfits. Therefore, we will see a shift in research focus towards the more lightweight methods of PTQ, although the unique challenges of operating at large scales did not stop the ingenuity of the research community from finding ways of accessing the benefits of QAT, as we will see. In this section, we first review the quantization efforts that took place in the period between the publication of the transformer in the seminal 2017 paper “Attention is All You Need,” and the dawn of LLMs marked by the monumental release of the 175B (billion parameter) GPT-3 in 2020. Then, we review the proliferation of PTQ methods in the LLM era, subsequently shift our focus into QAT approaches for LLMs, and finally close our investigation by reviewing the extreme quantization of LLMs. This section will complete our education in quantization, qualifying us to move on to the implementation guide in the next section, and begin employing neural network quantization in our own workflows.

Quantization in Early Era of Transformers

As we saw in the last section, the success of AlexNet in computer vision set off an explosion of research interest in quantizing CNNs for their efficient deployment, but investigations into the application of quantization in language models wouldn’t pick up speed until years later, with the catalytic moment being the striking success of the transformer architecture. Before the explosive growth of language model sizes that began with the release of GPT-3 in 2020, there was a period of more reasonably-sized explorations in NLP using transformers, although the tendency for these models to continue gaining performance with increasingly large sizes was quickly becoming clear. Here, we review the formative period of quantization research in transformer-based language models that occurred during this formative period between the advent of transformers and the rise of multi-billion parameter transformer networks. A we will see, this new network architecture posed unique challenges to quantization, particularly at low bit widths, which researchers moved quickly to understand and overcome.

The seminal Bidirectional Encoding Representation Transformer (BERT) published by Google in late 2018 continues to be a highly influential transformer-based language model. BERT trains on a masked language modeling (MLM) objective to learn how to encode bidirectional context to produce embeddings which consider information that appears both before and after a given input token in the input sequence, resulting in representations that contain deep contextual understanding, useful for tasks like sentiment classification and question answering. While BERT uses an encoder-only variant of the transformer architecture to create these bidirectionally encoded representations, the Generative Pretrained Transformer (GPT) models of OpenAI fame, by contrast, use a decoder-only transformer architecture to perform an autoregressive modeling task on Byte-Pair Encoded (BPE) text, considering only the preceding tokens in the sequence, and iteratively predicting the upcoming tokens. The revelations made about the potential scalability of these autoregressive decoder-only transformers and their unlabeled pretraining datasets in the GPT-2 paper would ultimately inspire the release of the unprecedentedly large 175B GPT-3 behemoth in 2020, but as we will see, the research in transformer quantization would already be well underway by then.

In late 2019, Shen et al. from UC Berkeley published Q-BERT, a QAT method which expanded on the Hessian-based sensitivity analysis of HAWQ by additionally considering the variance of the Hessian spectrum, rather than only the mean, across the subsample of training data. Q-BERT uses this improved measure of sensitivity to establish a layer-wise mixed precision quantization scheme, and then quantize each layer at their respective bit-widths using a group-wise granularity in which the matrices are split into sub-units that each have their own quantization range and look-up table. Using their method, the authors achieve 13x compression in the weights, 4x smaller activation size, and 4x smaller embedding size with a maximum accuracy drop of 2.3% from the baseline BERT. Like most QAT techniques, Q-BERT uses the STE in order to approximate the gradients through the non-differentiable quantization function.

In contrast to Q-BERT’s focus on maximizing compression using mixed precision, a contemporaneous work in late 2019 from Zafrir et al. at Intel, Q8BERT, focused instead on applying uniform 8-bit quantization across the model and activations, which is more advantageous from a hardware optimization perspective, as mixed precision operations tend to add overhead and are not conducive to generalized hardware acceleration. Their method performs QAT as a fine-tuning phase on pretrained BERT models to achieve 4x compression with minimal accuracy loss of 1% or less using a simulated quantization approach based on the Jacob et al. 2017 paper we saw earlier in the CNN quantization section, which again uses STE for gradient approximation. The scaling factor for the weights is calibrated using the maximum absolute weight magnitude, and the scaling factor for the activations is based on an exponential moving average that is accumulated during training.

In 2020, Fan et al. presented Quant-Noise, a QAT technique for achieving high rates of model compression by randomly quantizing just a subset of the network weights during each forward pass during training. The allows the majority of weights receive updates without the error introduced by the STE approximation, allowing their values to more accurately shift in ways that reduce the impact of the quantized subset. Over time, this leads to superior results than using QAT on all of the network weights at once. Quant-Noise investigates the use of Product Quantization (PQ), in which multiple weights are quantized together into single codewords, which allows for very high levels of compression with relatively low drops in performance. This approach takes advantage of the correlations between weights induced by the structure of the network.

In 2020, Zadeh et al. from University of Toronto published GOBO, a dictionary-based PTQ method for attention-based models which can use non-uniform quantization to compress nearly all of the FP32 weights in BERT to 3 bits without losing accuracy. The outliers are preserved in full precision to protect accuracy, and are automatically detected using a Gaussian distribution fit, while the rest of the weights are stored with 3-bit codewords which index a small set (8 in the case of 3-bit) of representative FP32 centroids. Since typical hardware cannot perform operations on 3-bit values directly (as the authors of Q-BERT also noted in their study), the GOBO authors develop a novel hardware architecture which enables the efficient acceleration of 3-bit computation to complement their quantization method. While the acceleration benefits are only fully accessible using the specialized hardware, the reduced memory footprint of the 3-bit quantized model will lead to reduced memory storage and traffic on more general hardware, and therefore will still provide some level of improvement in inference latency and energy consumption. The GOBO method was inspired by the storage of Huffman-encoded weights in a dictionary of few representative values (centroids) seen in “Deep Compression,” but in contrast does not require fine-tuning, greatly improves accuracy by not quantizing weight outliers, and uses a novel “centroid selection algorithm that converges 9x faster than K-means and consistently reduces the number of required centroids by half.”

Zhang et al.’s 2020 TernaryBERT combined Knowledge Distillation (inspired by TinyBERT) with ternary quantization in what they call distillation-aware ternarization, treating the ternarized model as a student model to the full-precision, equally-sized teacher. Their distillation method involves MSE minimization between teacher and student for the embedding layer, the outputs and attention scores of transformer layers, and the logits from the prediction layer with soft cross-entropy. TernaryBERT achieves comparable performance with the full-precision baseline, while being 14.9x smaller. The authors find that min-max 8-bit quantization works best for activations in BERT, since they find the distributions in BERT activations to be skewed towards negative values, particularly in the early layers. Their QAT process initializes weights from the full-precision model, and uses the STE to approximate gradients through the quantization function.

In 2020, BinaryBERT sought to bring BERT quantization to the extreme, but observed that a binary BERT model is hard to train directly due to its highly irregular loss landscape. Thus, the authors opt instead to train a half-sized ternary network, convert it into a binary network trough their proposed ternary weight splitting method, then fine-tune the result to achieve 24x compression of BERT with a minimal performance drop. They extend this concept to include adaptive splitting, where the more important layers are split and less sensitive layers are described in binary, to allow flexibility for different model size constraints. Like most QAT methods, BinaryBERT trains using the STE.

In this section, we will discuss practical implementation of quantization to optimize our workflows. As we’ve seen, quantization can be applied during training or as a post-processing step in model development, and the choice between these will depend on the model and use case. To become educated practitioners in quantized workflows, we will first review the current tool sets available to us, along with their strengths and weaknesses. Then, we will use what we’ve learned to develop a decision tree that will help us to align our goals with the methods available.

A popular and easy way to experiment with transformer development and inference is using the Hugging Face Transformers library (along with their model hub, which operates like a GitHub repo for deep learning models, allowing you to easily push your model checkpoints and pull them for use on other machines with just a few lines of code. While this is an amazing interface for people who can operate Python code, it does not provide a non-code user interface (UI) for using the models, and while this library can be used as a capable backend for inference, it generally serves better as a development ecosystem, since more optimized libraries provide faster backend serving, as we will see. However, on the training side, the integration of the bitsandbytes library for enabling efficient quantized training with QLoRA through the intuitive interface of Hugging Face’s Transformers library makes it a very powerful tool for model development.

Starting in July 2019, Nvidia offered the FasterTransformer library to optimize inference throughput of transformer models (originally focused on BERT), and used this to back the TensorRT SDK for their devices. Subsequently, this project has evolved into TensorRT-LLM (aka TRT-LLM), which is a Python API built to look similar to PyTorch, and includes support for GPTQ, AWQ, and an implementation of the SmoothQuant technique. These libraries are specifically engineered to optimize inference on Nvidia hardware or their Triton inference server, so while it is very effective for those using these, it is not a general-purpose quantization library. That being said, for those seeking to use multi-GPU settings with Nvidia hardware, it may be the right choice. The library includes pre-built versions of many popular open-source models, but users can also quantize custom models using the API.

Meanwhile, Georgi Gerganov’s Machine Learning library (GGML), a pure C library designed to accelerate LLM inference on Apple devices, was created in September of 2022, and has been steadily developing since. GGML uses quantization to create structured binary model files that can be used to perform optimized tensor calculations on various hardware. While the library is specifically tailored for Apple silicon, it now provides support for accelerating inference on x86 architectures and GPUs as well. The GGML library provides the backend for the highly popular llama.cpp inference library, which then in turn provides backend support for frontend libraries like Ollama and LM Studio. GGUF is the new and improved file format offered by the GGML library. The drawback to using these compression formats is that they mandate the use of llama.cpp for inference, which is not optimal for many hardware architectures, particularly non-Apple devices. However, llama.cpp has added support multi-GPU/CUDA hardware setups, so user-friendly tools like Ollama are quite effective, even if not the fastest.

vLLM is a powerful inference engine that was introduced in Kwon et al. 2023, which uses optimized CUDA kernels to accelerate performance on Nvidia and AMD GPUs. The authors point out the fact that the one-by-one sequential token generation process of LLMs is memory-bound, and thus underutilizes the computation power of GPUs. To address this, they build the vLLM serving engine on top of their proposed PagedAttention algorithm, which improves memory efficiency of the KV cache to avoid wasted space, crucial for increasing the maximum batch size on a given piece of hardware. Shows 2–4x better throughput than FasterTransformer, with the advantage more pronounced with larger and more complex models and larger sequences. vLLM has seamless integration with Hugging Face models, and supports multi-GPU workflows. The engine works well with GPTQ, AWQ, SqueezeLLM, and FP8 KV cache quantization, allowing models quantized through any of these methods to benefit from the speed benefits of PagedAttention. SmoothQuant+ integration is coming soon.

MLC-LLM is a powerful, universal deployment solution for optimized native deployment of quantized models on multiple types of hardware, including Apple, Nvidia, AMD, Intel, and mobile devices. At the time of this writing, it appears to be the fastest and most general serving engine available, and is preferred by the Jetson AI Lab for its superior throughput. The MLC-LLM library offers a set of prebuilt models, as well as the option to compile new models for use with the library.

LLM Quantization Decision Tree

In this article, we have covered a lot of methodology which creates a dizzying list of design options for quantization. Fortunately, a handful of open-source inference engines have put the most useful of these tools at our fingertips in some intuitive form factors. This decision tree can be used as a very basic rule-of-thumb based on the intended use case and deployment environment, but is not an exhaustive list of considerations. It is intended to give practitioners a launching point for implementing quantization in their workflows.

Q: Are you deploying a pretrained model on CPU or at the edge?

A: Yes — For Apple users, go with GGML-based inference (llama.cpp, Ollama, LM Studio). For Android and x86 hardware, use MLC-LLM.

No — Go to next question.

Q: Will you be serving simultaneous batched requests?

A: Yes — Use vLLM, it is specifically optimized for this.

No — Go to next question.

Q: Are you deploying a pretrained model on a GPU?

A: Yes — MLC-LLM appears to provide state-of-the-art throughput on Nvidia and AMD GPUs, and while it also supports Apple GPUs, llama.cpp has the pedigree for being optimized for Apple hardware, so a comparison is warranted. For Nvidia and AMD GPUs, the forthcoming integration of SmoothQuant+ in the vLLM library will be worth a test when available.

No — Go to next question.

Q: Are you fine-tuning a quantized LLM?

A: Yes — Use QLoRA. This is the most efficient way to enable a quantized LLM to perform new tasks or recover lost performance due to quantization. It is used easily in Hugging Face Transformers.

No — Go to next question.

Q: Are you training a foundation model?

A: Yes — Try using BitNet to prototype or develop foundation models, as it is far cheaper to train and provides competitive performance to full-precision baselines. Even better, try ensembling these low-bit models.

This checklist should provide the average user with a solid guide on where to look given their circumstance. For a more comprehensive guide into LLM inference frameworks, check out this post from Sergei Savvov.

Conclusion

In this article, we’ve seen some incredible feats in quantization research. We’ve seen lossless 8-bit quantization grow into a boring baseline, binary CNNs nearly match their full-precision counterparts, large transformer models trained in 4-bits of precision, and the rise of extreme quantization in LLMs. We’ve seen that INT8 PTQ is fully mature, easily matching full-precision baselines, with wide integration into most open-source libraries, so it is arguable that no one should be serving models using floating point data types >8bit. In the case of LLMs, which are often released in FP16, this lossless INT8 quantization halves memory footprints and provides “free lunch” compression. Further, quantizing at lower precision has been achieved with minimal performance loss, and for many practical use cases, an accelerated and seamless user-experience will be a welcome trade-off for a slight drop in accuracy. The idea of ensembling extremely low-bit networks like we saw in the BENN paper is very compelling, especially as we now see the rise of extreme quantization in LLMs, which may soon benefit from this methodology.

We’ve seen a variety of quantization approaches, catering to different use cases with a range of complexities, and watched their growth through time, revealing which approaches have caught on and become popularized in the open-source community. While PTQ and ZSQ have become very effective, QAT is still fundamentally the most performant approach because it incorporates quantization into the network training process. While this is much more resource intensive, and seems to preclude its use on LLMs by the average practitioner, the ingenuity of QLoRA allows us to blur the line between PTQ and QAT by training only a small number of parameters in low-rank matrices to recover loss due to quantization. We’ve seen PTQ recently become effective down to 2-bits of precision with the HQQ method, and we’ve seen that binary and ternary LLMs can be trained from scratch to nearly match the performance of their full-precision counterparts.

Congratulations on making it to this point. Familiarized with the works reviewed in this article, you now have an expert knowledge on neural network quantization. This survey was not for the faint of heart, but there was no need for a superficial summary of quantization methods in 2024. For proper ML practice, we require a comprehensive foundation of knowledge to make educated decisions that weigh all potential options, as well as the pros and cons of our chosen methodology. Quantization of neural networks is an extremely prolific field of research build on a rich legacy, which poses a daunting research challenge for those wanting to achieve a wholistic understanding. Hopefully, this article provides an accessible, self-contained, and comprehensive resource to practitioners in the field, as well as some effective suggestions for which tools are the best for their common use cases.

Future Work

BitNet does not use low-precision computation during training, and also does not fully capitalize on the binarization or ternarization of its weights, since it does not replace multiplications with addition/subtraction operations. These factors are crippling its ability to realize its true value. The legacy of work in the extreme quantization of CNNs should be applied to binarize the actual mathematics and make the operations used by BitNet more efficient. Further, once optimized kernels are established for BitNet, we should start investigating the ensembling of these low-bit LLMs, as we saw that this was highly effective with CNNs in the BENN paper.

HQQ is a very efficient algorithm for accurate data-free (zero-shot) PTQ, but so far only one optimized inference engine (vLLM) has started adding experimental support for running these models. Further, there are currently no benchmarks on the inference throughput of models quantized using HQQ compared to other methods. Both of these items are open areas for future work."
https://towardsdatascience.com/8-plots-for-explaining-linear-regression-to-a-layman-489b753da696,8 Plots for Explaining Linear Regression to a Layman,"“And don’t use any math” was my manager’s instruction.

How else am I supposed to explain how regression works!?"
https://www.the-scientist.com/advancing-clinical-research-through-effective-data-delivery-71778,Advancing Clinical Research Through Effective Data Delivery,"The clinical research landscape is rapidly transforming. Instead of viewing patients as subjects, sponsors now use the patients’ input to help reduce the burden they face during trials. This patient-centric approach is necessary to ensure that the clinical trial staff recruit and retain enough participants and it has led the industry to modify all stages of the clinical trial life cycle, from design to analysis. “What we are seeing is a lot more openness to innovations, digitization, remote visits for the patient, and telemedicine, for example,” said Rose Kidd, the president of Global Operations Delivery at ICON, who oversees a variety of areas including site and patient solutions, study start up, clinical data science, biostatistics, medical writing, and pharmacovigilance. “It is becoming a lot more decentralized in terms of how we collect clinical data, which is really constructive for the industry, and also hugely positive for patients.”

The Increasing Complexity of Clinical Trials

Accurate data is central to the success of a clinical trial. “Research results are only as reliable as the data on which they are based,” Kidd remarked. “If your data is of high quality, the conclusions of that data are trustworthy.” Sponsors are now collecting more data than ever through their trials.1 This allows them to observe trends and make well-informed decisions about a drug’s or device’s development.

However, these changes in data volume complicate how clinicians design and run their clinical trials. They must capture enough data to fully assess the drug or device without severely disrupting a patient’s lifestyle. Additionally, the investigational sites must ensure that they have enough staff to collect the data in the clinic or through home visits and keep up with their country’s clinical trial regulations. They also must develop efficient data collection and delivery strategies to ensure a trial’s success. While poorly collected data can introduce noise, properly collected data allows clinical trial leads to quickly consolidate and analyze this information.2 And they often require support with this process.

Innovative Solutions to Improve Data Collection and Delivery

Fortunately, sponsors can find that support with ICON, the healthcare intelligence and clinical research organization. “We essentially advance clinical research [by] providing outsourced services to the pharmaceutical industry, to the medical device industry, and also to government and public health organizations,” Kidd explained. With expertise in numerous therapeutic areas, such as oncology, cell and gene therapies, cardiovascular, biosimilars, vaccines, and rare diseases to mention just a few, ICON helps the pharmaceutical industry efficiently bring devices and drugs to the patients that need them, while ensuring patient safety and meeting local regulations.

One of the areas that Kidd’s team is specifically focused on is providing solutions to advance the collection, delivery, and analysis of clinical data.

The platform that ICON provides to support sponsors in this regard not only stores data directly entered into the system by clinicians during their site or home visits, but also serves as an electronic diary for patients to remotely record their symptoms as they happen. This makes it easier for patients to participate in clinical trials while maintaining their jobs and familial responsibilities. Moreover, this solution provides clinical trial staff with insights into their data as they emerge, such as adverse event profiles and the geographical spread of these events. However, this requires that the data is input into the system in the same manner at every participating site.

To address this problem, ICON’s solutions also include a site-facing web portal that helps to reduce the training burden by standardizing data capture and allowing site teams to learn key information about a drug or device. The portal also offers a visit-by-visit guide to ensure that clinicians are asking the necessary questions for a particular visit and helps them remember how to record the data correctly. “It is training at their fingertips when they need it most,” Kidd said. Solutions like these help sponsors obtain the high-quality clinical data that they need to progress from the trial to the market.

Clinical research is evolving and data strategies that support sites and patients alike must similarly evolve. With the right expertise, experience, and technology solutions, ICON is supporting better decision-making by sponsors.

References"
https://consumergoods.com/nestle-uses-ai-and-data-science-battle-environmental-challenges-coffee-production,Nestlé Uses AI and Data Science to Battle Environmental Challenges in Coffee Production,"Nestlé is brewing up data science- and artificial intelligence-powered innovations to breed climate-resilient plants in an environmentally challenged landscape.

With climate change reducing the amount of land that can grow hardy and weather-resilient coffee plants, Nestlé plant scientists have looked to data science to better identify a combination of factors that will yield healthy crops.

Using AI, the company scans a publicly available digital database of coffee traits including cherry size, flavor and aroma characteristics, and yield to create more sustainable coffee cultivation and plant more disease- and drought-resistant beans.

Also read:Keurig Dr Pepper Tests Plastic-Free Pods, Powers Innovation With Consumer Learnings

""In simple terms, our new reference is like a high-quality map of a big city. It will help us identify key genetic markers in the Arabica genome that are responsible for specific traits in adult plants,” said Jeroen Dijkman, head of Nestlé's Institute of Agricultural Sciences, in a statement. “This will help our plant scientists and other experts to better identify, select, and breed new and improved arabica coffee varieties.""

Genome Sequencing of Arabica Coffee Strains

The company has focused on Arabica because it makes up 70% of the world’s coffee production among the existing 120 species. This has posed a challenge, however, as it is more susceptible to disease and has a lower heat tolerance, making it harder to plant successfully amid water shortages and a reduction of arable land.

Also read:Starbucks Reinvention: New Leadership and Green Store Expansion

The company sought a solution to this challenge alongside the French National Institute for Sustainable Development and academic partners, members of a global consortium.

Co-author of a research paper on this effort, Patrick Descombes, who is the senior expert in genomics at Nestlé Research, said the company used state-of-the-art genomics approaches to create an advanced, complete, and continuous arabica reference.

The initiative is part of Nestlé's regenerative agriculture plan, Nescafé Plan 2030.

The coffee segment has been steadily growing. According to research from Circana, coffee outpaced both tea (+4%) and carbonated soft drinks (+3%) in year-over-year servings growth, making it one of the fastest-growing beverage categories globally.

Nestlé reports that coffee is part of five growth platforms that make up 50% of its revenue. Within powdered and liquid beverages, coffee sales grew at a high single-digit rate, per the last call with investors.

The company’s coffee brands include Nescafe, Nespresso, Dolce Gusto, Starbucks Coffee at Home, and Blue Bottle Coffee."
https://www.simplilearn.com/how-to-use-chatgpt-and-excel-for-data-analytics-webinar,Webinar: How to Use ChatGPT & Excel For Data Analytics in 2024,"Dive deep into the transformative world of Data Analytics in an unprecedented webinar, hosted in collaboration with the prestigious Indian Institute of Technology Kanpur. This is your golden ticket to exploring the vistas of data analytics and how it’s shaping the future of industries across the globe. Join us on 30th April, 2024, for a session that promises to elevate your understanding and skills in data analytics to new heights.

Speaker: Amitendra Srivastava

Amitendra Srivastava, a stalwart in Data Analytics with an illustrious academic and professional journey, will guide you through the nuances of data analytics, drawing from his extensive experience in both academia and industry. This webinar is a rare opportunity to gain insights from a luminary who has been at the forefront of analytics research and its practical applications.

Who Should Attend?

This webinar is meticulously crafted for a wide audience spectrum, appealing to:

Aspiring Data Analysts: Kickstart your journey into the world of data analytics.

Experienced Professionals: Enhance your analytical skills and knowledge to stay ahead in your career.

Business Analysts and Managers: Gain insights into data analytics to make informed decisions.

Academicians and Students: Understand the real-world applications of data analytics.

Tech Enthusiasts: Anyone curious about the impact of data analytics in driving business and technology forward.

Why This Webinar Is a Must-Attend?

Participating in this enlightening session will empower you with:

Expert Guidance: Learn directly from Amitendra Srivastava, whose expertise spans the broad spectrum of data analytics.

Industry Insights: Stay abreast of the current trends and future directions in the field of data analytics.

Comprehensive Knowledge: Understand the strategic and practical aspects of data analytics across various domains.

Networking Opportunities: Connect with peers and industry veterans attending the webinar.

Interactive Q&A Session: Get your specific questions answered, ensuring a personalized learning experience.

What You Will Learn

This engaging and interactive webinar will cover a range of topics from foundational concepts to advanced techniques in data analytics, ensuring you leave with a well-rounded understanding of:

Data Analytics Fundamentals: Grasp the basics of data analytics and its significance in today's data-driven world.

Advanced Analytical Techniques: Dive into the world of predictive analytics, machine learning, and big data.

Real-World Applications: Explore how data analytics is being applied across industries to drive decision-making and innovation.

Career Pathways: Discover the vast opportunities and career paths available in the field of data analytics.

Bonus: IITK Professional Certificate in Data Analytics - Program Preview

This segment of the webinar will provide an exclusive sneak peek into the Professional Certificate Course in Data Analytics offered by IIT Kanpur. Highlights include:

Comprehensive Curriculum: Covering the latest in data analytics, machine learning, and statistical analysis.

Hands-on Projects: Real-world projects to build and showcase your analytics capabilities.

Industry Recognition: A certificate from IIT Kanpur, recognized globally among industries and academic circles.

Career Assistance: Guidance and support to help you navigate your career path in data analytics.

Reserve your spot today and take the first step towards a successful career in data analytics."
https://towardsdatascience.com/how-to-generate-videos-with-open-sora-plan-video-generation-model-fdee4151ec90,How to Generate Videos with Open-Sora-Plan Video Generation Model,"In this article, you will learn how to use the Open-Sora-Plan video generation model [1], a powerful model that allows you to create your own videos, as you have seen with OpenAI Sora. I will discuss different tasks to which you can apply the model to, like generating animated videos and creating synthetic datasets. I will then give my thoughts on the model's performance and discuss…"
https://hub.jhu.edu/2024/04/11/data-science-institute-plans-presented-to-city-panel/,Johns Hopkins University presents building plans for Data Science and AI Institute to city panel,"Johns Hopkins University on Thursday presented design plans for the Data Science and Artificial Intelligence (DSAI) facility to the Baltimore City Department of Planning's Urban Planning Architecture Advisory Panel (UDAAP).

DSAI, a cornerstone of JHU's Ten for One strategic plan, will be a leading hub for data science and artificial intelligence to drive research and teaching in every corner of the university and magnify our impact in every corner of the world. The institute will bring together world-class experts in artificial intelligence, machine learning, applied mathematics, computer engineering and computer science to fuel data-driven discovery in support of research activities across the institution.

In all, 80 new affiliated faculty will join JHU's Whiting School of Engineering to support the institute's pursuits, in addition to 30 new Bloomberg Distinguished Professors with substantial cross-disciplinary expertise to ensure the impact of the new institute is felt across the university.

The initiative is led by the Whiting School of Engineering and the new facility will sit adjacent to existing engineering buildings on the south side of the Homewood campus. ""The basic idea is to place the people who generate the data beside the people who analyze the data,"" said Whiting School Dean Ed Schlesinger.

The two-building facility will be organized into ""neighborhoods,"" purposely collocating disciplines including bioengineering, materials, energy and environment with thematic wings dedicated to areas such as health and medicine, scientific discovery, and engineering systems. Unlike traditional academic silos, these groupings will create a world-class AI space for cross-disciplinary research and translation and will help establish an innovation district in the heart of Baltimore.

Just this fall, the federal government designated the greater Baltimore region as a ""Tech Hub."" Through DSAI, Hopkins can make vital contributions to Baltimore's growth as a tech hub, creating long-term jobs, attracting top talent, and spurring the growth of new companies that will compete in one of the world's most promising fields.

""This institute intends to spur a virtuous cycle of new research, product innovation, startups, private investment, and jobs,"" says Christy Wyskiel, senior adviser to the president for innovation and entrepreneurship. ""Our vision is that researchers, entrepreneurs, investors, and companies will look to Baltimore as the place to develop and launch products and companies based on data science and AI technology.""

Design architect and architect of record, ZGF, is designing the DSAI facility to appear as a series of smaller buildings arranged along the site, creatively dispersed and rotated to provide setbacks featuring a raised hillside, trees, and rich landscaping along Remington Avenue.

""This is an amazing site, part of the main campus, yet so connected to the experience of Remington,"" said ZGF design architect Vlad Pajkic. ""The building's concept, based on the deliberate and inventive arrangement of thematic research neighborhoods, will give the facility a more human scale, appropriate for its location and its function.""

The building will target a USGBC LEED Gold Rating. With on-site energy production and electrification, DSAI will be the Homewood campus' first net zero ready carbon facility (excluding emergency generation as mandated by the City of Baltimore). The design, especially storm water and erosion control, will strictly abide by the city's rigorous Site Plan Review Committee process, which prohibits overcharge into adjacent natural habitats or city infrastructure. No building or associated site work will encroach on the university's Forest Conservation Easement.

This project will include significant improvements to the Remington Avenue streetscape and replace three existing structures: the temporary facility that housed the Early Learning Center since 2015, a chiller plant facility that until recently served the Wyman Park Precinct, and the central plant that served the original Merchant Marine Hospital complex. The new facility and the associated open greenspace will also complement the soon-to-be-completed SNF Agora Institute.

The Whiting-Turner Contracting Company and Mahogany Inc. are managing construction for the project, which is anticipated to add 500 trade jobs to boost Baltimore's economy and will indirectly contribute to thousands of new jobs around the city. In alignment with JHU's HopkinsLocal program, the project will also include 20% MBE/WBE and 20% LBE participation. According to Jeff Hargrave, founder and president of Mahogany Inc.: ""Being part of such a transformative project means everything to Mahogany and our city. Johns Hopkins' commitment to improving our city is truly remarkable.""

Preparatory work is scheduled to begin in fall 2024. JHU anticipates the buildings will be occupied by summer 2029."
https://www.datasciencecentral.com/dsc-weekly-16-april-2024/,DSC Weekly 16 April 2024,"Announcements

In today’s constantly evolving digital landscape, networks are the backbone of modern enterprises. The need to prepare for potential network failures by instilling resilience and redundancy is more pressing than ever. Designing a stable, flexible and secure network infrastructure, with real-time visibility across assets and users is critical to maintaining reliability. Tune into the upcoming Strategies for a Resilient Network summit and discover strategies to design an agile, data-driven network that optimizes visibility, enhances DNS management and minimizes disruptions.

Properly managing data is more essential than ever. Organizations now operate among a complicated web of business applications, ushering in extensive amounts of analytical information that can quickly become unwieldy. Without the right oversight, companies miss out on the chance to make the most of this data to drive smarter decision making, strategic planning and even reduce costs. The growing use of generative AI, machine learning and other emerging technologies are posed to transform data management but how can businesses best leverage these platforms to glean the most business value and risk assessment? In the upcoming Data Management: Navigating Opportunities for Success summit, leading experts in the field will discuss the latest data management strategies as well as what’s next in data analytics and architecture.

Top Stories

Retrieval augmented fine-tuning and data integrations

April 16, 2024

by Dan Wilson

In the latest episode of the “AI Think Tank Podcast,” I had the pleasure of hosting a deep dive into the world of AI advancements, specifically focusing on “RAFT” (Retrieval Augmented Fine Tuning). Joining me were the esteemed guests Suman Aluru and Caleb Stevens, who both have much to do with AI infrastructure and application. Our conversation revolved around how RAFT bridges the critical gaps between fine-tuning and retrieval-augmented generation (RAG), and the significant impact this has on AI-driven applications.

How is machine learning changing the landscape of FinTech?

April 11, 2024

by Pritesh Patel

Machine learning in FinTech is a critical enabler in tech-driven banking, where efficiency and innovation are key to staying ahead of the competition. It transforms obstacles into lucrative possibilities by revolutionizing crucial areas such as risk management, fraud detection, algorithmic trading, and compliance.

Get ready for future innovations with large language models

April 12, 2024

by Prasanna Chitanand

Nowadays, almost all businesses use generative AI and large language models after realizing their ability to boost accuracy in various tasks. These AI models have become the topic of social media discussions nowadays. This blog explores more on the business and commercial uses of LLMs and genAI along with the differences between them.

In-Depth

Using window functions for advanced data analysis

April 15, 2024

by Erika Balla

Window functions are an advanced feature of SQL that provides powerful tools for detailed data analysis and manipulation without grouping data into single output rows, which is common in aggregate functions. These functions operate on a set of rows and return a value for each row based on the calculation against the set.

5 mistakes to avoid in CMMC compliance

April 12, 2024

by Erika Balla

Think of a battlefield — not filled with soldiers but cyber warriors. The­ Defense Industrial Base­ (DIB) stands as the front line. This digital battleground face­s nonstop cyberattacks, each one ge­tting trickier. Here, the­ Department of Defe­nse uses the Cybe­rsecurity Maturity Model Certification (CMMC) 2.0 program to prote­ct sensitive, unclassified information.

Building reliable and efficient ETL pipelines: Best practices for data wranglers

April 11, 2024

by Ovais Naseem

Data is crucial for your business—it helps with decisions and growth. But sometimes, it’s stuck in different places and hard to use. Implementing an ETL Pipeline is like sharpening that blurry map and fixing the broken compass—it turns frustration into clarity! But there’s good news! An ETL pipeline can help.

The new era of data handling: Tools that transform business strategies

April 10, 2024

by Ovais Naseem

Data Automation Tools play a crucial role in transforming how businesses handle data. They offer advanced functionalities that streamline data management processes, enabling organizations to enhance efficiency and accuracy. By automating tasks such as data entry, validation, and analysis, these tools reduce manual intervention and minimize the risk of errors.

DSC Weekly 9 April 2024

April 9, 2024

by Scott Thompson

Read more of the top articles from the Data Science Central community."
https://towardsdatascience.com/moirai-time-series-foundation-models-for-universal-forecasting-dc93f74b330f,Moirai: Time Series Foundation Models for Universal Forecasting,"This post was co-authored with Rafael Guedes.

Introduction

The development of time series foundation models has been accelerating over the last two quarters, and we have been witnessing the release of a new model nearly every month. It started with TimeGPT [1] in…"
https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450,ORPO: Preference Optimization without the Supervised Fine-tuning (SFT) Step,"There are now many methods to align large language models (LLMs) with human preferences. Reinforcement learning with human feedback (RLHF) was one of the first and brought us ChatGPT, but RLHF is very costly. DPO, IPO, and KTO are notably…"
https://news.unchealthcare.org/2024/04/haendel-to-join-unc-genetics-pediatrics-data-science-society-to-advance-precision-medicine-in-nc/,"Haendel to join UNC Genetics, Pediatrics, Data Science & Society to Advance Precision Medicine in NC","Melissa Haendel, PhD, and her lab of 25-plus members will join UNC-Chapel Hill to focus on early diagnosis of childhood diseases and significantly improve quality of life outcomes.

The University of North Carolina at Chapel Hill is delighted to welcome Dr. Melissa A. Haendel, PhD, FACMI, as the Sarah Graham Kenan Distinguished Professor in the Department of Genetics within the School of Medicine, starting April 15. Haendel’s groundbreaking work in biomedical informatics is set to significantly contribute to our pursuit of advanced healthcare solutions. In addition to her primary appointment, she will bring her considerable knowledge to the roles of professor in both the Department of Pediatrics and the forward-thinking School of Data Science and Society.

She joins UNC-CH from the University of Colorado Anschutz Medical Campus, where she leads the Translational and Integrative Sciences Laboratory and is the Chief Research Informatics officer, Marsico endowed chair in data science, and a Professor of Biomedical Informatics in the CU School of Medicine.

Expressing her enthusiasm, Haendel remarks, “The opportunity to collaborate with the vibrant communities at UNC-CH, UNC Health, and throughout North Carolina is incredibly exciting. We are poised to make genomic medicine a reality for all North Carolinians, leveraging our team’s diverse skills to tackle the most pressing issues in precision medicine.”

Haendel will serve as the Director of Precision Health & Translational Informatics and contribute her expertise as Deputy Director of Computational Science at NC TraCS, the hub of the UNC Clinical and Translational Science Award. Additionally, she will advise on Research Data Interoperability within the UNC Health System and participate in both the Computational Medicine Program and the Program in Precision Medicine and Healthcare. This combination of roles is expected to greatly extend the reach of her impactful work.

Emily Pfaff, PhD, assistant professor in the UNC Department of Medicine,commented “Melissa’s emphasis on the importance of collaboration and team science in informatics will make her an asset to our CTSA. She brings a number of exciting projects emphasizing critical areas in translational science, including real world data analysis, data linkage, and using informatics methods to address rare diseases and rural health disparities. As a long-time collaborator of Melissa’s, I could not be more excited to welcome her and her team to UNC Chapel Hill.”

“We are absolutely delighted that Dr. Melissa Haendel is joining us in the UNC School of Medicine,” said Blossom Damania, PhD, Vice Dean for Research, School of Medicine. “Dr. Haendel is an international leader in informatics and data science. She is an exceptional scientist with a remarkable vision to advance public health surveillance for rare diseases. Melissa has already established significant collaborations with many faculty across the School of Medicine, UNC campus, and UNC Health. Her research program will advance the early diagnosis of childhood diseases and significantly improve quality of life outcomes. We are thrilled to have Dr. Haendel as part of our community.”

Haendel characterizes her work as “the art of data translation,” leading initiatives that aim to improve data integration and promote collaborative education. She has played a pivotal role in enhancing national data sharing and interoperability – particularly notable during the COVID-19 pandemic with the creation of the National COVID Cohort Collaborative Data (N3C), which has been vital in our understanding of the virus. This work earned her and her team the NIH/FASEB DataWorks! Grand Prize in 2023.

“We are enormously excited and grateful that Melissa has decided to join us in the Department of Genetics. Her recruitment is perfectly aligned with the department’s strategic investments over the past four years in support of team science,” said Fernando Pardo-Manuel de Villena, PhD, professor and chair of the Department of Genetics. “Her program is an exceptional example of how to accelerate and multiply the impact of research conducted by faculty and trainees in a basic science department in the School of Medicine.”

Haendel also co-founded the Monarch Initiative, an international consortium dedicated to the integration of model organism and human data to support rare disease diagnostics and mechanism discovery. She brings with her to UNC a National Human Genome Research Institute Center of Excellence in Genome Sciences that is dedicated to making phenotypic data computable for genomic health applications.

“The talents and expertise brought by Haendel and her team will enable us to dramatically improve and streamline the way that patients with rare disease are identified, diagnosed, and cared for within the UNC Health system. We are also excited about the many opportunities for collaboration on genomic medicine research projects led by faculty in the Department of Genetics”, said Jonathan Berg, MD, PhD, the Bryson Distinguished Professor in the UNC Departments of Genetics and Medicine.

After earning her doctorate in neuroscience at the University of Wisconsin, she completed post-doctoral fellowships in developmental biology at the University of Oregon and in toxicology at Oregon State University. In 2010, Haendel joined the Oregon Health & Science University faculty. She joined the University of Colorado Anschutz Medical Campus in 2021. Haendel is currently a principal investigator on 10 externally funded programs totaling more than $25 million per year. She is the author of more than 190 peer-reviewed research articles and 57 book chapters, editorials, and reviews. Her work is frequently referenced in the scientific literature, with more than 20,000 citations to date."
https://news.yale.edu/2024/04/17/creating-global-impact-through-yales-data-driven-social-sciences,Creating global impact through Yale’s data-driven social sciences,"This story is the latest in a series about Yale’s evolution under President Peter Salovey as he prepares to return to the faculty later this year.

To understand how people moved around during the COVID-19 pandemic, Yale sociologist Emma Zang needs data — a lot of it. When asked, she reels off a list of datasets she works with to analyze patterns: voter-registration data, cell-phone location data, credit-card transaction data, X (formerly Twitter) data, and net-light data that captures population density through satellite imagery showing the geographic areas glowing brightest at night.

Traditionally, mapping migration patterns required survey data, which takes great effort and time to compile, Zang explains.

“By the time you collect the data, clean it, and use it, it’s already two years old, which limits its usefulness,” said Zang, an assistant professor of sociology in Yale’s Faculty of Arts and Sciences (FAS). “These new, innovative datasets are game-changers.”

For Zang and other social scientists, the emergence of large, computerized datasets containing information from the government, nonprofit sector, and private enterprise is altering how they approach their work — and Yale, during President Peter Salovey’s presidency, has responded by investing broadly to facilitate this work and other aspects of data-driven social science.

In the past decade, the university has launched bold initiatives and built key infrastructure to create a collaborative campus environment for data-driven social science that is being used to guide domestic policy, address climate change and other pressing global challenges, and which seeks to understand human cognition, values, and behavior.

“Social science research has the potential to shape policy and effect change on a global scale,” said Salovey, a distinguished psychologist who will return to social science research and teaching when he steps down at the end of the academic year. “That requires bringing together deep expertise on data, on theory, and on policy, facilitating collaboration, convening leaders and guiding important conversations, and pursuing insights into human relationships and society.

“At Yale, we are committed to leading the way in this field, to understand urgent global challenges and increase knowledge, without partisanship or ideology.”

Since Salovey began his tenure, in 2013, the university has strengthened the social science faculty by hiring top-flight scholars, many of whom have taken on leadership roles at the university. It has enhanced social-science education by transforming the Department of Statistics into the Department of Statistics and Data Science and establishing a major in economics and computer science to teach undergraduates the skills needed to harness data for a better understanding of the world.

Yale has also opened several innovative and cross-cutting centers for research. These include the Data-Intensive Social Science Center, which helps scholars across the university access and manage the comprehensive, complex datasets that increasingly drive boundary-expanding social science research; the Institute for Foundations of Data Science, which applies the methodology of data science across multiple disciplines; and the Tobin Center for Economic Policy, which brings Yale’s widely recognized excellence in economics and other fields to bear on public policy.

In 2021, the university launched the Wu Tsai Institute, an interdisciplinary research center that combines neuroscience, social science, and data science to accelerate breakthroughs unlocking the mysteries of human cognition.

And in 2022, Yale established the Yale Jackson School of Global Affairs, the university’s first new professional school since 1976.

“These are all remarkable accomplishments,” said Alan Gerber, Sterling Professor of Political Science in the FAS, who served as its inaugural dean of social science from 2014 to 2021 and currently directs Yale’s Institution for Social and Policy Studies. “Through them, President Salovey has created a fertile environment that encourages collaboration and enables social scientists in the FAS and professional schools to pursue ambitious, exciting research that has a real-world impact.”

New areas of excellence

Over the past 10 years, Yale has built on its historic strengths while advancing new techniques in data and analysis.

The Tobin Center, established in 2019, exemplifies the approach, harnessing recent advances in economics, data science, and analytics to conduct rigorous, evidence-based research that helps define and inform policy debates.

“Yale is traditionally strong in economics,” Gerber said. “The Tobin Center is expanding that traditional excellence into a new area of enormous public importance.”

The center unites researchers across campus in the pursuit of policy-relevant scholarship while connecting them with lawmakers and government officials at the local, state, and federal levels.

For example, faculty from the Yale School of Management, FAS, and the Yale School of Public Health recently met with administrators from Connecticut’s Medicaid program to discuss how the latest evidence-based research might strengthen the state’s health system. The center’s staff includes data analysts and investigators to support scholarship, as well as veterans of the policy process to help faculty share their work with lawmakers.

SELECTED MILESTONES IN DATA-DRIVEN SOCIAL SCIENCES

While the Tobin Center supports research primarily in the domestic sphere, the Yale Jackson School of Global Affairs provides opportunities for social scientists to tackle international issues. Its faculty includes economists, political scientists, historians, and anthropologists studying agricultural markets in sub-Saharan Africa, authoritarian regimes and transitions to democracy, mental health and child development among refugee populations, and education policy in developing countries, among other issues of global importance.

Aside from their research, the school’s faculty members are helping to train a new generation of leaders equipped to embrace the use of data to inform policy. The school aims to give its students deep knowledge of the world around them, fluency with data, and an agility working across disciplines that will equip them to meet global challenges.

“The Jackson School is a unique convening space for scholarship on a range of pressing global issues. We have a multidisciplinary faculty — with top scholars from the humanities, social sciences, and Yale’s other professional schools as well as senior practitioners of global affairs — in dialogue with one another,” said Jim Levinsohn, dean of the Jackson School and Charles W. Goodyear Professor in Global Affairs. “We’re training our students to take this cross-disciplinary, collaborative approach and bring fresh insight to urgent problems facing the world.”

The Wu Tsai Institute, established through a gift from Joseph Tsai ’86, ’90 J.D. and Clara Wu Tsai, bridges multiple fields of neuroscience research, from biology to psychology, and data science to engineering. Its leadership team includes three social scientists and a neuroscientist: Nicholas Turk-Browne, a professor of psychology and the institute’s director; Kia Nobre, Wu Tsai Professor of Psychology, associate director of the institute and director of the Center for Neurocognition and Behavior; John Lafferty, the John C. Malone Professor of Statistics and Data Science and director of the Center for Neurocomputation and Machine Intelligence; and Daniel Colón-Ramos, the Dorys McConnell Duberg Professor of Neuroscience and Cell Biology and director of the Center for Neurodevelopment and Plasticity.

The institute has united more than 150 faculty across disciplines, including from four social science departments, in a moonshot effort to reveal the secrets of human cognition.

“President Salovey and Joe Tsai conceived of the Wu Tsai Institute over a breakfast in La Jolla, California,” said Turk-Browne. “They recognized that, in an era obsessed with the intelligence of machines, the remarkable capabilities of humans are not yet understood, including our ability to perceive, remember, think, decide, and create. These wonders of human cognition are the basic ingredients of all knowledge, relationships, organizations, and pursuits, and in this way, their understanding is fundamental for advancing social science.”

‘One giant sandbox’

Perhaps no development has been more important to the study, teaching, and practice of social science over the past several years than the proliferation of large data sets. The vast quantity of information now available informs the study of health care, education, political polarization, immigration, economic inequality, and climate change, among other pressing issues.

“Decades ago, a lot of social science research was largely theoretical, based on small datasets, or methodologically focused on statistical techniques that were designed to compensate for the relatively poor quality of data,” said Steven Berry, the David Swensen Professor of Economics in the FAS and the inaugural faculty director of the Tobin Center for Economic Policy.

“Computerization has made many new sources of data available, which become potential datasets for the study of human behavior, social behavior, and economic behavior.”

The rise of these massive datasets presents challenges. While the Marx Science and Social Science Library provides researchers access to many important datasets, they also must negotiate user agreements with the vendors that compile them, devise way to securely store the data that often includes confidential information, and learn computing techniques, including machine learning and artificial intelligence, to analyze it.

In 2022, Yale established the Data-Intensive Social Science Center (DISSC) to help researchers across FAS and Yale’s professional schools — the faculties of Yale School of the Environment, the Yale School of Management, Yale School of Public Health, the Yale Jackson School of Global Affairs, and Yale Law School all include social scientists — looking to access, analyze, and preserve enormous amounts of data.

Faculty members have always found ways to access and analyze the data they need, but centralized support offered by the DISSC accelerates that process and ensures all researchers are supported, said Gerber, who serves as the DISSC’s faculty director along with Berry. Creating a center that accelerates the adoption and ease the use of methods like AI and machine learning, Gerber said, is a “truly visionary effort.”

“To do that as a university service open to all faculty in the social sciences distinguishes Yale from others.”

In its first year of operation, the DISSC provided programming to help faculty stay apprised of the latest innovations in analyzing and managing data, began helping faculty negotiate the cumbersome process of reaching user agreements with vendors for access to datasets, and launched a pilot project with Yale Amazon Web Services to create secure space on the cloud to safely store datasets.

Storing data in a centralized location allows multiple researchers to work without having to move the datasets around or make copies of them, explained Ron Borzekowski, the DISSC’s inaugural director. If a researcher adds value to data by cleaning it or appending new variables, then the enhanced version becomes available to anyone with login authorization, he said.

“Centralization is the key,” Borzekowski said. “Instead of working in four or five distinct silos, they’ll be working in one giant sandbox.”

The DISSC embodies Salovey’s goal of creating a more unified Yale, Berry said.

“It allows social scientists across the university to benefit from the same infrastructure,” he said. “When a problem is solved once or progress is made one project, or for one individual, that progress becomes available to other researchers.

“I don’t know of any other university in the country that is trying this at this scale.”"
https://www.simplilearn.com/data-analyst-job-description-article,"Data Analyst Job Description [Skills, Roles and Responsibilities]","Reviewed and fact-checked by Sayantoni Das

Data analysis is a critical field that is pivotal in interpreting complex data to help businesses make informed decisions. With the advent of big data, the demand for skilled data analysts has surged across various industries. Here's a detailed exploration of the data analyst job, including what data analysis entails, the role of a data analyst, their importance in today's data-driven world, and what skills are required for data analysts.

Key Takeaways:

Data Analysts plan decision-making, improve efficiency, enhance customer experiences, and manage risks by interpreting complex data sets.

Success in data analysis demands a blend of technical skills (data cleaning and statistical analysis) and soft skills (critical thinking and communication).

The surge in data usage has significantly increased the demand for skilled data analysts across various sectors.

Watch the video below that will help you have an understanding of the various responsibilities, skills required, and the salary structure of top Data Analytics job roles.

What Is Data Analysis?

Data analysis involves collecting, processing, and performing statistical analyses on large datasets to discover useful information, suggest conclusions, and support decision-making. It encompasses various techniques under various umbrellas, such as descriptive statistics, exploratory data analysis (EDA), and inferential statistics, to interpret and understand the patterns and behaviors within data. Data analysis aims to extract actionable insights from raw data that can influence strategies and operations.

Become a Data Science & Business Analytics Professional

28%Annual Job Growth By 2026

11.5 MExpected New Jobs For Data Science By 2026

Data Analyst

Industry-recognized Data Analyst Master’s certificate from Simplilearn

Dedicated live sessions by faculty of industry experts

Post Graduate Program in Data Analytics

Post Graduate Program certificate and Alumni Association membership

Exclusive hackathons and Ask me Anything sessions by IBM

prevNext

Here's what learners are saying regarding our programs:

Gayathri Ramesh

Associate Data Engineer , Publicis Sapient

The course was well structured and curated. The live classes were extremely helpful. They made learning more productive and interactive. The program helped me change my domain from a data analyst to an Associate Data Engineer.

Felix Chong

Project Manage , Codethink

After completing this course, I landed a new job & a salary hike of 30%. I now work with Zuhlke Group as a Project Manager.

prevNext

Not sure what you’re looking for?View all Related Programs

What Does a Data Analyst Do?

A data analyst systematically collects, processes, and performs statistical analyses on data sets. Their responsibilities include:

Data Cleaning and Preparation: This involves filtering the data, handling missing values, and preparing the dataset for analysis to ensure accuracy and relevance.

Data Exploration and Analysis: Analysts use statistical tools and techniques to explore and analyze data, identifying patterns, relationships, and trends.

Data Visualization: They create visual representations of data findings through charts, graphs, and dashboards to make the data understandable at a glance.

Reporting: Data analysts prepare reports and presentations to communicate the insights and findings from the data to stakeholders, which can influence policy and decision-making processes.

Collaboration: They often work with other departments to understand their data needs and help them make informed decisions based on data insights.

Importance of Data Analysts in Today’s Data-Driven World

In today's data-driven world, data analysts are indispensable and play a crucial role in various aspects:

Strategic Decision-Making: Data analysts provide the groundwork for strategic decision-making by uncovering trends and insights that can guide business strategies and improve outcomes.

Improving Efficiency: By identifying areas of inefficiency within operations, data analysts help organizations streamline processes, reduce costs, and increase productivity.

Enhancing Customer Experiences: Analyzing customer data allows businesses to understand customer behaviors and preferences, leading to improved products and services.

Risk Management: Data analysis helps identify potential risks and challenges, enabling businesses to devise strategies to mitigate these risks.

Data Analyst Job Description: Roles and Responsibilities

A Data Analyst job description typically outlines the key roles, responsibilities, and qualifications required for the position. Data Analysts are tasked with turning data into information, information into insight, and insight into business decisions. Below is a detailed job description highlighting the roles and responsibilities of a Data Analyst position.

Job Overview

The Data Analyst is responsible for overseeing our data systems and reporting frameworks, guaranteeing the integrity and precision of data. The ideal candidate will transform raw data into structured information, which will then be analyzed to glean insights that drive strategic business decisions. This position encompasses a comprehensive analysis lifecycle, covering requirement gathering, activity execution, and design planning. Data analysts are tasked with enhancing analytical and reporting functions, as well as supervising performance and quality assurance processes to pinpoint areas for enhancement.

Roles and Responsibilities

Gather data from primary and secondary sources, ensuring the upkeep of databases and data systems.

Detect, examine, and decode trends or patterns within intricate datasets.

Cleanse data and scrutinize computer-generated reports and outputs to identify and rectify coding errors.

Coordinate with management to align business and informational priorities.

Identify opportunities for process enhancements.

Employ statistical techniques to scrutinize data and produce actionable business insights.

Collaborate with the management team to determine and rank the needs of different business units.

Develop data dashboards, charts, and visual aids to support decision-making across departments.

Convey insights through both reports and visual presentations.

Partner with engineering and product development teams to understand business requirements.

Engage with managers from various departments to specify data requirements for analysis projects tailored to their unique business processes.

Skills and Qualifications

Possess a solid foundation in statistics and practical experience with statistical software (such as Excel, SPSS, SAS) and mastery in data analysis languages including SQL, Python, and R.

Exhibit exceptional analytical abilities to compile, structure, examine, and present substantial data sets with precision and thoroughness.

Capable of critically evaluating data to derive meaningful, actionable insights.

Demonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background.

A bachelor's degree in Computer Science, Information Management, Statistics, or a comparable discipline is required, with prior experience in data analysis or a related field being advantageous.

Additional Requirements

Adept at report writing and presenting findings.

Ability to work under pressure and meet tight deadlines.

A deep understanding of the industry and business operations is a plus.

Become a Data Science & Business Analytics Professional

28%Annual Job Growth By 2026

11.5 MExpected New Jobs For Data Science By 2026

Data Analyst

Industry-recognized Data Analyst Master’s certificate from Simplilearn

Dedicated live sessions by faculty of industry experts

Post Graduate Program in Data Analytics

Post Graduate Program certificate and Alumni Association membership

Exclusive hackathons and Ask me Anything sessions by IBM

prevNext

Here's what learners are saying regarding our programs:

Gayathri Ramesh

Associate Data Engineer , Publicis Sapient

The course was well structured and curated. The live classes were extremely helpful. They made learning more productive and interactive. The program helped me change my domain from a data analyst to an Associate Data Engineer.

Felix Chong

Project Manage , Codethink

After completing this course, I landed a new job & a salary hike of 30%. I now work with Zuhlke Group as a Project Manager.

prevNext

Not sure what you’re looking for?View all Related Programs

20 Essential Data Analyst Skills

Data analysts are pivotal in transforming raw data into insights that can drive business decisions. To excel in this field, the skills for data analyst range from technical proficiency to soft skills. Here’s a detailed look at the 20 essential skills for data analysts:

1. Data Cleaning and Preparation

The ability to clean and prepare data is fundamental. This involves handling missing data, removing outliers, and ensuring that the dataset is in a usable format for analysis.

2. Data Analysis and Exploration

Data analysts must use statistical methods and techniques to explore and analyze data, identifying patterns, trends, and relationships within the dataset.

3. Statistical Analysis

A robust understanding of statistical concepts and methods is crucial for interpreting data and performing analyses that can inform decision-making processes.

4. Programming

Knowledge of languages such as Python, R, or SQL is essential for manipulating data, automating tasks, and performing complex analyses.

5. Database Management

Skills in managing and querying databases are vital for accessing and managing large datasets, using SQL for structured databases or NoSQL for unstructured data.

6. Creating Dashboards and Reports

The ability to create interactive dashboards and detailed reports is important for presenting data insights in a clear and accessible manner to stakeholders.

7. Data Visualization

Proficiency in data visualization tools and techniques, such as Tableau or Power BI, helps present data findings visually to enhance understanding and decision-making.

8. Machine Learning

Understanding basic machine learning concepts and algorithms enables data analysts to apply predictive models and enhance their analysis with predictive insights.

9. Excel

Mastery of Excel is crucial for data analysis, especially for handling smaller datasets, performing quick analyses, and creating pivot tables and charts.

10. Critical Thinking

Thinking critically and evaluating information from multiple perspectives is essential for drawing accurate conclusions from data.

11. Attention to Detail

Data analysis requires a deep understanding and a keen eye for detail to ensure data cleaning, analysis, and reporting accuracy.

12. Communication

Strong communication skills (oral and written) are necessary for translating complex data findings into understandable insights for non-technical stakeholders.

13. Problem-Solving

The ability to identify problems, analyze potential solutions, and implement the most effective solution is critical in data analysis projects.

14. Teamwork

Collaboration with other team members, departments, and stakeholders is essential for successfully completing projects and achieving business objectives.

15. Ethical Judgment

Data analysts must navigate ethical considerations, ensuring privacy, data security, and ethical use of data in their analyses.

16. Domain Knowledge

Understanding the industry or sector in which they operate helps analysts contextualize data findings and provide more relevant insights.

17. Statistical Programming

Skills in statistical programming languages like R can enhance an analyst’s ability to perform advanced statistical analyses and modeling

18. Management

The ability to manage projects, time, and resources effectively is important for meeting deadlines and managing multiple tasks simultaneously.

19. Modeling

Knowledge of data modeling techniques and tools is essential for creating representations of complex data systems to support analysis and decision-making.

20. Python

Proficiency in Python is highly valued due to its versatility, extensive libraries for data analysis (Pandas, NumPy) and machine learning (Scikit-learn), and its readability and ease of use.

These Data Analyst skills empower them to navigate their role's complexities, making them invaluable assets in leveraging data for strategic advantages.

How Do You Highlight Your Skills in a Resume?

Highlighting your data analyst technical skills in a resume is crucial to stand out in the job application process. It's not just about listing your skills but presenting them in a way that demonstrates your proficiency and how they align with the job you're applying for. Here are strategic ways to highlight your skills in a resume:

1. Tailor Your Skills to the Job Description

Analyze the Job Posting: Identify the skills and qualifications the employer is looking for by carefully reading the job description.

Match Your Skills: Tailor your skills section and professional experience to reflect the skills listed in the job description, emphasizing those where you have strong competencies.

2. Use a Dedicated Skills Section

Create a Skills Section: Include a specific section titled ""Skills"" or ""Core Competencies"" near the top of your resume, after your contact information and summary statement.

Categorize Your Skills: Organize your skills into categories such as Technical Skills, Soft Skills, and Languages.

3. Quantify Your Achievements

Use Numbers and Metrics: Use quantitative measures to add context to your achievements wherever possible. For instance, ""Increased sales by 20% through targeted data analysis and customer segmentation.""

Project-Based Examples: Highlight specific projects or tasks that demonstrate your skills and the positive outcomes that resulted from your actions.

4. Incorporate Skills into Your Professional Experience

Action Verbs: Start bullet points that showcase your skills in action, such as ""Analyzed,"" ""Developed,"" and ""Managed.""

Skill Application: Describe how you applied specific skills to achieve results in your previous roles, focusing on scenarios that align with the prospective job's requirements.

5. Highlight Transferable Skills

For Career Changers: If you're changing careers, emphasize transferable skills. For example, project management, communication, and problem-solving skills are valuable in many roles.

Contextualize Your Skills: Provide examples of how you've successfully applied these skills in different settings or industries.

6. Leverage the Resume Summary or Objective

Summarize Key Skills: Use your resume summary or objective to highlight critical skills that make you a strong candidate for the position, especially if they are directly mentioned in the job listing.

Personalize Your Introduction: Tailor this section to reflect relevant skills that align with the job you're applying for.

7. Incorporate Keywords from the Job Description

ATS Optimization: Companies use Applicant Tracking Systems to screen resumes. Including keywords from the job description ensures your resume passes these initial screenings.

Natural Integration: Integrate these keywords naturally into your skills section, professional experience, and summary to improve your resume's visibility.

8. Proofread and Format Consistently

Attention to Detail: Ensure your resume is free from typos and grammatical errors, which can detract from your professionalism and attention to detail.

Consistent Formatting: Use a clean, professional format with consistent fonts, sizes, and bullet points to make your resume easy to read.

Become a Data Science & Business Analytics Professional

28%Annual Job Growth By 2026

11.5 MExpected New Jobs For Data Science By 2026

Data Analyst

Industry-recognized Data Analyst Master’s certificate from Simplilearn

Dedicated live sessions by faculty of industry experts

Post Graduate Program in Data Analytics

Post Graduate Program certificate and Alumni Association membership

Exclusive hackathons and Ask me Anything sessions by IBM

prevNext

Here's what learners are saying regarding our programs:

Gayathri Ramesh

Associate Data Engineer , Publicis Sapient

The course was well structured and curated. The live classes were extremely helpful. They made learning more productive and interactive. The program helped me change my domain from a data analyst to an Associate Data Engineer.

Felix Chong

Project Manage , Codethink

After completing this course, I landed a new job & a salary hike of 30%. I now work with Zuhlke Group as a Project Manager.

prevNext

Not sure what you’re looking for?View all Related Programs

Data analysts employ many tools to collect, process, analyze, and visualize data. These tools extract insights from complex datasets, facilitating data-driven decision-making. The choice of tools can depend on the requirements of the task at hand, including the nature of the data, the complexity of the analysis, and the preferred reporting methods. Below, we delve into some of data analysts' most commonly used tools and the reasons for their usage.

1. SQL (Structured Query Language)

Usage: SQL is used for managing and querying relational databases. It allows analysts to extract and manipulate data captured in a structured format within a database.

Why: SQL is essential for data analysts due to its powerful querying capabilities, enabling them to retrieve, filter, and transform data for analysis efficiently.

2. Python

Usage: Python is a versatile programming language that's widely used for data analysis, machine learning, automation, and data visualization. Libraries like Pandas, NumPy, and Matplotlib expand their functionality for data tasks.

Why: Python's simplicity, readability, and extensive libraries make it a favorite among data analysts for handling and analyzing large datasets, as well as for building complex data models.

3. R

Usage: R is a programming language and environment specialized for statistical computing and graphics. It's used for data analysis, statistical modeling, and data visualization.

Why: R is particularly valued for its vast array of packages for statistical analysis, making it ideal for conducting sophisticated statistical tests and exploratory data analysis.

4. Excel

Usage: Microsoft Excel is a spreadsheet program for data entry, manipulation, and basic analysis. It's particularly useful for handling small to medium-sized datasets and performing quick analyses.

Why: Excel's ease of use, widespread availability, and powerful features like pivot tables and functions make it a staple for preliminary data analysis, reporting, and visualization tasks.

5. Tableau

Usage: Tableau is a data visualization tool for creating interactive and shareable dashboards. It allows analysts to visualize data insights and trends in an accessible format.

Why: Tableau is favored for its user-friendly interface, ability to handle large volumes of data, and the dynamic visualizations it creates, facilitating better data storytelling and decision-making.

6. Power BI

Usage: Power BI is a business analytics service by Microsoft that provides tools for aggregating, analyzing, visualizing, and sharing data.

Why: It integrates seamlessly with other Microsoft products and offers robust data visualization capabilities, making it an excellent tool for creating comprehensive reports and dashboards.

7. Google Analytics

Usage: Google Analytics serves as a web analytics tool, monitoring and reporting on website traffic. It plays a crucial role in analyzing user interactions on websites and assessing the effectiveness of digital marketing initiatives.

Why: Data analysts use Google Analytics to gather insights into user engagement, demographics, and acquisition channels, which are crucial for optimizing web presence and marketing strategies.

8. SAS (Statistical Analysis System)

Usage: SAS is a comprehensive software suite designed for sophisticated analytics, including multivariate analysis, business intelligence, data management, and predictive analytics.

Why: Renowned for its powerful statistical analysis features, it is apt for tackling intricate data analysis challenges across sectors such as healthcare, finance, and pharmaceuticals.

9. Statistical Package for the Social Sciences

Usage: SPSS is a software package used for statistical analysis. It's commonly used in social sciences for analyzing survey data, market research, and experiment data.

Why: SPSS is preferred for its user-friendly interface, making it accessible for analysts who may not have a strong programming background.

10. Git

Usage: Git is a version control system that lets multiple users collaborate on the same projects without conflict. It's used for code management in data analysis projects.

Why: It's essential for data analysts working in teams to ensure that changes to code and analysis are tracked, allowing for collaboration and historical comparison of data analysis projects.

Data Analyst Qualifications: What Does it Take

Becoming a data analyst requires a combination of educational background, technical skills, and certain soft skills that enable one to analyze data and communicate findings effectively. Here’s a comprehensive breakdown of what it takes to become a data analyst:

Educational Qualifications

Bachelor’s Degree: Most data analyst positions require at least a bachelor's degree.

Master’s Degree (Optional): While not always necessary, a master's degree can be advantageous for more advanced positions or specialized fields.

Data Analyst Technical Skills

Programming Languages: Proficiency in languages for data manipulation, statistical analysis, and machine learning. SQL knowledge is crucial for extracting and querying data from databases.

Statistical Analysis: A foundation in statistics is essential for interpreting data and conducting various analyses to extract meaningful insights.

Data Visualization: Understanding tools like Tableau, Power BI, or even Python libraries (e.g., Matplotlib, Seaborn) to create compelling visual representations of data findings.

Machine Learning: Understanding basic machine learning concepts and algorithms can be beneficial, especially for roles that overlap with data science.

Software Proficiency: Familiarity with spreadsheet tools like Microsoft Excel for data entry, manipulation, and preliminary analysis. Experience with database management software and data analytics platforms is also valuable.

Soft Skills

Analytical Thinking: Think critically and analytically to solve problems and make data-driven decisions.

Attention to Detail: Precision is key in data analysis to ensure data cleaning, analysis, and reporting accuracy.

Communication Skills: Verbal and written communication skills are necessary to effectively translate technical findings into understandable insights for non-technical stakeholders.

Problem-Solving: The capacity to approach complex data challenges with innovative solutions.

Curiosity and Continuous Learning: A genuine interest in data and its implications for businesses, coupled with continuous learning and staying updated with industry trends and tools.

Professional Certifications

Certification Programs: Certifications can enhance a data analyst's qualifications, especially in specific tools or methodologies. Examples include the Professional Certificate Course In Data Analytics and Tableau Training.

Specialized Training: Online courses and bootcamps offer specialized training in data analytics, machine learning, and specific programming languages or tools.

Practical Experience

Internships: Gaining practical experience through internships in data analysis can provide hands-on experience with real-world data, tools, and projects.

Projects: Working on personal or open-source projects can help develop and showcase your analytical skills and proficiency with data analysis tools.

Industry Knowledge

Domain Expertise: Understanding the specific industry you wish to enter can be a significant advantage. Domain knowledge enables a data analyst to make more informed analyses and recommendations relevant to the business context.

Adaptability and Teamwork

Adaptability: Adapt to new tools, technologies, and methodologies as the field evolves.

Collaboration: Data analysts often work as part of a team, requiring good interpersonal skills and the ability to work effectively with others, including technical and non-technical colleagues.

Types of Data Analysts

The field of data analytics is diverse, with various types of data analysts specializing in different sectors or aspects of data analysis. This specialization allows professionals to focus on specific areas where they can apply their data analyst skills and knowledge most effectively. Below are some common types of data analysts, each with unique roles and areas of expertise:

1. Business Analysts

Focus: Business analysts primarily analyze data for business decisions. They work closely with management and stakeholders to identify business needs, growth opportunities, and improvement areas.

Skills and Tools: They often use tools like SQL, Excel, and business intelligence software like Tableau or Power BI. Understanding business processes and strong communication skills are crucial.

2. Data Analysts in Finance

Focus: These analysts analyze financial data to help organizations make investment decisions, forecast future financial performance, and manage risk.

Skills and Tools: They use statistical software, Excel, and financial modeling tools. Knowledge of financial principles and regulations is important.

3. Marketing Analysts

Focus: Marketing analysts examine data related to market trends, consumer behavior, sales data, and marketing strategies. Their insights help in shaping marketing campaigns, product development, and targeting strategies.

Skills and Tools: They utilize CRM software, Google Analytics, and data visualization tools. Skills in market research and digital marketing analytics are valuable.

4. Operations Analysts

Focus: Operations analysts focus on a company's internal processes. They analyze supply chain management, production processes, and service delivery data to improve efficiency and reduce costs.

Skills and Tools: Proficiency in process mapping tools, SQL, and ERP systems. Understanding the industry’s best practices and operational processes is essential.

5. Data Analysts in Healthcare

Focus: These analysts work with healthcare data, including patient records, treatment outcomes, and healthcare operations. Their analysis supports patient care improvements, operational efficiency, and policy development.

Skills and Tools: Knowledge of healthcare IT systems (like EHRs), statistical software, and data privacy regulations (such as HIPAA in the U.S.) are critical.

6. Web Analysts

Focus: Web analysts scrutinize website analytics to understand user behavior, optimize website performance, and improve user experience.

Skills and Tools: Google Analytics, Adobe Analytics, and knowledge of SEO practices are important. They often work closely with marketing and web development teams.

7. Quantitative Analysts

Focus: Often found in finance, quantitative analysts (quants) use statistical and mathematical models to inform financial and risk management decisions. They're heavily involved in trading strategies and investment modeling.

Skills and Tools: Deep knowledge of mathematics and statistics, programming skills (Python, R, C++), and experience with mathematical modeling tools are required.

8. Social Media Analysts

Focus: These analysts specialize in analyzing social media data to gain insights into consumer sentiment, brand presence, and the effectiveness of social media campaigns.

Skills and Tools: Experience with social media analytics tools (like Hootsuite Insights and Brandwatch), understanding social media platforms, and knowledge of content marketing strategies are crucial.

9. Data Quality Analysts

Focus: Data quality analysts ensure data accuracy, completeness, and reliability within an organization. They identify data quality issues, implement corrections, and develop data governance practices.

Skills and Tools: Proficiency in data profiling and cleansing tools, knowledge of data governance principles, and attention to detail are essential.

10. BI (Business Intelligence) Analysts

Focus: BI analysts use data analytics and visualization tools to develop insights, reports, and dashboards that inform business strategies. They translate complex data findings into actionable business intelligence.

Skills and Tools: Expertise in Power BI, Tableau, and SQL BI tools. Strong business acumen and the ability to communicate complex concepts clearly are important.

Become a Data Science & Business Analytics Professional

28%Annual Job Growth By 2026

11.5 MExpected New Jobs For Data Science By 2026

Data Analyst

Industry-recognized Data Analyst Master’s certificate from Simplilearn

Dedicated live sessions by faculty of industry experts

Post Graduate Program in Data Analytics

Post Graduate Program certificate and Alumni Association membership

Exclusive hackathons and Ask me Anything sessions by IBM

prevNext

Here's what learners are saying regarding our programs:

Gayathri Ramesh

Associate Data Engineer , Publicis Sapient

The course was well structured and curated. The live classes were extremely helpful. They made learning more productive and interactive. The program helped me change my domain from a data analyst to an Associate Data Engineer.

Felix Chong

Project Manage , Codethink

After completing this course, I landed a new job & a salary hike of 30%. I now work with Zuhlke Group as a Project Manager.

prevNext

Not sure what you’re looking for?View all Related Programs

Data Analyst Salary: How Much Does a Data Analyst Make?

Here's an overview of data analyst salaries across different regions, including the United States, India, Europe, and the UK.

United States

Entry-Level: Entry-level data analysts in the United States can earn between $50,000 and $70,000 annually, depending on the location and sector.

Mid-Level: With a few years of experience, salaries typically range from $70,000 to $90,000.

Senior-Level: Senior data analysts or those with specialized skills in high-demand areas can earn between $90,000 and $120,000 or more.

India

Entry-Level: In India, entry-level data analysts may earn between ₹3,00,000 and ₹6,00,000 annually.

Mid-Level: For professionals with a few years of experience, salaries can range from ₹6,00,000 to ₹9,00,000.

Senior-Level: Senior professionals can expect salaries in the range of ₹9,00,000 to ₹20,00,000 or more, especially in high-tech cities and for those with specialized data analyst skills.

Europe

Entry-Level: In Western European countries like Germany or France, entry-level analysts might earn between €30,000 and €50,000. In Eastern European countries, the range might be slightly lower.

Mid-Level: Mid-level salaries can range from €50,000 to €70,000 in Western Europe.

Senior-Level: Senior data analysts can earn €70,000 to €100,000 or more, especially in countries with higher living costs.

United Kingdom (UK)

Entry-Level: Entry-level data analysts in the UK typically earn between £25,000 and £35,000.

Mid-Level: With experience, salaries can range between £35,000 and £50,000.

Senior-Level: Senior-level data analysts can expect to earn between £50,000 and £70,000 or more, depending on their expertise and the industry.

Become a Data Science & Business Analytics Professional

28%Annual Job Growth By 2026

11.5 MExpected New Jobs For Data Science By 2026

Data Analyst

Industry-recognized Data Analyst Master’s certificate from Simplilearn

Dedicated live sessions by faculty of industry experts

Post Graduate Program in Data Analytics

Post Graduate Program certificate and Alumni Association membership

Exclusive hackathons and Ask me Anything sessions by IBM

prevNext

Here's what learners are saying regarding our programs:

Gayathri Ramesh

Associate Data Engineer , Publicis Sapient

The course was well structured and curated. The live classes were extremely helpful. They made learning more productive and interactive. The program helped me change my domain from a data analyst to an Associate Data Engineer.

Felix Chong

Project Manage , Codethink

After completing this course, I landed a new job & a salary hike of 30%. I now work with Zuhlke Group as a Project Manager.

prevNext

Not sure what you’re looking for?View all Related Programs

Top Companies Hiring Data Analysts

The need for data analysts is widespread across multiple sectors. Businesses understand the importance of making decisions based on data and are searching for proficient analysts to unlock the potential of their data resources. Here's a list of top companies known for hiring data analysts, reflecting a diverse range of sectors:

Google: A leader in the tech industry, Google hires data analysts to work on various projects, from improving search algorithms to analyzing user behavior.

Amazon: The e-commerce giant relies on data analysts to optimize its supply chain, enhance customer experience, and drive sales strategies.

Facebook (Meta): With billions of users, Meta hires data analysts to understand social media trends, user engagement, and advertising effectiveness.

Microsoft: Offers roles for data analysts in product development, sales strategies, and customer insights across its various services and platforms.

Apple: Hires data analysts to improve user experiences across its product lines and services, including hardware, software, and media offerings.

JPMorgan Chase & Co.: One of the largest banks in the U.S., it employs data analysts for risk management, customer analytics, and financial forecasting.

Goldman Sachs: Invests in data analytics for market analysis, investment strategies, and risk assessment.

American Express: Utilizes data analysts to enhance customer service, fraud detection, and loyalty programs.

Pfizer: The pharmaceutical giant hires data analysts for drug research, patient data analysis, and market trends.

Johnson & Johnson: Employs analysts to enhance healthcare products, analyze clinical trial data, and optimize supply chains.

Walmart: The world’s largest retailer uses data analytics for inventory management, customer behavior analysis, and sales optimization.

Target: Employs data analysts to enhance shopping experiences, both online and in-store, through personalized marketing and supply chain efficiencies.

McKinsey & Company: A leading consulting firm that hires data analysts to provide insights across various industries, from finance to healthcare.

Deloitte: Offers roles in data analytics to help clients with operational efficiency, market analysis, and strategy development.

Accenture: Provides analytics services, employing data analysts to help clients transform data into actionable insights.

Netflix: Uses data analytics extensively to understand viewer preferences and content performance and to drive production strategies.

The Walt Disney Company: Employs data analysts to enhance guest experiences in parks and resorts and analyze viewer data for its media properties.

IBM: Offers roles for data analysts in areas such as artificial intelligence, cloud services, and business intelligence solutions.

Salesforce: Hires data analysts to improve customer relationship management (CRM) tools and services through data-driven insights.

Uber: Utilizes data analytics for optimizing ride-sharing services, pricing strategies, and market expansion.

Choose the Right Program

Are you eager to embark on a career within the dynamic realm of data analytics? Our courses in Data Analytics are meticulously crafted to fill you with the essential data analyst skills and knowledge vital for thriving in this swiftly expanding field. Guided by our seasoned instructors, you'll engage in hands-on projects, tackle real-life situations, and delve into comprehensive case studies, all aimed at providing you with the practical experience necessary for success. Through our training, you'll master the art of data analysis, develop the ability to craft insightful reports and acquire the competence to make informed, data-driven decisions that contribute to achieving business objectives.

Program Name Data Analyst Post Graduate Program In Data Analytics

Data Analytics Bootcamp Program Available In All Geos All Geos US University Simplilearn Purdue Caltech Course Duration 11 Months 8 Months 6 Months Coding Experience Required No Basic No Skills You Will Learn 10+ skills including Python, MySQL, Tableau, NumPy and more

Data Analytics, Statistical Analysis using Excel, Data Analysis Python and R, and more Data Visualization with Tableau, Linear and Logistic Regression, Data Manipulation and more Additional Benefits Applied Learning via Capstone and 20+ industry-relevant Data Analytics projects Purdue Alumni Association Membership

Free IIMJobs Pro-Membership of 6 months Access to Integrated Practical Labs Caltech CTME Circle Membership Cost $$ $$$$ $$$$ Explore Program Explore Program Explore Program

Get Started to Become a Data Analyst Today!

Having explored the job description, essential skills, and qualifications for a data analyst, you might be curious about how to secure a position in this field. Let's outline the pathway to a career in data analysis. Securing an entry-level position in data analysis can be straightforward if you possess a certification from a highly regarded data analysis program, such as those provided by Simplilearn. Lack of prior experience in data analysis isn't a barrier; with the right training, you can embark on a successful data analyst career. By enrolling in our Data Analyst course, you can begin your journey into data analytics.

FAQs

1. What is the best way to start a career as a data analyst?

The best way to start a career as a data analyst is by gaining a strong foundation in statistics, programming (Python or R), and data visualization tools (e.g., Tableau, Power BI). Acquiring a relevant degree or certification from recognized programs can also be beneficial. Engaging in real-world projects or internships to apply your skills practically will enhance your resume and experience.

2. How important is programming knowledge for a data analyst?

Programming knowledge is crucial for a data analyst as it enables data manipulation, analysis, and the automation of tasks. Proficiency in languages such as Python or R is essential for performing complex data analysis and applying machine learning models to datasets.

3. Can someone without a technical background become a data analyst?

Yes, someone without a technical background can become a data analyst. It requires a commitment to learning key skills such as data analysis techniques, statistical knowledge, and programming languages. Many successful data analysts start from non-technical fields and transition into data roles through dedicated study and practice.

4. How often should I update my skills as a data analyst?

Data analysts should update their skills regularly, ideally several times a year, to keep pace with the rapidly evolving field of data science. Continuous learning through online courses, workshops, and conferences can help analysts stay current with new tools, technologies, and methodologies."
https://news.vanderbilt.edu/2024/04/15/vanderbilt-scientist-collaborates-with-cajal-institute-in-spain-to-train-a-bank-of-ai-models-to-identify-memory-formation-signals-in-the-brain/,Vanderbilt scientist collaborates with Cajal Institute in Spain to train a bank of AI models to identify memory formation signals in the brain,"An international research collaboration between Vanderbilt University and the Madrid-based de la Prida lab in the Cajal Institute led to the development of AI models that detect and analyze hippocampal ripples, which are considered biomarkers of memory.

The research discoveries, outlined in an article Communications Biology, could lead to new opportunities to detect seizures and neural changes in people with Alzheimer’s disease and other neurological disorders.

Kari Hoffman, associate professor of psychology and biomedical engineering at Vanderbilt, and her Ph.D. student Saman Abbaspoor worked on the study with lead authors Adrian Rubio and Andrea Navas Olive from the de la Prida lab. Hoffman is also a faculty affiliate at the Vanderbilt Brain Institute and the Data Science Institute.

As the group’s research outlines, the study of brain oscillations has brought new understanding of brain function. Hippocampal ripples are a type of fast oscillations that underlie the organization of memories. They are affected in such neurological disorders as epilepsy and Alzheimer’s disease, so they are considered an electroencephalographic (EEG) biomarker. However, ripples exhibit various waveform features and properties that can be missed by standard spectral methods.

The researchers set out to gain a better understanding of patterns of brain activity after scientists in the neuroscience community called for the need to better automate, harmonize and improve the detection of ripples across a range of tasks and species. In the study, the authors used recordings obtained in laboratory mice to first train a toolbox of machine learning models.

They then tested the generalizability of the models using data from non-human primates that were collected at Vanderbilt by Abbaspoor and Hoffman as part of the BRAIN Initiative. The researchers found that it is possible to train AI algorithms primarily on rodent data, and still manage highly accurate detection of ripples in primates with little to no additional training, suggesting that the AI models may be successful in humans. The model toolbox emerged as a result of a hackathon, which resulted in a short list for the best detection models. The group identified more than 100 possible models from the different architectures that are now available for application or retraining by other researchers.

“This bank of AI models will provide new applications in the field of neurotechnology and can be useful for detection and analysis of high-frequency oscillations in pathologies such as epilepsy, where they are considered clinical markers,” said Liset de la Prida, research professor at Instituto Cajal, CSIC.

“There is a great interest in taking advantage of AI to enable greater precision in detection of disease states and for oscillotherapeutics,” Hoffman added. “These methods offer the promise to go beyond detecting ‘where’ in the brain but also to detect and ultimately correct the ‘when and how’ of oscillopathies.”

LEARN MORE"
https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632,"Pushing RL Boundaries: Integrating Foundational Models, e.g. LLMs and VLMs, into Reinforcement Learning","Authors: Elahe Aghapour, Salar Rahili

Overview:

With the rise of the transformer architecture and high-throughput compute, training foundational models has turned into a hot topic recently. This has led to promising efforts to either integrate or train foundational models to enhance the capabilities of reinforcement learning (RL) algorithms, signaling an exciting direction for the field. Here, we’re discussing how foundational models can give reinforcement learning a major boost.

Before diving into the latest research on how foundational models can give reinforcement learning a major boost, let’s engage in a brainstorming session. Our goal is to pinpoint areas where pre-trained foundational models, particularly Large Language Models (LLMs) or Vision-Language Models (VLMs), could assist us, or how we might train a foundational model from scratch. A useful approach is to examine each element of the reinforcement learning training loop individually, to identify where there might be room for improvement:

1- Environment: Given that pre-trained foundational models understand the causal relationships between events, they can be utilized to forecast environmental changes resulting from current actions. Although this concept is intriguing, we’re not yet aware of any specific studies that focus on it. There are two primary reasons holding us back from exploring this idea further for now.

While the reinforcement learning training process demands highly accurate predictions for the next step observations, pre-trained LLMs/VLMs haven’t been directly trained on datasets that enable such precise forecasting and thus fall short in this aspect. It’s important to note, as we highlighted in our previous post, that a high-level planner, particularly one used in lifelong learning scenarios, could effectively incorporate a foundational model.

Latency in environment steps is a critical factor that can constrain the RL algorithm, especially when working within a fixed budget for training steps. The presence of a very large model that introduces significant latency can be quite restrictive. Note that while it might be challenging, distillation into a smaller network can be a solution here.

2- State (LLM/VLM Based State Generator): While experts often use the terms observation and state interchangeably, there are distinctions between them. A state is a comprehensive representation of the environment, while an observation may only provide partial information. In the standard RL framework, we don’t often discuss the specific transformations that extract and merge useful features from observations, past actions, and any internal knowledge of the environment to produce “state”, the policy input. Such a transformation could be significantly enhanced by employing LLMs/VLMs, which allow us to infuse the “state” with broader knowledge of the world, physics, and history (refer to Fig. 1, highlighted in pink).

3- Policy (Foundational Policy Model): Integrating foundational models into the policy, the central decision-making component in RL, can be highly beneficial. Although employing such models to generate high-level plans has proven successful, transforming the state into low-level actions has challenges we’ll delve into later. Fortunately, there has been some promising research in this area recently.

4- Reward (LLM/VLM Based Reward Generator): Leveraging foundational models to more accurately assess chosen actions within a trajectory has been a primary focus among researchers. This comes as no surprise, given that rewards have traditionally served as the communication channel between humans and agents, setting goals and guiding the agent towards what is desired.

Pre-trained foundational models come with a deep knowledge of the world, and injecting this kind of understanding into our decision-making processes can make those decisions more in tune with human desires and more likely to succeed. Moreover, using foundational models to evaluate the agent’s actions can quickly trim down the search space and equip the agent with a head start in understanding, as opposed to starting from scratch.

Pre-trained foundational models have been trained on internet-scale data generated mostly by humans, which has enabled them to understand worlds similarly to humans. This makes it possible to use foundational models as cost-effective annotators. They can generate labels or assess trajectories or rollouts on a large scale.

1- Foundational models in reward

It is challenging to use foundational models to generate low level control actions as low level actions are highly dependent on the setting of the agent and are underrepresented in foundational models’ training dataset. Hence, the foundation model application is generally focused on high level plans rather than low level actions. Reward bridges the gap between high-level planner and low level actions where foundation models can be used. Researchers have adopted various methodologies integrating foundation models for reward assignment. However, the core principle revolves around employing a VLM/LLM to effectively track the progress towards a subgoal or task.

1.a Assigning reward values based on similarity

Consider the reward value as a signal that indicates whether the agent’s previous action was beneficial in moving towards the goal. A sensible method involves evaluating how closely the previous action aligns with the current objective. To put this approach into practice, as can be seen in Fig. 2, it’s essential to:

- Generate meaningful embeddings of these actions, which can be done through images, videos, or text descriptions of the most recent observation.

- Generate meaningful representations of the current objective.

- Assess the similarity between these representations.

Let’s explore the specific mechanics behind the leading research in this area.

Dense and well-shaped reward functions enhance the stability and training speed of the RL agent. Intrinsic rewards address this challenge by rewarding the agent for novel states’ exploration. However, in large environments where most of the unseen states are irrelevant to the downstream task, this approach becomes less effective. ELLM uses background knowledge of LLM to shape the exploration. It queries LLM to generate a list of possible goals/subgoals given a list of the agent’s available actions and a text description of the agent current observation, generated by a state captioner. Then, at each time step, the reward is computed by the semantic similarity, cosine similarity, between the LLM generated goal and the description of the agent’s transition.

LiFT has a similar framework but also leverages CLIP4Clip-style VLMs for reward assignment. CLIP4Clip is pre-trained to align videos and corresponding language descriptions through contrastive learning. In LiFT, the agent is rewarded based on the alignment score, cosine similarity, between the task instructions and videos of the agent’s corresponding behavior, both encoded by CLIP4CLIP.

UAFM has a similar framework where the main focus is on robotic manipulation tasks, e.g., stacking a set of objects. For reward assignment, they measure the similarity between the agent state image and the task description, both embedded by CLIP. They finetune CLIP on a small amount of data from the simulated stacking domain to be more aligned in this use case.

1.b Assigning rewards through reasoning on auxiliary tasks:

In scenarios where the foundational model has the proper understanding of the environment, it becomes feasible to directly pass the observations within a trajectory to the model, LLM/VLM. This evaluation can be done either through straightforward QA sessions based on the observations or by verifying the model’s capability in predicting the goal only by looking at the observation trajectory.

Read and Reward integrates the environment’s instruction manual into reward generation through two key components, as can be seen in Fig. 3:

QA extraction module: it creates a summary of game objectives and features. This LLM-based module, RoBERTa-large, takes in the game manual and a question, and extracts the corresponding answer from the text. Questions are focused on the game objective, and agent-object interaction, identified by their significance using TF-IDF. For each critical object, a question as: “What happens when the player hits a <object>?” is added to the question set. A summary is then formed with the concatenation of all non-empty question-answer pairs.

Reasoning module: During gameplay, a rule-based algorithm detects “hit” events. Following each “hit” event, the LLM based reasoning module is queried with the summary of the environment and a question: “Should you hit a <object of interaction> if you want to win?” where the possible answer is limited to {yes, no}. A “yes” response adds a positive reward, while “no” leads to a negative reward.

EAGER introduces a unique method for creating intrinsic rewards through a specially designed auxiliary task. This approach presents a novel concept where the auxiliary task involves predicting the goal based on the current observation. If the model predicts accurately, this indicates a strong alignment with the intended goal, and thus, a larger intrinsic reward is given based on the prediction confidence level. To achieve this goal, To accomplish this, two modules are employed:

Question Generation (QG): This component works by masking all nouns and adjectives in the detailed objective provided by the user.

Question Answering (QA): This is a model trained in a supervised manner, which takes the observation, question masks, and actions, and predicts the masked tokens.

(P.S. Although this work does not utilize a foundational model, we’ve included it here due to its intriguing approach, which can be easily adapted to any pre-trained LLM)

1.c Generating reward function code

Up to this point, we’ve discussed generating reward values directly for the reinforcement learning algorithms. However, running a large model at every step of the RL loop can significantly slow down the speed of both training and inference. To bypass this bottleneck, one strategy involves utilizing our foundational model to generate the code for the reward function. This allows for the direct generation of reward values at each step, streamlining the process.

For the code generation schema to work effectively, two key components are required:

1- A code generator, LLM, which receives a detailed prompt containing all the necessary information to craft the code.

2- A refinement process that evaluates and enhances the code in collaboration with the code generator.

Let’s look at the key contributions for generating reward code:

R2R2S generates reward function code through two main components:

LLM based motion descriptor: This module uses a pre-defined template to describe robot movements, and leverages Large Language Models (LLMs) to understand the motion. The Motion Descriptor fills in the template, replacing placeholders e.g. “Destination Point Coordinate” with specific details, to describe the desired robot motion within a pre-defined template.

LLM based reward coder: this component generates the reward function by processing a prompt containing: a motion description, a list of functions with their description that LLM can use to generate the reward function code, an example code of how the response should look like, and constraints and rules the reward function must follow.

Text2Reward develops a method to generate dense reward functions as an executable code within iterative refinement. Given the subgoal of the task, it has two key components:

LLM-based reward coder: generates reward function code. Its prompt consists of: an abstract of observation and available actions, a compact pythonic style environment to represent the configuration of the objects, robot, and callable functions; a background knowledge for reward function design (e.g. “reward function for task X typically includes a term for the distance between object x and y”), and a few-shot examples. They assume access to a pool of instruction, and reward function pairs that top k similar instructions are retrieved as few-shot examples.

LLM-Based Refinement: once the reward code is generated, the code is executed to identify the syntax errors and runtime errors. These feedbacks are integrated into subsequent prompts to generate more refined reward functions. Additionally, human feedback is requested based on a task execution video by the current policy.

Auto MC-Reward has a similar algorithm to Text2Reward, to generate the reward function code, see Fig. 4. The main difference is in the refinement stage where it has two modules, both LLMs:

LLM-Based Reward Critic: It evaluates the code and provides feedback on whether the code is self-consistent and free of syntax and semantic errors.

LLM-Based Trajectory Analyser: It reviews the historical information of the interaction between the trained agent and the environment and uses it to guide the modifications of the reward function.

EUREKA generates reward code without the need for task-specific prompting, predefined reward templates, or predefined few-shot examples. To achieve this goal, it has two stages:

LLM-based code generation: The raw environment code, the task, generic reward design and formatting tips are fed to the LLM as context and LLM returns the executable reward code with a list of its components.

Evolutionary search and refinement: At each iteration, EUREKA queries the LLM to generate several i.i.d reward functions. Training an agent with executable reward functions provides feedback on how well the agent is performing. For a detailed and focused analysis of the rewards, the feedback also includes scalar values for each component of the reward function. The LLM takes top-performing reward code along with this detailed feedback to mutate the reward code in-context. In each subsequent iteration, the LLM uses the top reward code as a reference to generate K more i.i.d reward codes. This iterative optimization continues until a specified number of iterations has been reached.

Within these two steps, EUREKA is able to generate reward functions that outperform expert human-engineered rewards without any task specific templates.

1.d. Train a reward model based on preferences (RLAIF)

An alternative method is to use a foundational model to generate data for training a reward function model. The significant successes of Reinforcement Learning with Human Feedback (RLHF) have recently drawn increased attention towards employing trained reward functions on a larger scale. The heart of such algorithms is the use of a preference dataset to train a reward model which can subsequently be integrated into reinforcement learning algorithms. Given the high cost associated with generating preference data (e.g., action A is preferable to action B) through human feedback, there’s growing interest in constructing this dataset by obtaining feedback from an AI agent, i.e. VLM/LLM. Training a reward function, using AI-generated data and integrating it within a reinforcement learning algorithm, is known as Reinforcement Learning with AI Feedback (RLAIF).

MOTIF requires access to a passive dataset of observations with sufficient coverage. Initially, LLM is queried with a summary of desired behaviors within the environment and a text description of two randomly sampled observations. It then generates the preference, selecting between 1, 2, or 0 (indicating no preference), as seen in Fig. 5. This process constructs a dataset of preferences between observation pairs. Subsequently, this dataset is used to train a reward model employing preference based RL techniques.

2- Foundation models as Policy

Achieving the capability to train a foundational policy that not only excels in tasks previously encountered but also possesses the ability to reason about and adapt to new tasks using past learning, is an ambition within the RL community. Such a policy would ideally generalize from past experiences to tackle novel situations and, through environmental feedback, achieve goals previously unseen with human-like adaptability.

However, several challenges stand in the way of training such agents. Among these challenges are:

The necessity of managing a very large model, which introduces significant latency into the decision-making process for low-level control actions.

The requirement to collect a vast amount of interaction data across a wide array of tasks to enable effective learning.

Additionally, the process of training a very large network from scratch using RL introduces extra complexities. This is because backpropagation efficiency inherently is weaker in RL compared to supervised training methods .

Up to now, it’s mostly been teams with substantial resources and top-notch setups who’ve really pushed the envelope in this domain.

AdA paved the way for training an RL foundation model within the X.Land 2.0 3D environment. This model achieves human time-scale adaptation on held-out test tasks without any further training. The model’s success is founded on three ingredients:

The core of the AdA’s learning mechanism is a Transformer-XL architecture from 23 to 265 million parameters, employed alongside the Muesli RL algorithm. Transformer-XL takes in a trajectory of observations, actions, and rewards from time t to T and outputs a sequence of hidden states for each time step. The hidden state is utilized to predict reward, value, and action distribution π. The combination of both long-term and short-term memory is critical for fast adaptation. Long-term memory is achieved through slow gradient updates, whereas short-term memory can be captured within the context length of the transformer. This unique combination allows the model to preserve knowledge across multiple task attempts by retaining memory across trials, even though the environment resets between trials.

The model benefits from meta-RL training across 1⁰⁴⁰ different partially observable Markov decision processes (POMDPs) tasks. Since transformers are meta-learners, no additional meta step is required.

Given the size and diversity of the task pool, many tasks will either be too easy or too hard to generate a good training signal. To tackle this, they used an automated curriculum to prioritize tasks that are within its capability frontier.

RT-2 introduces a method to co-finetune a VLM on both robotic trajectory data and vision-language tasks, resulting in a policy model called RT-2. To enable vision-language models to generate low-level actions, actions are discretized into 256 bins and represented as language tokens.

By representing actions as language tokens, RT-2 can directly utilize pre-existing VLM architectures without requiring substantial modifications. Hence, VLM input includes robot camera image and textual task description formatted similarly to Vision Question Answering tasks and the output is a series of language tokens that represent the robot’s low-level actions; see Fig. 6.

They noticed that co-finetuning on both types of data with the original web data leads to more generalizable policies. The co-finetuning process equips RT-2 with the ability to understand and execute commands that were not explicitly present in its training data, showcasing remarkable adaptability. This approach enabled them to leverage internet-scale pretraining of VLM to generalize to novel tasks through semantic reasoning.

3- Foundation Models as State Representation

In RL, a policy’s understanding of the environment at any given moment comes from its “state” which is essentially how it perceives its surroundings. Looking at the RL block diagram, a reasonable module to inject world knowledge into is the state. If we can enrich observations with general knowledge useful for completing tasks, the policy can pick up new tasks much faster compared to RL agents that begin learning from scratch.

PR2L introduces a novel approach to inject the background knowledge of VLMs from internet scale data into RL.PR2L employs generative VLMs which generate language in response to an image and a text input. As VLMs are proficient in understanding and responding to visual and textual inputs, they can provide a rich source of semantic features from observations to be linked to actions.

PR2L, queries a VLM with a task-relevant prompt for each visual observation received by the agent, and receives both the generated textual response and the model’s intermediate representations. They discard the text and use some or all of the models intermediate representation generated for both the visual and text input and the VLM’s generated textual response as “promptable representations”. Due to the variable size of these representations, PR2L incorporates an encoder-decoder Transformer layer to embed all the information embedded in promptable representations into a fixed size embedding. This embedding, combined with any available non-visual observation data, is then provided to the policy network, representing the state of the agent. This innovative integration allows the RL agent to leverage the rich semantic understanding and background knowledge of VLMs, facilitating more rapid and informed learning of tasks.

Also Read Our Previous Post: Towards AGI: LLMs and Foundational Models’ Roles in the Lifelong Learning Revolution

References:"
https://towardsdatascience.com/live-graphs-with-ffmpeg-to-enhance-your-data-storytelling-61cc12529382?source=rss----7f60cf5620c9---4,Live Graphs with FFmpeg to Enhance your Data Storytelling,"Animate your Data Visualizations with this FFmpeg Tutorial

Fouad Faraj

·

Follow

Published in

Towards Data Science

·

4 min read

·

2 days ago

--

Introduction

There are tons of open source data visualization libraries available for creating graphs but most have limited functionality for creating dynamic moving plots. The most common approach is to generate various images and…"
https://www.marktechpost.com/2024/04/14/top-data-analytics-books-to-read-in-2024/,Top Data Analytics Books to Read in 2024,"In today’s data-driven world, data analytics plays a key role in helping organizations make better decisions, identify opportunities, and mitigate risks. Data analytics enables businesses to gain insights into customer preferences and market dynamics, enhancing overall performance. As such, the demand for competent analysts has increased significantly over the past few years. This article lists the top data analytics books one should read in 2024 to augment one’s skills and stay ahead in this rapidly evolving field.

Python for Data Analysis

“Python for Data Analysis” is a comprehensive guide to manipulating, processing, and cleaning datasets in Python. It covers the tools to load, clean, transform, merge, and reshape data, focusing on libraries like Pandas and Numpy. The book also teaches how to solve real-world problems with detailed examples.

Fundamentals of Data Analytics

This book is a guide to the data analytics process, providing a five-step framework to help readers start the journey of analyzing data. The book covers the data mining and machine learning principles and provides strategies to build a problem-solving mindset.

Data Analytics for Absolute Beginners

This book is aimed at beginners and provides an introduction to data, data visualization, business intelligence, and statistics. The book consists of numerous practical and visual examples, along with coding exercises in Python. It also covers some of the machine learning concepts like regression, classification, and clustering.

Everything Data Analytics

“Everything Data Analytics” is a beginner’s guide to data literacy that helps understand the process of turning data into insights. The book covers the process of data collection, management, and storage, along with the essential machine-learning algorithms necessary for analysis, like regression, classification, and clustering.

SQL for Data Analysis

“SQL for Data Analysis” covers improving one’s SQL skills and making the most of SQL as part of their workflow. The book provides some advanced techniques for transforming data into insights, covering topics like joins, window functions, subqueries, and regular expressions.

Advancing into Analytics

This is a practical guide for Excel users to help them gain an understanding of analytics and the data stack. The author covers the key statistical concepts with spreadsheets and helps Excel users transition to performing exploratory data analysis and hypothesis testing using Python and R.

Modern Data Analytics in Excel

This book covers the features of modern Excel and the powerful tools for analytics. The author teaches how to leverage tools like Power Query and Power Pivot to build repeatable data-cleaning processes and create relational data models and analysis measures. The book also covers using AI and Python for more advanced Excel reporting.

Data Visualization with Excel Dashboards and Reports

This book teaches how to analyze large amounts of data in Excel and report them in a meaningful way. It also teaches the fundamentals of data visualization and covers how to automate redundant reporting and analyses.

Data Analysis for Business, Economics, and Policy

This book is a practical guide to using tools to carry out data analysis to support better decision-making in business, economics, and policy. The book covers topics like data wrangling, regression analysis, and causal analysis, along with numerous case studies with real-world data.

Storytelling with Data

“Storytelling with Data” is a data visualization guide for business professionals. The book teaches how to convert the data into a high-impact visual story to resonate the message with the audience.

Fundamentals of Data Visualization

This book provides a guide to making informative and compelling figures that help convey a compelling story. The book also provides extensive examples of good and bad figures.

Data Visualization: A Practical Introduction

This book covers how to create compelling visualizations using R programming language, more specifically using the ggplot2 library. It covers topics like plotting continuous and categorical variables, grouping, summarizing, and transforming data for plotting, creating maps, and refining plots to make them more understandable.

Naked Statistics

“Naked Statistics” is a beginner-friendly book focusing on the underlying intuition driving statistical analysis. The book covers topics like inference, correlation, and regression analysis in a witty and funny manner, which simplifies the learning process.

The Art of Statistics

“The Art of Statistics” is a practical guide to using data and mathematics to understand real-world problems better. The book covers how to clarify questions and assumptions and interpret the results.

Essential Math for Data Science

This book teaches the mathematics essential for excelling in data science, machine learning, and statistics. It covers topics like calculus, probability, linear algebra, and statistics, as well as their applications in algorithms like linear regression and neural networks.

Practical Statistics for Data Scientists

This book covers how to apply statistical methods to data science using programming languages like Python and R. It emphasizes the importance of exploratory data analysis and also covers the underlying statistical concepts behind supervised and unsupervised machine learning algorithms.

Business unIntelligence

This book talks about the ever-changing and complex business intelligence landscape in today’s world. It covers numerous new models that businesses can leverage to design support systems for future successful organizations.

Data Science for Business

This book covers how organizations can leverage data science to gain a competitive advantage. It talks about general concepts that are useful in extracting knowledge from data. The book also provides various real-world examples to explain different concepts.

The Model Thinker

This book guides how to organize, apply, and understand the data that is being analyzed to become a true data ninja. The book covers mathematical, statistical, and computational models such as linear regression and random walks and provides a toolkit for its readers to make them leverage data to their advantage.

Becoming a Data Head

“Becoming a Data Head” teaches how to think, speak, and understand data science and statistics. It also covers the recent trends in machine learning, text analytics, and artificial intelligence.

We make a small profit from purchases made via referral/affiliate links attached to each book mentioned in the above list."
https://www.simplilearn.com/power-bi-interview-questions-and-answers-article,60 Power BI Interview Questions and Expert Answers for 2024,"Back in 2011, the rise of Business Intelligence tools posed a challenge to Microsoft to build its own business intelligence tool. Microsoft introduced the Power BI to deliver compelling analytical capabilities to existing Microsoft Excel and upgrade it to be intelligent enough to generate interactive reports.

According to Gartner's Magic Quadrant, Microsoft Power BI is one of today’s top business intelligence tools, chiefly because most IT firms rely on Power BI for their business analytics. As a result, the current IT industry finds a massive demand for Power BI Experts.

This tutorial is solely dedicated to helping aspiring Power BI professionals grasp the essential fundamentals of Power BI and crack the interviews in real-time. The tutorial is organized based on three categories, outlined below.

Power BI Interview Questions - Beginner Level

Power BI Interview Questions - Intermediate Level

Power BI Interview Questions - Advanced Level

Most Asked Power BI Interview Questions

What is Power BI?

Difference between Power BI and Tableau

Difference between Power Query and Power Pivot

What is Power BI Desktop

What is Power Pivot?

What is Power Query?

What is DAX?

What are Filters in Power BI?

What are Custom Visuals in Power BI?

What is GetData in Power BI?

We have five dozen questions for you, so let’s begin by going through some refresher-level or frequently asked beginner-level Power BI interview questions.

Become a Business and Leadership Professional

Top 10 skills in demandBusiness Analysis As A Skill In 2020

14% Growth in JobsOf Business Analysis Profile By 2028

Business Analyst

Industry-recognized certifications from IBM and Simplilearn

Masterclasses from IBM experts

Post Graduate Program in Business Analysis

Certificate from Simplilearn in collaboration with Purdue University

Become eligible to be part of the Purdue University Alumni Association

prevNext

Here's what learners are saying regarding our programs:

Sauvik Pal

Assistant Consultant at Tata Consultancy Services , Tata Consultancy Services

My experience with Simplilearn has been great till now. They have good materials to start with, and a wide range of courses. I have signed up for two courses with Simplilearn over the past 6 months, Data Scientist and Agile and Scrum. My experience with both is good. One unique feature I liked about Simplilearn is that they give pre-requisites that you should complete, before a live class, so that you go there fully prepared. Secondly, there support staff is superb. I believe there are two teams, to cater to the Indian and US time zones. Simplilearn gives you the most methodical and easy way to up-skill yourself. Also, when you compare the data analytics courses across the market that offer web-based tutorials, Simplilearn, scores over the rest in my opinion. Great job, Simplilearn!

Vy Tran

I was keenly looking for a change in my domain from business consultancy to IT(Business Analytics). This Post Graduate Program in Business Analysis course helped me achieve the same. I am proficient in business analysis now and am looking for job profiles that suit my skill set.

prevNext

Not sure what you’re looking for?View all Related Programs

Power BI Interview Questions For Freshers

1. What is Power BI?

Power BI is a business analytics tool developed by Microsoft that helps you turn multiple unrelated data sources into valuable and interactive insights. These data may be in the form of an Excel spreadsheet or cloud-based/on-premises hybrid data warehouses. You can easily connect to all your data sources and share the insights with anyone.

2. Why should we use Power BI?

Because Power BI provides an easy way for anyone, including non-technical people, to connect, change, and visualize their raw business data from many different sources and turn it into valuable data that makes it easy to make smart business decisions.

3. Difference between Power BI and Tableau

Both Tableau and Power BI are the current IT industry's data analytics and visualization giants. Yet, there are a few significant differences between them. You will now explore the important differences between Tableau and Power BI.

4. Difference between Power Query and Power Pivot

The differences between Power Query and Power Pivot are explained as follows:

Become a Business and Leadership Professional

Top 10 skills in demandBusiness Analysis As A Skill In 2020

14% Growth in JobsOf Business Analysis Profile By 2028

Business Analyst

Industry-recognized certifications from IBM and Simplilearn

Masterclasses from IBM experts

Post Graduate Program in Business Analysis

Certificate from Simplilearn in collaboration with Purdue University

Become eligible to be part of the Purdue University Alumni Association

prevNext

Here's what learners are saying regarding our programs:

Sauvik Pal

Assistant Consultant at Tata Consultancy Services , Tata Consultancy Services

My experience with Simplilearn has been great till now. They have good materials to start with, and a wide range of courses. I have signed up for two courses with Simplilearn over the past 6 months, Data Scientist and Agile and Scrum. My experience with both is good. One unique feature I liked about Simplilearn is that they give pre-requisites that you should complete, before a live class, so that you go there fully prepared. Secondly, there support staff is superb. I believe there are two teams, to cater to the Indian and US time zones. Simplilearn gives you the most methodical and easy way to up-skill yourself. Also, when you compare the data analytics courses across the market that offer web-based tutorials, Simplilearn, scores over the rest in my opinion. Great job, Simplilearn!

Vy Tran

I was keenly looking for a change in my domain from business consultancy to IT(Business Analytics). This Post Graduate Program in Business Analysis course helped me achieve the same. I am proficient in business analysis now and am looking for job profiles that suit my skill set.

prevNext

Not sure what you’re looking for?View all Related Programs

5. What is Power BI Desktop

Power BI Desktop is an open-source application designed and developed by Microsoft. Power BI Desktop will allow users to connect to, transform, and visualize your data with ease. Power BI Desktop lets users build visuals and collections of visuals that can be shared as reports with your colleagues or your clients in your organization.

6. What is Power Pivot?

Power Pivot is an add-on provided by Microsoft for Excel since 2010. Power Pivot was designed to extend the analytical capabilities and services of Microsoft Excel.

7. What is Power Query?

Power Query is a business intelligence tool designed by Microsoft for Excel. Power Query allows you to import data from various data sources and will enable you to clean, transform and reshape your data as per the requirements. Power Query allows you to write your query once and then run it with a simple refresh.

8. Describe the components of Microsoft’s self-service BI solution.

Self-service business intelligence (SSBI) is divided into the Excel BI Toolkit and Power BI.

9. What is self-service BI, anyway?

SSBI is an abbreviation for Self-Service Business Intelligence and is a breakthrough in business intelligence. SSBI has enabled many business professionals with no technical or coding background to use Power BI and generate reports and draw predictions successfully. Even non-technical users can create these dashboards to help their business make more informed decisions.

10. What is DAX?

DAX stands for Data Analysis Expressions. It's a collection of functions, operators, and constants used in formulas to calculate and return values. In other words, it helps you create new info from data you already have.

11. What are Filters in Power BI?

The term ""Filter"" is self-explanatory. Filters are mathematical and logical conditions applied to data to filter out essential information in rows and columns. The following are the variety of filters available in Power BI:

Manual filters

Auto filters

Include/Exclude filters

Drill-down filters

Cross Drill filters

Drillthrough filters

Drillthrough filters

URL filters–transient

Pass-Through filters

12. What are Custom Visuals in Power BI?

Custom Visuals are like any other visualizations, generated using Power BI. The only difference is that it developes the custom visuals using a custom SDK. The languages like JQuery and JavaScript are used to create custom visuals in Power BI.

13. What is GetData in Power BI?

Get Data is a simple icon on Power BI used to import data from the source.

14. Mention some advantages of Power BI.

Some of the advantages of using Power BI:

It helps build an interactable data visualization in data centers

It allows users to transform data into visuals and share them with anyone

It establishes a connection for Excel queries and dashboards for fast analysis

It provides quick and accurate solutions

It enables users to perform queries on reports using simple English words

Become a Business and Leadership Professional

Top 10 skills in demandBusiness Analysis As A Skill In 2020

14% Growth in JobsOf Business Analysis Profile By 2028

Business Analyst

Industry-recognized certifications from IBM and Simplilearn

Masterclasses from IBM experts

Post Graduate Program in Business Analysis

Certificate from Simplilearn in collaboration with Purdue University

Become eligible to be part of the Purdue University Alumni Association

prevNext

Here's what learners are saying regarding our programs:

Sauvik Pal

Assistant Consultant at Tata Consultancy Services , Tata Consultancy Services

My experience with Simplilearn has been great till now. They have good materials to start with, and a wide range of courses. I have signed up for two courses with Simplilearn over the past 6 months, Data Scientist and Agile and Scrum. My experience with both is good. One unique feature I liked about Simplilearn is that they give pre-requisites that you should complete, before a live class, so that you go there fully prepared. Secondly, there support staff is superb. I believe there are two teams, to cater to the Indian and US time zones. Simplilearn gives you the most methodical and easy way to up-skill yourself. Also, when you compare the data analytics courses across the market that offer web-based tutorials, Simplilearn, scores over the rest in my opinion. Great job, Simplilearn!

Vy Tran

I was keenly looking for a change in my domain from business consultancy to IT(Business Analytics). This Post Graduate Program in Business Analysis course helped me achieve the same. I am proficient in business analysis now and am looking for job profiles that suit my skill set.

prevNext

Not sure what you’re looking for?View all Related Programs

15. List out some drawbacks/limitations of using Power BI.

Here are some limitations to using Power BI:

Power BI does not accept file sizes larger than 1 GB and doesn't mix imported data accessed from real-time connections.

There are very few data sources that allow real-time connections to Power BI reports and dashboards.

It only shares dashboards and reports with users logged in with the same email address.

Dashboard doesn't accept or pass user, account, or other entity parameters.

16. What are some differences in data modeling between Power BI Desktop and Power Pivot for Excel?

Power Pivot for Excel supports only single directional relationships (one to many), calculated columns, and one import mode. Power BI Desktop supports bi-directional cross-filtering connections, security, calculated tables, and multiple import options.

17. Name the different connectivity modes available in Power BI?

There are three main connectivity modes used in Power BI.

SQL Server Import

An SQL Server Import is the default and most common connectivity type used in Power BI. It allows you to use the full capabilities of the Power BI Desktop.

Direct Query

The Direct Query connection type is only available when you connect to specific data sources. In this connectivity type, Power BI will only store the metadata of the underlying data and not the actual data.

Live Connection

With this connectivity type, it does not store data in the Power BI model. All interaction with a report using a Live Connection will directly query the existing Analysis Services model. There are only 3 data sources that support the live connection method - SQL Server Analysis Services (Tabular models and Multidimensional Cubes), Azure Analysis Services (Tabular Models), and Power BI Datasets hosted in the Power BI Service.

18. What are the various types of refresh options provided in Power BI?

Four important types of refresh options provided in Microsoft Power BI are as follows:

Package refresh - This synchronizes your Power BI Desktop or Excel file between the Power BI service and OneDrive, or SharePoint Online.

Model or data refresh - This refreshes the dataset within the Power BI service with data from the original data source.

Tile refresh - This updates the cache for tile visuals every 15 minutes on the dashboard once data changes.

Visual container refresh - This refreshes the visible container and updates the cached report visuals within a report once the data changes.

19. Name the data sources can Power BI can connect to?

Several data sources can be connected to Power BI, which is grouped into three main types:

Files

It can import data from Excel (.xlsx, .xlxm), Power BI Desktop files (.pbix) and Comma-Separated Values (.csv).

Content Packs

These are a collection of related documents or files stored as a group. There are two types of content packs in Power BI:

Content packs from services providers like Google Analytics, Marketo, or Salesforce and Content packs are created and shared by other users in your organization.

Connectors

Connectors help you connect your databases and datasets with apps, services, and data in the cloud.

20. What is a dashboard in Power BI?

A dashboard is a single-layer presentation sheet of multiple visualizations reports. The main features of the Power BI dashboard are:

It allows you to drill through the page, bookmarks, and selection pane and also lets you create various tiles and integrate URLs

A dashboard can also help you set report layout to mobile view.

21. Explain how relationships are defined in Power BI Desktop?

Relationships between tables are defined in two ways:

Manually - Relationships between tables are manually defined using primary and foreign keys.

Automatic - When enabled, this automated feature of Power BI detects relationships between tables and creates them automatically.

22. Can you have more than one functional relationship between two tables in a Power Pivot data model?

No. There can be multiple inactive relationships, but only one active relationship between two tables in a Power Pivot data model. Dotted lines represent inactive relationships, and continuous lines represent active relationships.

23. Can you have a table in the model which does not have any relationship with other tables?

Yes. There are two main reasons why you can have disconnected tables:

The table is used to present the user with parameter values to be exposed and selected in slicers

It uses the table as a placeholder for metrics in the user interface

24. What is the CALCULATE function in DAX?

The CALCULATE function evaluates the sum of the Sales table Sales Amount column in a modified filter context. It is also the only function that allows users to modify the filter context of measures or tables.

Moving ahead, you will step up to the following Power BI Interview Questions from the Intermediate Level.

Power BI Interview Questions For Intermediate Level

25. Where is data stored in Power BI?

Most of the time, power BI gets assisted by the cloud to store the data. Power BI can use a desktop service. Microsoft Azure is used as the primary cloud service to store the data.

Azure SQL Database

Azure Blob Storage

26. What is row-level security?

Row-level security limits the data a user can view and has access to, and it relies on filters. Users can define the rules and roles in Power BI Desktop and also publish them to Power BI Service to configure row-level security.

27. Why should you apply general formatting to Power BI data?

Users can use general formatting to make it easier for Power BI to categorize and identify data, making it considerably easier to work with.

28. What are the different views available in Power BI Desktop?

There are three different views in Power BI, each of which serves another purpose:

Report View - In this view, users can add visualizations and additional report pages and publish the same on the portal.

Data View - In this view, data shaping can be performed using Query Editor tools.

Model View - In this view, users can manage relationships between complex datasets.

Become a Business and Leadership Professional

Top 10 skills in demandBusiness Analysis As A Skill In 2020

14% Growth in JobsOf Business Analysis Profile By 2028

Business Analyst

Industry-recognized certifications from IBM and Simplilearn

Masterclasses from IBM experts

Post Graduate Program in Business Analysis

Certificate from Simplilearn in collaboration with Purdue University

Become eligible to be part of the Purdue University Alumni Association

prevNext

Here's what learners are saying regarding our programs:

Sauvik Pal

Assistant Consultant at Tata Consultancy Services , Tata Consultancy Services

My experience with Simplilearn has been great till now. They have good materials to start with, and a wide range of courses. I have signed up for two courses with Simplilearn over the past 6 months, Data Scientist and Agile and Scrum. My experience with both is good. One unique feature I liked about Simplilearn is that they give pre-requisites that you should complete, before a live class, so that you go there fully prepared. Secondly, there support staff is superb. I believe there are two teams, to cater to the Indian and US time zones. Simplilearn gives you the most methodical and easy way to up-skill yourself. Also, when you compare the data analytics courses across the market that offer web-based tutorials, Simplilearn, scores over the rest in my opinion. Great job, Simplilearn!

Vy Tran

I was keenly looking for a change in my domain from business consultancy to IT(Business Analytics). This Post Graduate Program in Business Analysis course helped me achieve the same. I am proficient in business analysis now and am looking for job profiles that suit my skill set.

prevNext

Not sure what you’re looking for?View all Related Programs

29. What are the various versions of Power BI?

Power BI Desktop

Power BI service

Mobile Power BI apps for iOS and Android devices

30. Explain the building blocks of Microsoft Power BI.

The important building blocks of Power BI are as follows:

Visualizations

Visualization is the process of generating charts and graphs for the representation of insights on business data.

Datasets

A dataset is the collection of data used to create a visualization, such as a column of sales figures. Dataset can get combined and filtered from a variety of sources via built-in data plugins.

Reports

The final stage is the report stage. Here, there is a group of visualizations on one or more pages. For example, charts and maps are combined to make a final report.

Dashboards

A Power BI dashboard helps you to share a single visualization with colleagues and clients to view your final dashboard.

Tiles

A tile is an individual visualization on your final dashboard or one of your charts in your final report.

31. What are the critical components of the Power BI toolkit?

The critical components of Power BI are mentioned below.

Power Query

Power Pivot

Power View

Power Map

Power Q&A

32. What do you mean by the content pack?

A content pack is defined as a ready-made collection of visualizations and Power BI reports using your chosen service. You'd use a content pack when you want to get up and running quickly instead of creating a report from scratch.

33. Define bi-directional cross filtering.

Bidirectional cross-filtering lets data modelers to decide how they want their Power BI Desktop filters to flow for data, using the relationships between tables. The filter context is transmitted to a second related table that exists on the other side of any given table relationship. This procedure helps data modelers solve the many-to-many issue without having to complicated DAX formulas. So, to sum it up, bidirectional cross-filtering makes the job for data modelers easier.

34. What are the three fundamental concepts of DAX?

Syntax

This is how the formula is written—that is, the elements that comprise it. The Syntax includes functions such as SUM (used when you want to add figures). If the Syntax isn't correct, you'll get an error message.

Functions

These are formulas that use specific values (also known as arguments) in a particular order to perform a calculation, similar to the functions in Excel. The categories of functions are date/time, time intelligence, information, logical, mathematical, statistical, text, parent/child, and others.

Context

There are two types: row context and filter context. Row context comes into play whenever a formula has a function that applies filters to identify a single row in a table. When one or more filters are applied in a calculation that determines a result or value, the filter context comes into play.

35. Why and how would you use a custom visual file?

You will use a custom visual file if the prepackaged files don't fit the needs of your business. Developers create custom visual files, and you can import them and use them in the same way as you would the prepackaged files.

36. What are some familiar sources for data in the Get Data menu in Power BI?

A few familiar data sources are Excel, Power BI datasets, web, text, SQL server, and analysis services.

37. What are the categories of data types?

All

File

Database

Power BI

Azure

Online Services

Other

38. Name some commonly used tasks in the Query Editor.

Connect to data

Shape and combine data

Group rows

Pivot columns

Create custom columns

Query formulas

39. What do you mean by grouping?

Power BI Desktop helps you to group the data in your visuals into chunks. You can, however, define your groups and bins. For grouping, use Ctrl + click to select multiple elements in the visual. Right-click one of those elements and, from the menu that appears, choose Group. In the Groups window, you can create new groups or modify existing ones.

40. Explain responsive slicers in Power BI.

On a Power BI final report page, a developer can resize a responsive slicer to various sizes and shapes, and the data collected in the container will be rearranged to find a match. If a visual report becomes too small to be useful, an icon representing the visual takes its place, saving space on the report page.

41. What is query folding in Power BI?

Query folding is used when steps defined in the Query Editor are translated into SQL and executed by the source database instead of your device. It helps with scalability and efficient processing.

42. What is ""M language.""

M is a programming language used in Power Query as a functional, case-sensitive language similar to other programming languages and easy to use.

Power BI Interview Questions For Experienced

43. What are the major differences between visual-level, page-level, and report-level filters in Power BI?

Visual-level filters are used to filter data within a single visualization. Page-level filters are used to work on an entire page in a report, and different pages can have various filters.

Report-level filters are used to filter all the visualizations and pages in the report.

44. List the most common techniques for data shaping.

Adding indexes

Applying a sort order

Removing columns and rows

45. How is the Schedule Refresh feature designed to work?

Users can set up for an automatic refresh over data based on daily or weekly requirements. Users can schedule only one refresh maximum daily unless they have Power BI Pro. The Schedule Refresh section uses the pull-down menu choices to select a frequency, time zone, and time of day.

46. What information is needed to create a map in Power Map?

Power Map can display geographical visualizations. Therefore, some location data is needed—for example, city, state, country, or latitude and longitude.

47. Which in-memory analytics engine does Power Pivot use?

Power Pivot uses the xVelocity engine. xVelocity can handle huge amounts of data, storing data in columnar databases. All data gets loaded into RAM memory when you use in-memory analytics, which boosts the processing speed.

48. Mention important components of SSAS

Following are some of the important Components of SSAS:

OLP Engine

An OLAP Engine is used to extensively run the ADHOC queries at a faster pace by the end-users

Data Drilling

It describes data Drilling in SSAS as the process of exploring details of the data with multiple levels of granularity.

Slicers

The data Slicing process in SSAS is defined as the process of storing the data in rows and columns.

Pivot Tables

Pivot Tables helps in switching between the different categories of data stored between rows and columns

49. What are the three fundamental concepts of DAX?

Syntax: This is how the formula is written—the elements that comprise it. The syntax includes functions such as SUM (used when you want to add figures). If the syntax isn't correct, you'll get an error message.

Functions: These are formulas that use specific values (also known as arguments) in a certain order to perform a calculation, similar to the functions in Excel. The categories of functions are date/time, time intelligence, information, logical, mathematical, statistical, text, parent/child, and others.

Context: There are two types: row context and filter context. Row context comes into play whenever a formula has a function that applies filters to identify a single row in a table. When one or more filters are applied in a calculation that determines a result or value, the filter context comes into play.

50. Name the variety of Power BI Formats.

Power BI is available mainly in three formats, as mentioned below.

Power BI Desktop: Open-Source version for Desktop users

Power BI Services: For Online Services

Power BI Mobile Application: Compatible with mobile devices

51. What are the different stages in the working of Power BI?

There are three different stages in working on Power BI, as explained below.

Data Integration

Data Processing

Data Presentation

Data Integration

The primary step in any business intelligence is to establish a successful connection with the data source and integrate it to extract data for processing.

Data Processing

The next step in business intelligence is data processing. Most of the time, the raw data also includes unexpected erroneous data, or sometimes a few data cells might be empty. The BI tool needs to interpret the missing values and inaccurate data for processing in the data processing stage.

Data Presentation

The final stage in business intelligence is analyzing the data got from the source and presenting the insights using visually appealing graphs and interactive dashboards.

52. Which professionals use Power BI the most?

Beginners and experts prefer Power BI in business intelligence. Power BI is used mainly by the following professionals.

Business Analysts

Business Owners

Business Developers

Business Analysts

A business analyst is a professional who analyses the business data and represents the insights found using visually appealing graphs and dashboards

Business Owners

Business owners, decision-makers, or organizations use Power BI to view the insights and understand the prediction to make a business decision.

Business Developers

Business Developers are just software developers who get hired for business purposes to develop custom applications and dashboards to help the business process be smooth.

53. What is the advanced editor?

Advanced editor is used to view queries that Power BI is running against the data sources importing data. The query is rendered in M-code. Users wanting to view the query code select “Edit Queries” from the Home tab, then click on “Advanced Editor” to perform work on the query. Any changes get saved to Applied Steps in the Query Settings.

Become a Business and Leadership Professional

Top 10 skills in demandBusiness Analysis As A Skill In 2020

14% Growth in JobsOf Business Analysis Profile By 2028

Business Analyst

Industry-recognized certifications from IBM and Simplilearn

Masterclasses from IBM experts

Post Graduate Program in Business Analysis

Certificate from Simplilearn in collaboration with Purdue University

Become eligible to be part of the Purdue University Alumni Association

prevNext

Here's what learners are saying regarding our programs:

Sauvik Pal

Assistant Consultant at Tata Consultancy Services , Tata Consultancy Services

My experience with Simplilearn has been great till now. They have good materials to start with, and a wide range of courses. I have signed up for two courses with Simplilearn over the past 6 months, Data Scientist and Agile and Scrum. My experience with both is good. One unique feature I liked about Simplilearn is that they give pre-requisites that you should complete, before a live class, so that you go there fully prepared. Secondly, there support staff is superb. I believe there are two teams, to cater to the Indian and US time zones. Simplilearn gives you the most methodical and easy way to up-skill yourself. Also, when you compare the data analytics courses across the market that offer web-based tutorials, Simplilearn, scores over the rest in my opinion. Great job, Simplilearn!

Vy Tran

I was keenly looking for a change in my domain from business consultancy to IT(Business Analytics). This Post Graduate Program in Business Analysis course helped me achieve the same. I am proficient in business analysis now and am looking for job profiles that suit my skill set.

prevNext

Not sure what you’re looking for?View all Related Programs

54. What gateways does Power BI have and why should you use them?

Gateways function as bridges between the in-house data sources and Azure Cloud Services.

Personal Gateway: Used only by one person, data can be imported, and is only valid on Power BI Service.

On-Premises Gateway: This is an advanced form of the Personal Gateway, supporting Direct Query and usable by multiple users to refresh data.

55. Mention some applications of Power BI

There are multiple applications of Power BI; some of them are as follows:

Business Analysis

Data Analysis

Database Administration

IT Professional

Data Science

56. How can you depict a story in Power BI?

Every individual chart or visualization report generated is collected and represented on a single screen. Such an approach is called a Power BI Dashboard. A Dashboard in Power BI is used to depict a story.

57. What are KPIs in Power BI?

KPI is abbreviated as Key Performance Indicator. Any professional organization has teams and employees follow the KPI protocols. The organizations set up KPIs for all the employees. These KPIs act as their targets. These KPIs are compared to previous performance and analyze the progress.

58. What is a Slicer?

Slicers are an integral part of a business report generated using Power BI. The functionality of a slicer can be considered similar to that of a filter, but, unlike a filter, a Slicer can display a visual representation of all values and users will be provided with the option to select from the available values in the slicer’s drop-down menu.

59. Explain Power BI Designer.

It is a combined solution offered to upload the reports and dashboards to the PowerBI.com website for reference. It consists of Power Pivot, Power Query, and Power Table.

60. How do you reshape data in Power BI?

Power BI offers a wide variety of data source connectivity options. Data Editor is one of the tools used to manipulate rows and columns of data and helps you reshape it according to the requirements.

With that, you have come to an end to this tutorial on “Top 60 Power BI Interview Questions and Answers for 2024”.

Wrapping Up

Power BI Architecture and Features can be your next vital step in learning data analytics.

Are you interested in learning more about Business Analytics? Or do you wish to get trained and certified to become a successful Business Analyst? Then feel free to explore the Business Analytics certification course from Simplilearn. Ranked among the five topmost business analytics courses, this Simplilearn program is offered in partnership with Purdue University. This course is an outcome-driven training and certification program that helps you master the fundamental concepts of statistics and data analytics.

Do you have any questions for us on this tutorial on ""Top 60 Power BI Interview Questions and Answers for 2024”? If you do, or you have questions about our certification courses, do reach out to us by sharing them as comments below. Our team of experts will address them and will be happy to answer them at the earliest."
https://www.simplilearn.com/power-bi-alternatives-article,Top Power BI Alternatives for Dynamic Data Visualization,"Power BI Alternatives 2024

The top power BI alternatives that can be helpful in the year 2024 are:

Looker

Looker is a sophisticated data visualization and business intelligence tool that offers a wide range of analytical skills, embedded analytics, data exploration, and predictive models. It offers a user-friendly interface and robust functionalities and is preferred by businesses seeking data-based insights.

Tableau

Tableau is the most popular data visualization tool, and its user-friendly interface and impressive analytics are among its most recognizable features. It allows users to create different reports, such as interactive dashboards, visualizations, and reports, with data from multiple sources. Therefore, it is convenient for analyzing and exploring different data sets.

Qlik Sense

Qlik Sense is a modern data analytics platform capable of solving problems related to self-service visualization, exploration, and data discovery. It provides its users with ways of creating interactive dashboards and reporting on dashboards.

Sisense

Sisense is a full-featured platform for business intelligence analysis that allows users to easily prepare, analyze, and visualize both complex and multi-dimensional datasets. Its robust features, like AI-powered analytics and embedded analytics, make the tool an appropriate solution for large, medium, and even small businesses.

Zoho Corporation

Zoho Corporation's cloud-based business intelligence and analytics solutions include Zoho Analytics, which makes it easy to produce insightful reports, KPIs, and dashboards from a variety of heterogeneous data sources using a DIY approach. Its interface is very user-friendly, so it’s the most loved tool for start-ups and SMEs in particular.

Domo

Domo is cloud-based software that integrates data from several different sources and provides powerful visualizations and analytics features to the business. Its intuitive and user-friendly interface is designed to help users share insights and work towards achieving their business goals.

Google Data Studio

The Google Data Studio, which is available for free, allows users to design interactive reports and dashboards. The data for these reports is acquired from various sources like Google Sheets, Google Analytics, and BigQuery. The platform is compatible with the rest of Google's products, which provides convenience for businesses that also use Google's other products.

SAP Analytics Cloud

SAP Analytics Cloud is an Analysis system built with modern data visualization capabilities, predictive analytics, and planning features. Its integration with SAP's enterprise solutions makes it a preferred choice for large organizations looking for end-to-end analytics solutions.

Holistics

Holistics is a business intelligence and data analytics tool with many useful visualization functionalities, ranging from data reporting to dashboards. It allows users to easily connect to multiple data sources, create interactive dashboards, and share insights with stakeholders. Holistics provides powerful data visualization, and its user-friendly interface and flexible customization capabilities make it a preferred platform for businesses aiming to enhance the efficiency of their data analytics processes.

Metabase

Metabase, an open-source analytical tool, provides a straightforward, GUI-friendly data visualization interface. It enables the creation of interactive dashboards, runs ad hoc queries, and collaborates with team members. Economic feasibility and simplicity of use are the key factors that put Metabase ahead of its competitors in the small and medium business market.

Dundas Data Visualization

Dundas Data Visualization is the best tool because it offers comprehensive data visualization and allows for creating interactive dashboards and reporting.

It enables users to integrate data from multiple sources, perform advanced analytics, and share insights with stakeholders. As a result of its comprehensive features and various customizations, it becomes the first choice of organizations that need to cater to the needs of a complex data environment.

GoodData

GoodData is a cloud-based business intelligence and analytics platform that offers data visualization and powerful data insights capabilities for the company. It provides a platform for analysts to build interactive dashboards and present reports to investors. AI-based analytics and scalable infrastructure in GoodData make it ideal for decision-making for businesses looking to use data-driven insights to drive growth and innovation.

Excel4apps

Excel4apps is a built-in data visualization and reporting tool that seamlessly integrates with Microsoft Excel so users can quickly extract data from business applications like Oracle and SAP systems. It gives users various Excel-like features, templates, and reports like those specifically designed for particular business functions. Excel4apps' integration with Excel and business systems is highly commendable, which makes it the best tool for any company that aims to use its existing Excel expertise while analyzing data or reporting.

Cognos

Cognos is a business intelligence and performance management software package by IBM. It has a full set of highlighting reporting, analytics, dashboards, and scorecard features integrated into it. With its robust capabilities, Cognos enables organizations to gain insights from their data, make informed decisions, and drive business performance.

MicroStrategy

MicroStrategy is a widely used dynamic management software with sophisticated visualization and reporting capabilities. It is a single framework that can handle multiple functions, like building dashboards for real-time analytics. MicroStrategy as a platform makes it fit for organizations of all sizes and levels of industry.

Apache Superset

Apache Superset is an open-source data exploration and visualization platform that supplies a variety of visualization options and provides interaction-based features. One can design dashboards, dive deep into data, and perform customized analysis. Apache Superset is a desirable choice for organizations that need a customized and flexible data visualization solution.

Geckoboard

Geckoboard is a real-time dashboard software that helps businesses monitor and track different metrics and KPIs in one place. Among its features is the ability to add customizable widgets and integrate other data sources, allowing users to create and communicate innovative dashboards quickly. Geckoboard's simple real-time data analysis makes it an ideal solution for organizations searching for what just works.

Klipfolio Inc.

Klipfolio is a Cloud-based business performance management system that enables users to build custom dashboards and reports for different data types from multiple sources. It provides a variety of built-in visualizations and data connectors for the development of dashboards that are free from any need to code.

Klipfolio benefits from its user-friendly interface and integrations, making the tool a preferred choice for businesses to track multiple performance metrics simultaneously.

Mode

Mode is a cloud-based analytic integrated platform that allows SQL, Python, and R to be used together. It allows users to search for data, make an infographic, and distribute information among the team members. Mode's emphasis on collaboration and its integration with popular programming languages make it a preferred choice for data-driven teams looking to streamline their analytics workflows.

Qualtrics

Qualtrics is a leading experience management platform that offers powerful survey tools, data analytics, and insights capabilities. These options allow businesses to gather customer, worker, and partner feedback, examine the data instantaneously, and immediately transform things into better experiences.

Its user-friendly interface offers a range of analytics capabilities and the power to understand customer sentiment, employee engagement, and market trends to guide business decisions and increase the business presence in the market.

Reportei

Reportei is a reporting and analytics platform that combines the processes of creating and sharing professional-looking reports, therefore, easing the task. It offers customizable templates, drag-and-drop functionality, and integrations with popular data sources, making it easy for users to visualize data and communicate insights effectively.

Reportei's user-friendly interface and automation functionally make Reportei a useful tool for marketing agencies, businesses, and freelancers looking to minimize the problem of reporting processes.

Spotfire

Spotfire is an analytical software that offers advanced visualization capabilities, predictive analytics, and dashboard features. It empowers users to discover and communicate with stakeholders. The intuitive interface, advanced analytics features, and the ability to integrate with statistical and machine learning tools make Spotfire the preferred tool by data scientists, analysts, and business users who want to derive important insights for their businesses.

SAS Business Intelligence

SAS Business Intelligence is a valuable combination of analytics and business intelligence tools that enable organizations to understand and interpret data, produce reports, and ease decision-making processes based on valuable data. It possesses a variety of data visualization features, predictive analysis features, and data mining capabilities. SAS Business Intelligence is a trusted solution for organizations irrespective of their industry, be it finance, healthcare, or retail and manufacturing.

Infor Birst

Infor Birst is a cloud-based business intelligence and analytics program that offers advanced visualization, reporting, and dashboard creation. It enables users to link to several data sources, gather data, and customize dashboards and reports. With its scalable architecture and self-service capabilities, Infor Birst is preferred for businesses looking to leverage data analytics to drive growth and innovation."
https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9,Quantifying the Complexity and Learnability of Strategic Classification Problems,"How generalizing the notion of VC dimension to a strategic setting can help us understand whether or not a problem is learnable

Jonathan Yahav

·

Follow

Published in

Towards Data Science

·

8 min read

·

5 days ago

--

In the first article in this series, we formally defined the strategic classification problem, denoted Sᴛʀᴀᴄ⟨H, R, c⟩, as a generalization of canonical binary classification. We did so based on the paper PAC-Learning for Strategic Classification (Sundaram, Vullikanti, Xu, & Yao, 2021). Along the way, we explored why we should care about considering the various preferences of rational agents during classification and how we can do so (subject to certain assumptions). We will rely heavily on the concepts introduced in the previous article, so I encourage you to read it if you haven’t already.

Extending PAC Learning to a Strategic Classification Setting

A case study of the meeting point between game theory and fundamental concepts in machine learning

towardsdatascience.com

We’ll pick up where we left off, using the definition of Sᴛʀᴀᴄ⟨H, R, c⟩ as a jumping-off point for the useful concept of strategic VC dimension (SVC). Once we make sense of SVC, what I call the Fundamental Theorem of Strategic Learning will follow rather naturally.

While helpful, prior familiarity with shattering coefficients, the canonical VC dimension, and the Fundamental Theorem of Statistical Learning will not be necessary for you to follow along. However, there’s far more depth to each of them than I could ever hope to cover as part of this series, let alone in a single article. The curious reader is referred to Andrew Rothman’s wonderful and very thorough articles on the (canonical) shattering coefficient and VC dimension.

Statistical Learning Theory Part 5: Shattering Coefficient

Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over Infinite Function Classes leveraging the…

anr248.medium.com

Statistical Learning Theory Part 6: Vapnik–Chervonenkis (VC) Dimension

Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over Infinite Function Classes leveraging the…

anr248.medium.com

As we’ll see, strategic shattering coefficients and SVC are fairly natural generalizations of their canonical (i.e., non-strategic) counterparts. We will therefore begin with a brief rundown of each of those counterparts before explaining how they can be modified to work in a strategic setting.

Counting Achievable Labelings: Canonical Shattering Coefficients

Verbally defining shattering coefficients seems straightforward at first glance:

Given a hypothesis class H, its nᵗʰ shattering coefficient, denoted Sₙ(H), represents the largest number of labelings achievable by classifiers in H on a sample of n feature vectors.

But what is a “labeling”? And what makes it “achievable”? Answering those questions will help us lay some groundwork in pursuit of a more formal definition.

In the context of binary classification, a labeling of a sample of feature vectors is simply any one of the ways we can assign values from the set { -1, 1 } to those vectors. As a very simple example, consider two one-dimensional feature vectors (i.e., points on a number line), x₁ = 1 and x₂ = 2.

The possible labelings are any combination of the classification values we can assign the individual feature vectors independent of one another. We can represent each labeling as a vector, where the first and second coordinate represent the values assigned to x₁ and x₂, respectively. The set of possible labelings is thus { (-1, -1), (-1, 1), (1, -1), (1, 1) }. Note that a sample of size 2 yields 2² = 4 possible labelings — we’ll see how this generalizes to arbitrarily-sized samples soon.

We say that a labeling is achievable by a hypothesis class H if there exists a classifier h ∈ H from which that labeling can result. Continuing with our simple example, suppose we are limited to classifiers of the form x ≥ k, k ∈ ℝ, that is, one-dimensional thresholds such that anything to the right of the threshold is positively classified. The labeling (1, -1) is not achievable by this hypothesis class. x₂ being greater than x₁ implies that any threshold that classifies x₁ positively must do the same for x₂. The set of achievable labelings is therefore { (-1, -1), (-1, 1), (1, 1) }.

Having understood the basic terminology, we can start to develop some notation to formally express elements of the verbal definition with which we started.

We stick to representing labelings as vectors as we did in our simple example, with each coordinate representing the classification value assigned to the corresponding feature vector. There are 2ⁿ possible labelings in total: there are two possible choices for each feature vector, and we can think of a labeling as a collection of n such choices, each made independently of the rest. If a hypothesis class H can achieve all possible labelings of a sample 𝒞ₙ, i.e., if the number of achievable labelings of 𝒞ₙ equals 2ⁿ, we say that H shatters 𝒞ₙ.

Finally, using the notation from above, we converge on a more rigorous definition of Sₙ(H):

In keeping with our explanation of shattering, Sₙ(H) equalling 2ⁿ implies that there exists a sample of size n that is shattered by H.

Estimating Hypothesis Class Expressiveness: Canonical VC Dimension

The Vapnik–Chervonenkis (VC) dimension is a way to gauge the expressive power of a hypothesis class. It’s based on the idea of shattering we just defined, and it plays an important role in helping us determine which hypothesis classes are PAC learnable and which aren’t.

Let’s begin by attempting to intuitively define the canonical VC dimension:

Given a hypothesis class H, its VC dimension, denoted VCdim(H), is defined to be the greatest natural number n for which there exists a sample of size n that is shattered by H.

Using Sₙ(H) enables us to express this much more cleanly and succinctly:

VCdim(H) = max{ n ∈ ℕ : Sₙ(H) = 2ⁿ }

However, this definition isn’t precise. Note that the set of numbers for which the shattering coefficient equals 2ⁿ may be infinite. (Consequently, it is possible that VCdim(H) = ∞.) If that’s the case, the set has no well-defined maximum. We address this by taking the supremum instead:

VCdim(H) = sup{ n ∈ ℕ : Sₙ(H) = 2ⁿ }

This rigorous and concise definition is the one we’ll use moving forward.

Adding Preferences to the Mix: Strategic Shattering Coefficients

Generalizing the canonical notions we just went over so that they work in a strategic setup is fairly straightforward. Redefining shattering coefficients in terms of the data point best response we defined in the previous article is practically all we’ll have to do.

Given a hypothesis class H, a preference set R, and a cost function c, the nᵗʰ shattering coefficient of Sᴛʀᴀᴄ⟨H, R, c⟩, denoted σₙ(H, R, c), represents the largest number of labelings achievable by classifiers in H on a set of n potentially-manipulated feature vectors, i.e., n data point best responses.

As a reminder, this is how we defined the data point best response:

We can tweak the notation we used in our discussion of canonical shattering coefficients to further formalize this:

The main difference is that each x in the sample has to have a corresponding r. Other than that, putting the data point best response where we had x in the canonical case works smoothly.

As a quick sanity check, let’s consider what happens if R = { 0 }. The realized reward term 𝕀(h(z) = 1) ⋅ r will be 0 across all the data points. Maximizing utility thus becomes synonymous with minimizing cost. The best way to minimize the cost incurred by a data point is trivial: never manipulating its feature vector.

Δ(x, r; h) ends up always just being x, placing us firmly within the territory of canonical classification. It follows that σₙ(H, { 0 }, c) = Sₙ(H) for all H, c. This is consistent with our observation that the impartial preference class represented by R = { 0 } is equivalent to canonical binary classification.

Expressiveness with Preferences: Strategic VC Dimension (SVC)

Having defined the nᵗʰ strategic shattering coefficient, we can simply swap out the Sₙ(H) in the canonical definition of the VC dimension for σₙ(H, R, c).

SVC(H, R, c) = sup{ n ∈ ℕ : σₙ(H, R, c) = 2ⁿ }

Based on the example we considered above, we find that SVC(H, { 0 }, c) = VCdim(H) for any H, c. Indeed, SVC is to VCdim as the strategic shattering coefficient is to its canonical equivalent: both are elegant generalizations of non-strategic concepts.

From SVC to Strategic PAC Learnability: The Fundamental Theorem of Strategic Learning

We can now use SVC to state the Fundamental Theorem of Strategic Learning, which relates the complexity of a strategic classification problem to its (agnostic) PAC learnability.

A strategic classification instance Sᴛʀᴀᴄ⟨H, R, c⟩ is agnostic PAC learnable if and only if SVC(H, R, c) is finite. The sample complexity for strategic agnostic PAC learning is m(δ, ε) ≤ Cε ⁻² ⋅ (SVC(H, R, c) + log⁡(1/δ)), with C being a constant.

We won’t elaborate too much on how this can be proven. Suffice it to say that it boils down to a clever reduction to the (well-documented) Fundamental Theorem of Statistical Learning, which is essentially the non-strategic version of the theorem. If you’re mathematically inclined and interested in the nuts and bolts of the proof, you can find it in Appendix B of the paper.

This theorem essentially completes our generalization of classic PAC learning to a strategic classification setting. It shows that the way we defined SVC actually doesn’t just make sense in our heads; it actually works as a generalization of VCdim where it matters most. Armed with the Fundamental Theorem, we are well-equipped to analyze strategic classification problems much as we would any old binary classification problem. In my opinion, having the ability to determine whether a strategic problem is theoretically learnable or not is pretty incredible!"
https://i2db.wustl.edu/calendar_event/2024-annual-i2db-symposium/,2024 I2DB Symposium: The Power of AI in Medicine,"Joshua Denzer, PhD

BI Solution Developer

Business Intelligence & Data Solutions

BJC Healthcare

Levi Kaster

Bioinformatics Research Assistant

Institute for Informatics, Data Science, and Biostatistics

School of Medicine

Washington University in St. Louis

Seunghwan (Nigel) Kim

PhD Candidate, Biomedical Informatics and Data Science

Division of Biology and Biomedical Sciences

School of Medicine

Washington University in St. Louis

Sunny Lou, MD, PhD

Instructor in Anesthesiology

Division of Adult Cardiothoracic Anesthesiology

Division of Clinical and Translational Research

School of Medicine

Washington University in St. Louis

Matthew Schuelke

Medical Informaticist III

Institute for Informatics Center for Applied Clinical Informatics

School of Medicine

Washington University in St. Louis

Mei Wang

PhD Student

Division of Biology & Biomedical Sciences

Division of Public Health Sciences

Department of Surgery

School of Medicine

Washington University in St. Louis

Ben Warner

PhD Student

McKelvey School of Engineering

AI for Health Institute

Washington University in St. Louis

The inaugural I2DB Datathon is a unique opportunity for teams to engage in a spirited competition, leveraging synthetic medical data, to build accurate risk prediction models. Participants receive a comprehensive training dataset and the chance to present their work to experts in the field at the annual I2DB Symposium. Winners in first, second, and third place are duly rewarded with cash prizes. Join us in this first-ever competition to showcase your talent and contribute to the advancement of medical data analysis.

Participants include (team name followed by team captain and fellow team members):

BDM_Team2: Xuping Luo, Ruby Gao, Zhen Luo, and Purva Patel.

Dan DI2: Dan Maranan.

The AI prediction pawtners: Mei Wang, Yi-Hsuan Shih, Yuan-Hung Kuan, Su-Hsin Chang, and Jr-Shin Li.

BDM_T4: Jingyi Zhang, Wenjing Lin, Zirui Chen, and Trevor Zimmerman.

LifeSavers Analytics: Gauri Hemant Darekar, Yian Liu, Xuanwei Li, and Qingqing Li.

The Predictablizers: Min Zhao, Hao Fan, and Weiwei Ma.

Sandhya: Sandhya Tripathi and Joshin Kumar.

DataGeeks: Zheng Xu, Zining Yang, Yueshui Lyu, and Preeti Sharma.

Dude, Where’s My Model?: Jason Dude, Keith Lohse, Tanner Reece.

Python Pandemic Predictors: Ashish Vaidyanathan, Jeevan Sivamohan, Ben Dizdar, Zachary Bertino, and Jacob Haralson.

Student: Matthew Schuelke.

Team 156: Jeff Zhang.

Charles (Chuck) Goss, PhD

Associate Director, Center for Biostatistics and Data Science

Assistant Professor of Biostatistics

Center for Biostatistics and Data Science

School of Medicine

Washington University in St. Louis

Mackenzie Hofford, MD

Associate Chief Research Information Officer

Assistant Professor of Medicine Division of General Medicine

Center for Applied Clinical Informatics

School of Medicine

Washington University in St. Louis

Jinghua Ou, PhD

Program Officer, Methodological Research

Patient-Centered Outcomes Research Institute (PCORI)

Linying Zhang, PhD

Assistant Professor of Biostatistics

Center for Biostatistics and Data Science

School of Medicine

Washington University in St. Louis

Phillip V Asaro, MD

Director, Emergency Medicine Informatics, Applications & Analytics

Assistant Professor, Emergency Medicine

School of Medicine

Washington University in St. Louis

Graham Bachman

Bioinformaticist

McDonnell Genome Institute

School of Medicine

Washington University in St. Louis

Laura R. Baratta

Medical Scientist Training Program Student

Division of Biology and Biomedical Sciences

School of Medicine

Washington University in St. Louis

Patrick Donohue

MD/MS Biomedical Informatics Student

School of Medicine

Washington University in St. Louis

Joshua Denzer, PhD

BI Solution Developer

Business Intelligence & Data Solutions

BJC Healthcare

Sung Min Ha

PhD Candidate, Imaging Science

Washington University in St. Louis

School of Medicine

Washington University in St. Louis

Ethan Hillis

Medical Informaticist II

Washington University in St. Louis

School of Medicine

Washington University in St. Louis

Matthew Schuelke

Medical Informaticist III

Institute for Informatics Center for Applied Clinical Informatics

School of Medicine

Washington University in St. Louis

Levi Kaster

Bioinformatics Research Assistant

Institute for Informatics, Data Science, and Biostatistics

School of Medicine

Washington University in St. Louis

Seunghwan (Nigel) Kim

PhD Candidate, Biomedical Informatics and Data Science

Division of Biology and Biomedical Sciences

School of Medicine

Washington University in St. Louis

Kendall Kiser, MD MS

Resident Physician

Department of Radiation Oncology

School of Medicine

Washington University in St. Louis

Yuan-Hung, Kuan

PhD Candidate, Electrical and Systems Engineering

McKelvey School of Engineering

Washington University in St. Louis

Sunny Lou, MD, PhD

Instructor in Anesthesiology

Division of Adult Cardiothoracic Anesthesiology

Division of Clinical and Translational Research

School of Medicine

Washington University in St. Louis

Brianna Munnich

Research Technician

Department of Pathology and Immunology

School of Medicine

Washington University in St. Louis

Inez Oh

Senior Scientist

Institute for Informatics, Data Science, and Biostatistics

School of Medicine

Washington University in St. Louis

James R. Rudloff, MD

Clinical Fellow, Pediatric Emergency Medicine

School of Medicine

Washington University in St. Louis

Yi-Hsuan Shih

PhD Candidate, Electrical and Systems Engineering

McKelvey School of Engineering

Washington University in St. Louis

Sandhya Tripathi, PhD

Postdoctoral Research Associate

Department of Anesthesiology

School of Medicine

Washington University in St. Louis

Ashish Vaidyanathan

Undergraduate Research Assistant, Biomedical Engineering

McKelvey School of Engineering

Washington University in St. Louis

Mei Wang

PhD Student

Division of Biology & Biomedical Sciences

Division of Public Health Sciences

Department of Surgery

School of Medicine

Washington University in St. Louis

Ben Warner

PhD Student

McKelvey School of Engineering

AI for Health Institute

Washington University in St. Louis

Pan Xiao

PhD Student, Imaging Science

Mallinckrodt Institute of Radiology

School of Medicine

Washington University in St. Louis"
https://www.prnewswire.com/news-releases/google-cloud-announces-generative-ai-cybersecurity-and-data-analytics-trainings-with-new-skills-based-interview-accelerator-for-hiring-partners-302116098.html,"Google Cloud Announces Generative AI, Cybersecurity, and Data Analytics Trainings with New Skills-Based Interview Accelerator for Hiring Partners","New trainings launched in collaboration with employer and education partners, including the U.S. Department of the Treasury, Rackspace, Purdue Global, and Jack Henry

Hiring initiative with AI-powered training pathways unlocks job opportunities and degree-earning credits

SUNNYVALE, Calif., April 15, 2024 /PRNewswire/ -- Today, at the ASU + GSV Summit, Google Cloud announced new certificates and courses in generative AI, cybersecurity, and data analytics to help connect talent to in-demand tech jobs across industries and sectors. Launch employer partners include the U.S. Department of the Treasury, Rackspace, Jack Henry, and Purdue Global, all of whom are helping learners advance their careers through the new trainings.

As the demand for AI and cybersecurity skills increases, U.S. employers posted nearly 200,000 jobs for AI skills and job roles in the past 12 months, according to Network World. In addition, the World Economic Forum reports that demand for AI and machine learning specialists is expected to grow 40%, adding about one million jobs to the global economy over the next five years. Collaboration between industry, government, education, and employers can help prepare to keep up with this demand.

The new Google Cloud Data Analytics and Cybersecurity Certificates, and courses in generative AI provide dynamic, hands-on learning experiences that validate skills with industry-recognized credentials. The learning journey is powered by generative AI tools, like Interview Warmup, to prepare learners for in-demand cloud roles in data analytics, cybersecurity, software development, systems operations, and more—particularly as jobs start to require advanced AI knowledge.

In addition to the digital trainings that are available on YouTube, certificate completers will also have the chance to take custom, hands-on labs that represent on-the-job scenarios. The labs are specific to each employer partner and will be considered the first stage in their hiring process.

""Google Cloud is committed to driving innovation in the public and private sectors, and that includes working with future technologists and the current workforce to prepare for careers in high-growth areas like AI, cybersecurity, and data analytics,"" said Lee Moore, Vice President, Google Cloud Consulting. ""While these investments enable individuals to pursue exciting new jobs, they also ultimately help organizations achieve their own digital transformation goals.""

The U.S. Department of the Treasury will use the certificates for cyber talent identification as part of the execution of recommendations in President Biden's Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence.

""In an age of rapid innovation and adoption of new technology offering the promise of improved productivity, it is imperative that we equip every worker with accessible training and development opportunities to understand and apply this new technology,"" said Todd Conklin, Chief AI Officer and Deputy Assistant Secretary for Cybersecurity and Critical Infrastructure Protection, U.S. Department of the Treasury. ""We are excited to partner with Google to provide the new cloud certificates training for our current and future employees to accelerate their careers in cybersecurity and data analytics.""

To expand access to these programs, the new trainings are being offered to educational institutions, government, and nonprofit workforce development programs across the globe at no cost. The Google Cloud Certificates also offer credit-bearing pathways to academic institutions, like Purdue Global.

""Purdue Global's students have benefited greatly from the strong working relationship between Purdue Global and Google,"" said Frank Dooley, Chancellor of Purdue Global. ""Encouraging our students to build industry-recognized, stackable credentials will make them more competitive candidates for in-demand tech careers. We believe these new Google Cloud Cybersecurity and Data Analytics Certificates will equip our working adult learners with the essential skills to move forward and succeed in today's cloud-driven market.""

Google Cloud provides a large and continuously updated portfolio of learning resources to help individuals, the public sector, and organizations equip the current and future workforce with technology skills. This work builds on the success of the Google Career Certificates program, which offers training for in-demand tech fields with no prior experience or degree required. To learn more about the new Google Cloud Certificates and training courses, visit cloud.google.com/blog.

About Google Cloud

Google Cloud is the new way to the cloud, providing AI, infrastructure, developer, data, security, and collaboration tools built for today and tomorrow. Google Cloud offers a powerful, fully integrated and optimized AI stack with its own planet-scale infrastructure, custom-built chips, generative AI models and development platform, as well as AI-powered applications, to help organizations transform. Customers in more than 200 countries and territories turn to Google Cloud as their trusted technology partner.

SOURCE Google Cloud"
https://www.globenewswire.com/news-release/2024/04/11/2861607/0/en/Standout-Data-Technology-Innovators-Honored-in-5th-Annual-Data-Breakthrough-Awards-Program.html,Standout Data Technology Innovators Honored in 5th Annual,"LOS ANGELES, April 11, 2024 (GLOBE NEWSWIRE) -- Data Breakthrough, an independent market intelligence organization that recognizes the top companies, technologies and solutions in the global data technology market, today announced the winners of the organization’s fifth annual awards program, showcasing technologies and companies that drive innovation and exemplify the best in data technology solutions across the globe.

The annual Data Breakthrough Awards is the premier awards program founded to recognize the data technology innovators, leaders and visionaries from around the world in a range of categories, including DataOps, Data Analytics, Data Management, Data Storage and many more. The 5th annual Data Breakthrough Award program attracted thousands of nominations from across the globe.

“As we complete ½ a decade of evaluating the world’s most innovative data technologies, it’s evident that digital innovation and most notably, artificial intelligence (AI), have done more than just grow – they have revolutionized the way we approach technology and business,” said Steve Johansson, Managing Director, Data Breakthrough. “In the last decade, global data volumes have climbed by over 1200%, soaring from 9 to 120 zettabytes. Our annual program aims to recognize the global innovators that are unlocking the true power of this data, converting data into strategic assets, and we’re thrilled to recognize our 2024 Data Breakthrough Award winners that are pushing the boundaries of what’s possible in data technology.”

The Data Breakthrough Awards program evaluates global nominations from companies representing a comprehensive set of data technology specialties within the larger fields of data science, analytics, AI and more with the winning products and companies selected based on a variety of criteria, including most innovative and technologically advanced solutions and services.

The 2024 Data Breakthrough Award winners include:

Data Management

Data Management Solution of the Year: WalkMe

Data Catalog Solution of the Year: ZE PowerGroup, ZEMA™

Database Modeling Solution of the Year: SqlDBM

Data Virtualization Solution of the Year: CData

Data Processing Solution of the Year: Expanso

Data Monitoring Solution of the Year: Saturam, Qualdo™

Data Management Innovation of the Year: WEKA

Data Analytics

Cross Infrastructure Analytics Solution of the Year: BMC Software, BMC Helix AIOps

Data Analytics Solution of the Year: Dremio

Overall Data Analytics Platform of the Year: Alteryx

Overall Data Analytics Company of the Year: Treasure Data

Compute & Infrastructure

In-Memory Solution of the Year: Aerospike

Graph DBS Solution of the Year: TigerGraph

Cloud EDW Solution of the Year: Hydrolix

Data Transformation Solution of the Year: Liqid

Data Integration and Warehousing

Data Integration Solution of the Year: Actian, Actian Data Platform

Data Warehouse Solution of the Year: Indegene

Data Warehouse Solution Provider of the Year: BridgeFT

Data Storage

Flash Storage Solution of the Year: Pliops, Pliops Extreme Data Processor (XDP)

Object Storage Solution of the Year: MinIO

Data Backup Solution of the Year: ExaGrid, ExaGrid Tiered Backup Storage

Data Storage Innovation of the Year: Hammerspace, Hyperscale NAS

Overall Data Storage Solution of the Year: Lightbits Labs

Industry Applications

Data Solution of the Year – Marketing: Digital Element

Data Solution of the Year – Sales: collective[i]

Data Solution of the Year – Retail: Birdzi

Data Solution of the Year – Healthcare: Code Focus

Data Solution of the Year – Finance: Provenir

Data Solution of the Year – Transportation: Bosch

Data Solution of the Year – Human Resources: Aisera

Industry Leadership

Customer Data Platform of the Year: Infocepts

Best Use of Artificial Intelligence: Red Hat, Red Hat OpenShift AI

Data-as-a-Service Company of the Year: Bright Data

Data Technology Startup of the Year: illumex

Data Technology CEO of the Year: Sam Gutmann, Own Company

Data Intelligence Platform of the Year: Azira

Overall Data Science Solution of the Year: iCIMS, iCIMS Insights

Overall Data Tech Innovation of the Year: Parsec Automation Corp, TrakSYS

Overall Data Tech Company of the Year: VAST Data

About Data Breakthrough

Part of Tech Breakthrough, a leading market intelligence and recognition platform for technology innovation and leadership, the Data Breakthrough Awards program is devoted to honoring excellence in data technologies, services, companies and products. The Data Breakthrough Awards program provides a forum for public recognition around the achievements of data companies and solutions in categories including data analytics, management, infrastructure and hardware, storage, Business Intelligence and more. For more information visit DataBreakthroughAwards.com.

Tech Breakthrough LLC does not endorse any vendor, product or service depicted in our recognition programs, and does not advise technology users to select only those vendors with award designations. Tech Breakthrough LLC recognition consists of the opinions of the Tech Breakthrough LLC organization and should not be construed as statements of fact. Tech Breakthrough LLC disclaims all warranties, expressed or implied, with respect to this recognition program, including any warranties of merchantability or fitness for a particular purpose."
https://www.analyticsinsight.net/free-oxford-data-analytics-courses-2024/,"FREE Oxford Data Analytics Courses, 2024","Discover Free Data Analytics Offered by Oxford University in 2024

Today, Data Science has emerged as a dynamic sector with data science being extensively used for every task performed across various industries. Oxford University, a renowned institution known for its excellence in education offers a range of short-term courses in Data Science.

The Department for Continuing Education at Oxford University is offering a wide range of free courses from Python Programming on Data Science to courses on applications of Data Science for a pandemic. These courses are catered for beginners as well as professionals working under tight deadlines as they offer flexibility to individuals worldwide. These free data analytic courses include fundamental topics on data science to advanced data science for professionals willing to widely explore the field of data analytics. for professionals willing to widely explore the field of data science.

1. Python Programming in Data Science – Part 2

You can have a deep dive into deep learning, machine learning, and natural language processing. This course explains to the students about data science tools and methodologies rather than providing theoretical knowledge on data analysis. It is tailored for students who do not have much knowledge about data science and are not from any mathematical background. For successful completion of the course, students are required to complete an assignment that will be assigned based on the course. Students will learn to leverage machine learning algorithms for data extraction using Python libraries from this course and also get to know how to handle scaling issues in Big Data.

2. The Data Science of Family History

As the name suggests, this course may seem to excite you to explore the genealogical data of individuals. The students will learn the information that genealogical DNA tests are likely to reveal and analyze the ancestral history and tribe origin. The course also explains how datasets can help in finding our ancestral origin as well as there are challenges involved.

3. Pandemic Data Science

The pandemic response can be considered a great success in the field of data science as it played a huge role in it. The Pandemic Data Science course covers the critical role and contribution of data science in the healthcare sector during the pandemic. In this course, students get to know about the data analytical approaches and the extreme challenges faced in the field of data science and artificial intelligence community during the period of pandemic. The candidates will also learn the way data analysis has helped in framing the government policy and vaccination strategies in response to the pandemic.

4. Intermediate Linear Algebra

This course involves fundamental mathematics used in the area of data analysis. The course covers Linear Algebra – Rank-Nullity Theorem and Rank-Nullity Theorem as well as the basics of Inner Product. With real-world examples and step-by-step tutorials, students will learn to solve linear equations. The students will also get an introduction to the central concept of vector space.

5. Python Pandas for Data Manipulation

Students will get to learn about data cleaning, finding missing values, and replacing specific values. The individuals will gain an understanding of multi- and hierarchical indexes as well as multi-row headers. The course will also cover SQL for merging, transposing, and finding duplicate values. The course also discusses data binning, and data aggregations and also covers the basic part of data visualization."
https://www.simplilearn.com/unlock-your-data-game-with-generative-ai-techniques-webinar,Webinar: Unlock Your Data Game with Generative AI Techniques in 2024,"Unlock the Power of Data Analytics: Propel Your Career to New Heights

Join us on Tuesday, April 30, 2024, for an immersive online Data Analytics Bootcamp in collaboration with Caltech. This bootcamp is designed to accelerate your career in the rapidly evolving field of data analytics, suitable for professionals across all sectors looking to harness the power of data-driven decision-making.

Hosted by Dr. Rick Hefner, the Executive Director at Caltech’s Center for Technology and Management Education (CTME), this bootcamp offers unparalleled expert guidance. With over 40 years of experience in systems development and management education, Dr. Hefner will share invaluable insights that bridge academia and industry practices.

Who Should Attend?

This program is meticulously structured for a diverse audience, including but not limited to:

Business Professionals: Enhance your strategic decisions through data insights.

Data Enthusiasts: Dive deeper into the world of data analytics.

Aspiring Data Analysts: Kickstart your career in data analytics.

Tech Innovators: Leverage data for technological advancements.

Career Shifters: Transition into a thriving field with endless opportunities.

Why This Bootcamp Is a Must

Embarking on this bootcamp will equip you with:

Expert Guidance: Learn from Dr. Rick Hefner and other industry leaders.

Academic Excellence: Benefit from the academic rigor and cutting-edge curriculum designed by Caltech.

Practical Experience: Engage in over 20 hands-on projects and capstone projects.

Networking Opportunities: Connect with peers and professionals in the field.

What You Will Learn:

This comprehensive bootcamp covers essential advanced topics in data analytics, ensuring a robust understanding of:

Core Analytics Tools: Master Excel, SQL, and data visualization with Tableau.

Programming Skills: Learn Python and R for data analysis.

Advanced Data Techniques: Dive into machine learning and predictive analytics.

Industry Applications: See real-world applications across various industries.

Career Development: Navigate the job market with the bootcamp’s dedicated career services.

Bonus: Caltech PGP Data Analytics Preview

Get a glimpse into the Post Graduate Program in Data Analytics, which offers:

Caltech Certification: Receive a prestigious certificate recognized across industries.

In-depth Learning: Tackle complex data challenges with a comprehensive curriculum and live sessions on latest AI trends.

Hands-On Projects: Real-world applications through capstone projects and integrated labs.

Join us to transform your understanding of data analytics and carve a niche in this booming industry. Secure your spot today and be part of the future of data-driven decision making.

[CTA] Enroll Now"
https://www.kdnuggets.com/utilizing-pandas-ai-for-data-analysis,Utilizing Pandas AI for Data Analysis,"Are you proficient in the data field using Python? If so, I bet most of you use Pandas for data manipulation.

If you don’t know, Pandas is an open-source Python package specifically developed for data analysis and manipulation. It’s one of the most-used packages and one you usually learn when starting a data science journey in Python.

So, what is Pandas AI? I guess you are reading this article because you want to know about it.

Well, as you know, we are in a time when Generative AI is everywhere. Imagine if you can perform data analysis on your data using Generative AI; things would be much easier.

This is what Pandas AI brings. With simple prompts, we can quickly analyze and manipulate our dataset without sending our data somewhere.

This article will explore how to utilize Pandas AI for Data Analysis tasks. In the article, we will learn the following:

Pandas AI Setup

Data Exploration with Pandas AI

Data Visualization with Pandas AI

Pandas AI Advanced usage

If you are ready to learn, let’s get into it!

Pandas AI Setup

Pandas AI is a Python package that implements a Large Language Model (LLM) capability into Pandas API. We can use standard Pandas API with Generative AI enhancement that turns Pandas into a conversational tool.

We mainly want to use Pandas AI because of the simple process that the package provides. The package could automatically analyze data using a simple prompt without requiring complex code.

Enough introduction. Let’s get into the hands-on.

First, we need to install the package before anything else.

Next, we must set up the LLM we want to use for Pandas AI. There are several options, such as OpenAI GPT and HuggingFace. However, we will use the OpenAI GPT for this tutorial.

Setting the OpenAI model into Pandas AI is straightforward, but you would need the OpenAI API Key. If you don’t have one, you can get on their website.

If everything is ready, let’s set up the Pandas AI LLM using the code below.

You are now ready to do Data Analysis with Pandas AI.

Data Exploration with Pandas AI

Let’s start with a sample dataset and try the data exploration with Pandas AI. I would use the Titanic data from the Seaborn package in this example.

We need to pass them into the Pandas AI Smart Data Frame object to initiate the Pandas AI. After that, we can perform conversational activity on our DataFrame.

Let’s try a simple question.

The percentage of passengers who survived is: 38.38%

From the prompt, Pandas AI could come up with the solution and answer our questions.

We can ask Pandas AI questions that provide answers in the DataFrame object. For example, here are several prompts for analyzing the data.

Image by Author

You can see from the image above that the Pandas AI can provide information with the DataFrame object, even if the prompt is quite complex.

However, Pandas AI can’t handle a calculation that is too complex as the packages are limited to the LLM we pass on the SmartDataFrame object. In the future, I am sure that Pandas AI could handle much more detailed analysis as the LLM capability is evolving.

Data Visualization with Pandas AI

Pandas AI is useful for data exploration and can perform data visualization. As long as we specify the prompt, Pandas AI will give the visualization output.

Let’s try a simple example.

Image by Author

In the example above, we ask Pandas AI to visualize the distribution of the Fare column. The output is the Bar Chart distribution from the dataset.

Just like Data Exploration, you can perform any kind of data visualization. However, Pandas AI still can’t handle more complex visualization processes.

Here are some other examples of Data Visualization with Pandas AI.

Image by Author

The plot looks nice and neat. You can keep asking the Pandas AI for more details if necessary.

Pandas AI Advances Usage

We can use several in-built APIs from Pandas AI to improve the Pandas AI experience.

Cache clearing

By default, all the prompts and results from the Pandas AI object are stored in the local directory to reduce the processing time and cut the time the Pandas AI needs to call the model.

However, this cache could sometimes make the Pandas AI result irrelevant as they consider the past result. That’s why it’s good practice to clear the cache. You can clear them with the following code.

You can also turn off the cache at the beginning.

In this way, no prompt or result is stored from the beginning.

Custom Head

It’s possible to pass a sample head DataFrame to Pandas AI. It’s helpful if you don’t want to share some private data with the LLM or just want to provide an example to Pandas AI.

To do that, you can use the following code.

Pandas AI Skills and Agents

Pandas AI allows users to pass an example function and execute it with an Agent decision. For example, the function below combines two different DataFrame, and we pass a sample plot function for the Pandas AI agent to execute.

The Agent would decide if they should use the function we assigned to the Pandas AI or not.

Combining Skill and Agent gives you a more controllable result for your DataFrame analysis.

Conclusion

We have learned how easy it is to use Pandas AI to help our data analysis work. Using the power of LLM, we can limit the coding portion of the data analysis works and instead focus on the critical works.

In this article, we have learned how to set up Pandas AI, perform data exploration and visualization with Pandas AI, and advance usage. You can do much more with the package, so visit their documentation to learn further."
https://towardsdatascience.com/callbacks-and-pipeline-structures-in-langchain-925aa077227e?source=rss----7f60cf5620c9---4,Callbacks and Pipeline structures in LangChain,"Learn about the structure of LangChain pipelines, callbacks, how to create custom callbacks and integrate them into your pipelines for improved monitoring

Roshan Santhosh

·

Follow

Published in

Towards Data Science

·

11 min read

·

23 hours ago

--

Callbacks are an important functionality that helps with monitoring/debugging your pipelines. In this note, we cover the basics of callbacks and how to create custom ones for your use cases. More importantly, through examples, we also develop an understanding of the structure/componentization of LangChain pipelines and how that plays into the design of custom callbacks.

This note assumes basic familiarity with LangChain and how pipelines in LangChain work.

Basic Structure of Callbacks

To learn about the basics of callbacks in LangChain, we start with the official documentation where we can find the definition of the BaseCallbackHandler class.

BaseCallbackManager code

As you can see this is an abstract class that defines quite a few methods to cover various events in your LangChain pipeline. These methods can be grouped together into the following segments :

LLM [start, end, error, new token]

Chain [start, end, error]

Tool [start, end, error]

Agent [action, finish]

If you have worked with LangChain pipelines before, the methods along with their provided descriptions should be mostly self explanatory. For example, the on_llm_start callback is the event that gets triggered when the LangChain pipeline passes input to the LLM. And that on_llm_end is subsequently triggered when the LLM provides its final output.

NOTE : There are events triggers that can be used in addition to whats shown above. These can be found here. These cover triggers relating to Retrievers, Prompts, ChatModel etc.

Understanding how Callbacks work

Callbacks are a very common programming concept that have been widely used for a while now, so the high level concept of how callbacks work is well understood. So in this post, we focus on the specific nuances of how callbacks work in LangChain and how we could use it to satisfy our specific use cases.

Keeping in the mind the base Callback class that we saw in the previous section, we explore Callbacks in LangChain through a series of increasingly complex examples and in the process gain a better understanding of the structure of pipelines in LangChain. This would be a top-down approach to learning where we start with examples first and actual definitions later as I found that to be more useful personally for this specific topic.

Example 1

We start with a simple dummy chain that has 3 components : 2 prompts and a custom function to join them. I refer to this as a dummy example because its very unlikely that you would need two separate prompts to interact with each other, but it makes for an easier example to start with for understanding callbacks and LangChain pipelines.

Implementing this in code would look like :

The above code is pretty textbook stuff. The only possibly complex piece is the retrieve_text and RunnableLambda function thats being used here. The reason this is necessary is because the format of the output from qa_prompt1 is not compatible with the format of the output required by qa_prompt2.

Defining the custom Callback

For our custom callback, we define a new subclass of BaseCallbackHandler called CustomCallback1 which defines the on_chain_start method. The method definition is straightforward as it simply takes the input values passed to it and saves it in 2 specific variables : chain_input and serialized_input

Invoking the custom callback

The above code shows one of the possible ways to pass your custom callback to your pipeline : As a list of callback objects as the value to a corresponding key of ‘callbacks’. This also makes it easy to guess that you can pass multiple callbacks to your LangChain pipeline.

Decoding the Callback/Pipeline Structure

Now comes the interesting part. After we have defined the callbacks and passed it on to our pipeline, we now perform a deep dive into the callback outputs

We first look at the values stored in chain_input

Observations :

Though there are 3 components in our chain, there are 4 values in chain_input. Which corresponds to the on_chain_start method being triggered 4 times instead of 3.

For the first two chain_input values/ on_chain_start triggers, the input is the same as the user provided input.

We next look at the outputs of serialized_input

Observations :

The first component is a RunnableSequence which is a component that wasnt added by the user but was automatically added by LangChain. The rest of the components correspond directly to the user-defined components in the pipeline.

The full contents of serialized_input is extensive! While there is a definite structure to that content, its definitely out of scope for this post and possibly doesnt have much practical implications for an end user.

How do we interpret these results

For the most part, the outputs seen in the chain_input and serialized_input make sense. Whether its the input values or the names/IDs of the components. The only largely unknown part is the RunnableSequence component, so we take a closer look at this.

As I mentioned previously, the full contents of serialized_input is extensive and not easy to digest. So to make things easier, we look at only the high level attributes described in serialized_input and try to intrepret the results through these attributes. For this, we make use of a custom debugging function called getChainBreakdown (code in notebook).

We call getChainBreakdown on all values of serialized_input and observe the output. Specifically for the first RunnableSequence element, we look at the keys of the kwargs dict : first, midde, last, name.

On closer inspection of the kwargs argument and their values, we see that they have the same structure as our previous pipeline components. In fact, the first, middle and last components correspond exactly to the user-defined components of the pipeline.

The above details form the basis of the final conclusion that we make here. That the structure of the pipeline is like shown below :

We do make a bit of a leap here as the above flowchart was confirmed after going through a bunch of examples and observing the format in which these components are created internally by LangChain. So bear with me as we go through these other examples which will solidify the conclusion that we make here.

With the above defined structure, the other pieces of the puzzle fit together quite well. Focusing on the chain_input values, lets map them to the components (with their ordering) defined above.

Observations :

For RunnableSequence, as it acts like a wrapper for the whole pipeline, the input from the user acts as the input for the RunnableSequence component as well.

For the first ChatPromptTemplate (qa_prompt1), as the first ‘true’ component of the pipeline, it receives the direct input from the user

For RunnableLambda (retrieve_text), it receives as input the output from qa_prompt1, which is a Message object

For the last ChatPromptTemplate (qa_prompt2), it receives as input the output from retrieve_text, which is a dict with ‘prompt’ as its single key

The above breakdown shows how the structure of the pipeline described above fits perfectly with the data seen in serialized_input and chain_input

Example 2

For the next example, we extend Example 1 by adding a LLM as the final step.

For the callback, since we have now added a LLM into the mix, we define a new custom callback that additionally defines the on_llm_start method. It has the same functionality as on_chain_start where the input arguments are saved into the callback object variables : chain_input and serialized_input

Proposing the Pipeline structure

At this stage, instead of evaluating the callback variables, we switch things up and propose the potential structure of the pipeline. Given what we had learnt from the first example, the following should be the potential structure of the pipeline

So we would have a RunnableSequence component as a wrapper for the pipeline. And additionally include a new ChatOpenAI object thats nested within the RunnableSequence component.

Validating proposed structure using data

We now look at the values of in the callback object to validate the above proposed structure.

We first look at the values stored in chain_input

And then the serialized_input values :

As well as a deeper inspection of the RunnableSequence components

Observations :

The values of serialized_input validate the activation/trigger sequence that was proposed in the pipeline structure : RunnableSequence -> ChatPromptTemplate(qa_prompt1) -> RunnableLambda(retrieve_text) -> ChatPromptTemplate(qa_prompt2) -> ChatOpenAI

The values of chain_input also map correctly to the proposed structure. The only new addition is the fifth entry, which corresponds to the output from qa_prompt2, which is fed as input to the ChatOpenAI object

The components of the RunnableSequence kwargs also verify the proposed structure as the new ‘last’ element is the ChatOpenAI object

By this stage, you should have an intuitive understanding of how LangChain pipelines are structured and when/how different callback events are triggered.

Though we have only focused on Chain and LLM events so far, these translate well to the other Tool and Agent triggers as well

Example 3

For the next example, we progress to a more complex chain involving a parallel implementation (RunnableParallel)

Chain/Callback Implementation

The chain has a parallel implementation as its first block which computes two values : context and question, which are then passed on to a prompt template to create the final prompt. The parallel functionality is required because we need to pass both context and question to the prompt template at the same time, where the context is retrived from a different source while the question is provided by the user.

For the context value, we use a static function get_data that returns the same piece of text (this is a dummy version of an actual retriever used in RAG applications).

For the callback implementation, we use the same callback as the first example, CustomCallback1

Decoding the Callback/Pipeline Structure

Similar to previous examples, we start by looking at the outputs of chain_input and serialized_input

We also look do a deep dive into the RunnableSequence (index 0) and RunnableParallel (index 1) components

Observations :

Consistent with previous examples, the RunnableSequence acts as a wrapper to the whole pipeline. Its first component is the RunnableParallel component and its last component is the ChatPromptTemplate component

The RunnableParallel in turn encompasses two components : the RunnablePassthrough and the RunnableLambda (get_data).

The inputs to the first 4 components : RunnableSequence, RunnableParallel, RunnablePassthrough and RunnableLambda (get_data) are the same : the provided user input. Only for the final ChatPromptTemplate component do we have a different input, which is a dict with question and context keys.

Based on these observations, we can infer the final structure of the pipeline as such :

Example 4

Same as Example 3, but with an additional processing function for retrieving context

Chain/Callback Implementation

Decoding the Callback/Pipeline Structure

Similar to previous examples, we again look at the usual data points

We observe that there are now 2 RunnableSequence components in our pipeline. So for the next step, we deep dive into both of these RunnableSequence components to see its internal components

Observations :

For the first RunnableSequence components, its components are the same as the previous example. Starts with RunnableParallel and ends with ChatPromptTemplate

For the second RunnableSequence, its first component is the RunnableLambda (get_data) component and the last component is the RunnableLambda (format_docs) component. This is basically the part of the pipeline responsible for generating the ‘context’ value. So its possible for a LangChain pipeline to have multiple RunnableSequence components to it. Especially when you are creating ‘sub-pipelines’

In this case, the creation of the ‘context’ value can be considered a pipeline by itself as it involves 2 different components chained together. So any such sub-pipelines in your primary pipeline will be wrapped up by a RunnableSequence component

3. The values from chain_input also match up well with the pipeline components and their ordering (Not going to breakdown each component’s input here as it should be self-explanatory by now)

So based on the above observations, the following is the identified structure of this pipeline

Conclusion

The objective of this post was to help develop an (intuitive) understanding of how LangChain pipelines are structured and how callback triggers are associated with the pipeline.

By going through increasingly complex chain implementations, we were able to understand the general structure of LangChain pipelines and how a callback can be used for retrieving useful information. Developing an understanding of how LangChain pipelines are structured will also help facilitate the debugging process when errors are encountered.

A very common use case for callbacks is retrieving intermediate steps and through these examples we saw how we can implement custom callbacks that track the input at each stage of the pipeline. Add to this our understanding of the structure of the LangChain pipelines, we can now easily pinpoint the input to each component of the pipeline and retrieve it accordingly.

Resources

Notebook with code/examples : Contains few additional examples not covered in this note.

Unless specified otherwise, all images are created by the author.

In addition to Medium, I share my thoughts, ideas and other updates on Linkedin."
https://www.kdnuggets.com/7-steps-to-mastering-data-engineering,7 Steps to Mastering Data Engineering,"Image by Author

Data engineering refers to the process of creating and maintaining structures and systems that collect, store, and transform data into a format that can be easily analyzed and used by data scientists, analysts, and business stakeholders. This roadmap will guide you in mastering various concepts and tools, enabling you to effectively build and execute different types of data pipelines.

1. Containerization and Infrastructure as Code

Containerization allows developers to package their applications and dependencies into lightweight, portable containers that can run consistently across different environments. Infrastructure as Code, on the other hand, is the practice of managing and provisioning infrastructure through code, enabling developers to define, version, and automate cloud infrastructure.

In the first step, you will be introduced to the fundamentals of SQL syntax, Docker containers, and the Postgres database. You will learn how to initiate a database server using Docker locally, as well as how to create a data pipeline in Docker. Furthermore, you will develop an understanding of Google Cloud Provider (GCP) and Terraform. Terraform will be particularly useful for you in deploying your tools, databases, and frameworks on the cloud.

2. Workflow Orchestration

Workflow orchestration manages and automates the flow of data through various processing stages, such as data ingestion, cleaning, transformation, and analysis. It is a more efficient, reliable, and scalable way of doing things.

In thes second step, you will learn about data orchestration tools like Airflow, Mage, or Prefect. They all are open source and come with multiple essential features for observing, managing, deploying, and executing data pipeline. You will learn to set up Prefect using Docker and build an ETL pipeline using Postgres, Google Cloud Storage (GCS), and BigQuery APIs .

Check out the 5 Airflow Alternatives for Data Orchestration and choose the one that works better for you.

3. Data Warehousing

Data warehousing is the process of collecting, storing, and managing large amounts of data from various sources in a centralized repository, making it easier to analyze and extract valuable insights.

In the third step, you will learn everything about either Postgres (local) or BigQuery (cloud) data warehouse. You will learn about the concepts of partitioning and clustering, and dive into BigQuery's best practices. BigQuery also provides machine learning integration where you can train models on large data, hyperparameter tuning, feature preprocessing, and model deployment. It is like SQL for machine learning.

4. Analytical Engineer

Analytics Engineering is a specialized discipline that focuses on the design, development, and maintenance of data models and analytical pipelines for business intelligence and data science teams.

In the fourth step, you will learn how to build an analytical pipeline using dbt (Data Build Tool) with an existing data warehouse, such as BigQuery or PostgreSQL. You will gain an understanding of key concepts such as ETL vs ELT, as well as data modeling. You will also learn advanced dbt features such as incremental models, tags, hooks, and snapshots.

In the end, you will learn to use visualization tools like Google Data Studio and Metabase for creating interactive dashboards and data analytic reports.

5. Batch Processing

Batch processing is a data engineering technique that involves processing large volumes of data in batches (every minute, hour, or even days), rather than processing data in real-time or near real-time.

In the fifth step of your learning journey, you will be introduced to batch processing with Apache Spark. You will learn how to install it on various operating systems, work with Spark SQL and DataFrames, prepare data, perform SQL operations, and gain an understanding of Spark internals. Towards the end of this step, you will also learn how to start Spark instances in the cloud and integrate it with the data warehouse BigQuery.

6. Streaming

Streaming refers to the collecting, processing, and analysis of data in real-time or near real-time. Unlike traditional batch processing, where data is collected and processed at regular intervals, streaming data processing allows for continuous analysis of the most up-to-date information.

In the sixth step, you will learn about data streaming with Apache Kafka. Start with the basics and then dive into integration with Confluent Cloud and practical applications that involve producers and consumers. Additionally, you will need to learn about stream joins, testing, windowing, and the use of Kafka ksqldb & Connect.

If you wish to explore different tools for various data engineering processes, you can refer to 14 Essential Data Engineering Tools to Use in 2024.

7. Project: Build an end-to-end Data Pipeline

In the final step, you will use all the concepts and tools you have learned in the previous steps to create a comprehensive end-to-end data engineering project. This will involve building a pipeline for processing the data, storing the data in a data lake, creating a pipeline for transferring the processed data from the data lake to a data warehouse, transforming the data in the data warehouse, and preparing it for the dashboard. Finally, you will build a dashboard that visually presents the data.

Final Thoughts

All the steps mentioned in this guide can be found in the Data Engineering ZoomCamp. This ZoomCamp consists of multiple modules, each containing tutorials, videos, questions, and projects to help you learn and build data pipelines.

In this data engineering roadmap, we have learned the various steps required to learn, build, and execute data pipelines for processing, analysis, and modeling of data. We have also learned about both cloud applications and tools as well as local tools. You can choose to build everything locally or use the cloud for ease of use. I would recommend using the cloud as most companies prefer it and want you to gain experience in cloud platforms such as GCP."
https://www.mcgill.ca/datascience/channels/event/edugenai-shaping-tomorrows-learning-generative-ai-356066,EduGenAI: Shaping Tomorrow's Learning with Generative AI,"DATE: May 13th & 14th 2024

LOCATION : Arts Building, room 150, McGill University, 853 Sherbrooke West

Note: On May 13th AM, there will be an Upskill Workshop at Google HQ. Further details will be sent to participants.

Overview: In this 2-day symposium, we will explore the impacts and applications of generative AI (GenAI) in higher education from academic and industry perspectives, focusing on both the theoretical and practical sides of GenAI.

Designed to foster connections between academic research and real-world application, this bilingual event (English and French) will take place at the McGill University Downtown campus and will feature presentations from academic and industry leaders through keynote speeches, panel discussions, and hands-on workshops, exploring best practises and the ethical implications of using GenAI in higher education.

Food and beverages will be provided. Spaces are limited, so please make sure to register below. Further event details will be sent to registrants by email.

Schedule

Co-Hosted by:"
https://www.simplilearn.com/tutorials/machine-learning-tutorial/stock-price-prediction-using-machine-learning,Stock Market Prediction using Machine Learning in 2024,"Stock price analysis has been a critical area of research and is one of the top applications of machine learning. This tutorial will teach you how to perform stock price prediction using machine learning and deep learning techniques. Here, you will use an LSTM network to train your model with Google stocks data.

What is the Stock Market?

A stock market is a public market where you can buy and sell shares for publicly listed companies. The stocks, also known as equities, represent ownership in the company. The stock exchange is the mediator that allows the buying and selling of shares.

Importance of Stock Market

Stock markets help companies to raise capital.

It helps generate personal wealth.

Stock markets serve as an indicator of the state of the economy.

It is a widely used source for people to invest money in companies with high growth potential.

Stock Price Prediction

Stock Price Prediction using machine learning algorithm helps you discover the future value of company stock and other financial assets traded on an exchange. The entire idea of predicting stock prices is to gain significant profits. Predicting how the stock market will perform is a hard task to do. There are other factors involved in the prediction, such as physical and psychological factors, rational and irrational behavior, and so on. All these factors combine to make share prices dynamic and volatile. This makes it very difficult to predict stock prices with high accuracy.

Understanding Long Short Term Memory Network

Here, you will use a Long Short Term Memory Network (LSTM) for building your model to predict the stock prices of Google.

LTSMs are a type of Recurrent Neural Network for learning long-term dependencies. It is commonly used for processing and predicting time-series data.

From the image on the top, you can see LSTMs have a chain-like structure. General RNNs have a single neural network layer. LSTMs, on the other hand, have four interacting layers communicating extraordinarily.

LSTMs work in a three-step process.

The first step in LSTM is to decide which information to be omitted from the cell in that particular time step. It is decided with the help of a sigmoid function. It looks at the previous state (ht-1) and the current input xt and computes the function.

There are two functions in the second layer. The first is the sigmoid function, and the second is the tanh function. The sigmoid function decides which values to let through (0 or 1). The tanh function gives the weightage to the values passed, deciding their level of importance from -1 to 1.

The third step is to decide what will be the final output. First, you need to run a sigmoid layer which determines what parts of the cell state make it to the output. Then, you must put the cell state through the tanh function to push the values between -1 and 1 and multiply it by the output of the sigmoid gate.

With this basic understanding of LSTM, you can dive into the hands-on demonstration part of this tutorial regarding stock price prediction using machine learning.

Google Stock Price Prediction Using LSTM

1. Import the Libraries.

2. Load the Training Dataset.

The Google training data has information from 3 Jan 2012 to 30 Dec 2016. There are five columns. The Open column tells the price at which a stock started trading when the market opened on a particular day. The Close column refers to the price of an individual stock when the stock exchange closed the market for the day. The High column depicts the highest price at which a stock traded during a period. The Low column tells the lowest price of the period. Volume is the total amount of trading activity during a period of time.

3. Use the Open Stock Price Column to Train Your Model.

4. Normalizing the Dataset.

5. Creating X_train and y_train Data Structures.

6. Reshape the Data.

7. Building the Model by Importing the Crucial Libraries and Adding Different Layers to LSTM.

8. Fitting the Model.

9. Extracting the Actual Stock Prices of Jan-2017.

10. Preparing the Input for the Model.

11. Predicting the Values for Jan 2017 Stock Prices.

12. Plotting the Actual and Predicted Prices for Google Stocks.

As you can see above, the model can predict the trend of the actual stock prices very closely. The accuracy of the model can be enhanced by training with more data and increasing the LSTM layers.

Conclusion

The stock market plays a remarkable role in our daily lives. It is a significant factor in a country's GDP growth. In this tutorial, you learned the basics of the stock market and how to perform stock price prediction using machine learning.

Do you have any questions related to this tutorial on stock prediction using machine learning? In case you do, then please put them in the comments section. Our team of experts will help you answer your questions.

If you are interested in learning further about Machine Learning, including the various ML applications across industries, do explore Simplilearn’s Post Graduate Program in AI and Machine Learning in partnership with Purdue University, and in collaboration with IBM. This comprehensive 12-month program covers everything from Statistics, Machine Learning, Deep Learning, Reinforcement Learning, to Natural Language Programming and more. You get to learn from global experts and at the end of the program walk away with great endorsements from industry and academic leaders and a skillet that is today the most in-demand in organizations across the world.

Happy learning!"
https://www.datasciencecentral.com/retrieval-augmented-fine-tuning-and-data-integrations/,tuning and data integrations,"Presentation and discussion with Suman Aluru and Caleb Stevens

In the latest episode of the “AI Think Tank Podcast,” I had the pleasure of hosting a deep dive into the world of AI advancements, specifically focusing on “RAFT” (Retrieval Augmented Fine Tuning). Joining me were the esteemed guests Suman Aluru and Caleb Stevens, who both have much to do with AI infrastructure and application. Our conversation revolved around how RAFT bridges the critical gaps between fine-tuning and retrieval-augmented generation (RAG), and the significant impact this has on AI-driven applications.

We opened the episode by discussing the importance of RAFT in the current AI landscape, where Suman eloquently described its role in enhancing the accuracy of AI responses and reducing the common errors known as “hallucinations.” Caleb complemented this by highlighting the practical deployment of RAFT in IT infrastructures, particularly its effectiveness in managing semantic data where traditional databases might struggle.

A pivotal moment of our discussion was Suman’s live demonstration, which involved querying a model fine-tuned with data from the AI Think Tank podcast’s website. This not only showcased RAFT’s real-world applicability but also demonstrated its power in maintaining the relevancy of AI systems with updated data, eliminating the need for extensive retraining.

Figure-1 Cited from https://arxiv.org/abs/2403.10131 Download full pdf here.

We also delved into the challenges associated with updating AI models post-training. Here, RAFT was discussed as a dynamic solution capable of integrating fresh data seamlessly, thus enabling AI systems to process complex queries with enhanced contextual understanding. The discussion on vector databases and embedding techniques provided a clear insight into the technological strategies that make RAFT a standout choice.

Figure-2 Cited from https://arxiv.org/abs/2403.10131 Download full pdf here.

The episode wrapped up with an engaging Q&A session where our listeners had the opportunity to probe deeper into the applications of RAFT, its advantages over traditional AI training techniques, and its potential transformative impact across various sectors.

Overall, this episode offered a thorough exploration of how advanced techniques like RAFT can significantly bolster the functionality and reliability of AI systems, ensuring they perform domain-specific tasks more effectively and with greater accuracy. The feedback from our community was immensely positive, highlighting the importance and interest in such cutting-edge technologies in the AI space.

As usual, I gained much insight from Suman’s presentations and Caleb’s keen understanding at the code level. We expect to continue this exploration of RAG and RAFT as things develop."
https://news.uark.edu/articles/70151/arkansas-data-science-association-presents-march-madness-challenge-celebration-april-16,Arkansas Data Science Association Presents March Madness Challenge Celebration April 16,"The March Madness Challenge brought together students from across campus, showcasing their sports knowledge and competitive spirit. The top performers in the bracket challenge will be honored at an event at 5 p.m. Tuesday, April 16, in Bell Engineering Center 1108E. Alongside the celebration of victory, attendees can indulge in complimentary food and enjoy the opportunity to claim merchandise.

The Bracket Challenge has been a thrilling journey, and we're excited to recognize the outstanding performances of our participants. This event not only celebrates the winners but also provides an opportunity for our university community to come together and enjoy some well-deserved festivities.

The celebration promises to be an evening of camaraderie, competition and fun. Attendees can look forward to reliving the excitement of March Madness, sharing stories of their bracket predictions and enjoying the company of fellow sports enthusiasts. For more information about the March Madness Challenge Winners Celebration, please visit hogsync.uark.edu/adsa/home or contact Chloe Burgess at ceb043@uark.edu or Sophia Forrester at adrianaf@uark.edu.

This event is supported by the Student Activities Fee as a funded event by the Associated Student Government and is free to all currently enrolled University of Arkansas, Fayetteville, students who pay the student activities fee. This event is held in a venue that meets ADA standards. Individuals with disabilities are encouraged to attend this event. If you require a reasonable accommodation in order to participate in this event, please contact Lee Shoultz, eshoultz@uark.edu."
https://www.kdnuggets.com/geospatial-data-analysis-with-geemap,Geospatial Data Analysis with Geemap,"Illustration by Author

Geospatial data analysis is a field addressed to deal with, visualize and analyze a special type of data, called geospatial data. Compared to the normal data, we have tabular data with an additional column, the location information, such as latitude and longitude.

There are two main types of data: vector data and raster data. When dealing with vector data, you still have a tabular dataset, while raster data are more similar to images, such as satellite images and aerial photographs.

In this article, I am going to focus on raster data provided by Google Earth Engine, a cloud computing platform that provides a huge data catalog of satellite imagery. This kind of data can be easily mastered from your Jupyter Notebook using a life-saving Python package, called Geemap. Let’s get started!

What is Google Earth Engine?

Screenshot by Author. Home page of Google Earth Engine.

Before getting started with the Python Library, we need to understand the potential of Google Earth Engine. This cloud-based platform, powered by Google Cloud platform, hosts public and free geospatial datasets for academic, non-profit and business purposes.

Screenshot by Author. Overview of Earth Engine Data Catalog.

The beauty of this platform is that it provides a multi-petabyte catalog of raster and vector data, stored on the Earth Engine servers. You can have a fast overview from this link. Moreover, it provides APIs to facilitate the analysis of raster datasets.

What is Geemap?

Illustration by Author. Geemap library.

Geemap is a Python library that allows to analyze and visualize huge amounts of geospatial data from Google Earth Engine.

Before this package, it was already possible to make computational requests through JavaScript and Python APIs, but Python APIs had limited functionalities and lacked documentation.

To fill this gap, Geemap was created to permit users to access resources of Google Earth Engine with few lines of code. Geemap is built upon eartengine-api, ipyleaflet and folium.

To install the library, you just need the following command:

I recommend you experiment with this amazing package in Google Colab to understand its full potential. Take a look at this free book written by professor Dr. Qiusheng Wu for getting started with Geemap and Google Earth Engine.

How to Access Earth Engine?

First, we need to import two Python libraries, that will be used within the tutorial:

In addition to geemap, we have imported the Earth Engine Python client library, called ee.

This Python library can be utilized for the authentication on Earth Engine, but it can be faster by using directly the Geemap library:

You need to click the URL returned by this line of code, which will generate the authorization code. First, select the cloud project and, then, click the “GENERATE TOKEN” button.

Screenshot by Author. Notebook Authenticator.

After, it will ask you to choose the account. I recommend taking the same account of Google Colab if you are using it.

Screenshot by Author. Choose an account.

Then, click the check box next to Select All and press the “Continue” button. In a nutshell, this step allows the Notebook Client to access the Earth Engine account.

Screenshot by Author. Allow the Notebook Client to access your Earth Engine account.

After this action, the authentication code is generated and you can paste it into the notebook cell.

Screenshot by Author. Copy the Authentication Code.

Once the verification code is entered, you can finally create and visualize this interactive map:

For now, you are just observing the base map on top of ipyleaflet, a Python package that enables the visualization of interactive maps within the Jupyter Notebook.

Create Interactive Maps

Previously, we have seen how to authenticate and visualize an interactive map using a single line of code. Now, we can customize the default map by specifying the latitude and longitude of the centroid, the level of zoom and the height. I have chosen the coordinates of Rome for the centre to focus on the map of Europe.

If we want to change the base map, there are two possible ways. The first way consists of writing and running the following code line:

Alternatively, you can change manually the base map by clicking the icon of ring spanner positioned at the right.

Moreover, we see the list of base maps provided by Geemap:

This is the output:

As you can notice, there is a long series of base maps, most of them available thanks to OpenStreetMap, ESRI and USGS.

Earth Engine Data Types

Before showing the full potential of Geemap, it’s important to know two main data types in Earth Engine. Take a look at the Google Earth Engine’s documentation for more details.

Illustration by Author. Example of vector data types: Geometry, Feature and FeatureCollection.

When handling vector data, we use principally three data types:

Geometry stores the coordinates needed to draw the vector data on a map. Three main types of geometries are supported by Earth Engine: Point, LineString and Polygon.

Feature is essentially a row that combines geometry and non-geographical attributes. It’s very similar to the GeoSeries class of GeoPandas.

FeatureCollection is a tabular data structure that contains a set of features. FeatureCollection and GeoDataFrame are almost identical conceptually.

Screenshot by Author. Example of Image data type. It shows the Australian Smoothed Digital Elevation Model (DEM-S)

In the world of raster data, we focus on Image objects. Google Earth Engine’s Images are composed of one or more brands, where each band has a specific name, estimated minimum and maximum, and description.

If we have a collection or time series of images, ImageCollection is more appropriate as a data type.

Screenshot by Author. Copernicus CORINE Land Cover.

We visualize the satellite imagery showing the land cover map of Europe. This dataset provides the changes between 1986 and 2018.

First, we load the image using ee.Image and, then, select the band “landcover”. Finally, let’s visualize the image by adding the loaded dataset to the map as a layer using Map.addLayer.

Screenshot by Author.

Similarly, we can do the same thing to load and visualize the satellite imagery showing the land cover map of Europe. This dataset provides the changes between 1986 and 2018.

Screenshot by Author. Offline high-resolution imagery of methane concentrations.

To visualize an Earth Engine ImageCollection, the lines of code are similar, except for ee.ImageCollection.

Screenshot by Author.

That’s great! From this map, we notice how Methane, one of the most important contributors to the greenhouse effect, is distributed within the globe.

Final Thoughts

This was an introductory guide that can help you work with Google Earth Engine data using Python. Geemap is the most complete Python library to visualize and analyze this type of data.

If you want to go deeper into this package, you can take a look at the resources I suggested below.

The code can be found here. I hope you found the article useful. Have a nice day!

Useful resources:

Google Earth Engine

Geemap Documentation

Book Earth Engine and Geemap: Geospatial Data Science with Python"
https://www.opportunitiesforafricans.com/standard-bank-data-science-graduate-programme-2024/,Standard Bank Data Science Graduate Programme 2024 for young South Africans. – Opportunities For Africans,"531

Application Deadline: 30 June 2024

Applications are now open for the Standard Bank Data Science Graduate Programme. In an era where our world is shaped by data-led solutions, we’re at the forefront of building this critical function in our business. The Standard Bank Data Science Graduate Programme puts you right at the centre of this fast-moving area, with a focus on structured training and development and a commitment to technical excellence.

You’ll be hugely talented, with a strong quantitative background. You’ll also be obsessed with solving real client problems by analysing data, identifying insights, and building bespoke solutions. Bring these skills to our graduate programme, and discover how – through learning, support, and unrivalled exposure – you’ll transform your potential into impact.

Qualifications

Masters or Honours in:

Data Science

Computer Science

Statistics

Applied Mathematics

Actuarial Science

Engineering (Chemical, Electrical, Mechatronics)

Informatics

Quantitative Risk Management

Honours qualification to be completed by 31 December 2024.

Minimum requirements:

South African Citizen

Should you have work experience, it should not exceed 24 months

Full academic transcripts to be submitted with application for undergraduate and postgraduate studies.

For More Information:"
https://www.aramark.com/newsroom/news/2024/april/data-insights--how-a-baseball-rule-change-impacts-the-fan-experi,Data Insights: How a Baseball Rule Change Impacts the Fan Experience,
https://www.analyticsinsight.net/best-ways-to-use-chatgpt-for-data-science-in-google-sheets/,Best Ways to Use ChatGPT for Data Science in Google Sheets,"Here is the best way to use ChatGPT for Data Science in Google Sheets

Google Sheets has been extensively acknowledged as a complete spreadsheet tool, serving as a foundation for data association and analysis. Still, exercising complete capabilities might lead to challenges, particularly for those lacking a background in data analytics. ChatGPT provides a valid result for data wisdom in Google Sheets; the only thing you need to do is create an OpenAI account and exploit the power of AI.

Artificial Intelligence is transforming our interaction with digital tools and enabling a drastic change in the technological landscape. Utilizing ChatGPT within Google Sheets enhances users’ abilities and simplifies tasks efficiently.

Using the ChatGPT

GPT—3 can interpret advanced prompts and orders. Thus, combining GPT—3 with Google Sheets and an integrated point like Google Script would not pose a challenge.

First, open Google Workspace, right-click on extensions and add them to Google Sheets. You might now see a window pop up displaying Google Workspace Marketspace.

Enter the word “ChatGPT” in the search box field, which is located at the top right of your screen.

Click on the Google Sheets option. After installation is completed, one needs to spark the integration in Google Sheets.

Setting up ChatGPT Integration

The ChatGPT API is an extension that allows you to incorporate ChatGPT’s features into your programs, goods, or services.

It understands and generates such responses utilizing natural language processing (NLP). This is ideal for erecting Artificial Intelligence chatbots, virtual sidekicks, and other interactive services. You can pierce ChatGPT’s capability to give mortal- suchlike comebacks to queries and bear on casual exchanges.

It can manage a massive measure of data and incorporate seamlessly with multitudinous systems and platforms. It also enables programmers to customize the frame tallying to their unique demands, which could boost the perfection and connection of the produced content.

It understands and generates responses utilizing natural language processing (NLP). This is ideal for creating AI chatbots, virtual sidekicks, and other interactive services.

Allowing API Access

The GPT for the Sheets plugin requires an API key, which can be fluently attained from the OpenAI gate. To induce an OpenAI API key, follow this procedure.

Still, go to OpenAI and subscribe if you haven’t yet created a stoner account.

Log in after creating an account, and go to the OpenAI API keys webpage.

Following that, find and click the produce new Secret key option in the runner’s center.

Still, if everything goes smoothly, you should see a pop-up window with the API keys and an “API crucial Generated” communication on your display.

Next, click the green dupe option next to your API key to dupe it to the clipboard.

Open a fresh spreadsheet or refresh the bone that’s formerly open.

Click Extensions> GPT for SheetsTM and DocsTM> Set API key.

Under the API input portion, bury the previously attained API key and elect Check. If it’s a working API key, you’ll get an announcement saying, “Your OpenAI API key is valid!”

To continue, click the Save API key button.

The final step is to enable the GPT for sheet extension. To do this, go to Extensions> GPT for Sheets and Croakers> Enable GPT functions.

Conclusion"
https://www.analyticsinsight.net/can-you-start-a-data-science-career-in-30-days/,Can you Start a Data Science Career in 30 Days?,"Can You Start a Data Science Career in 30 Days? Exploring the Path to Entry-Level Proficiency

Embarking on a career in data science can be an exciting and rewarding journey, but it often comes with the misconception that it requires years of study and experience to get started. However, with the right approach and resources, it’s possible to kickstart your data science career in as little as 30 days. In this article, we’ll explore the path to entry-level proficiency in data science and how you can accelerate your learning journey within a month.

Setting Realistic Expectations

Before diving into the specifics, it’s essential to set realistic expectations. While 30 days may not be enough time to become a data science expert, it is sufficient to lay a solid foundation and start building essential skills. The key is to focus on acquiring fundamental knowledge and practical experience that will enable you to tackle real-world data science challenges.

Day 1-10: Learn the Basics

The first ten days should be dedicated to learning the fundamental concepts of data science. Start by familiarizing yourself with key topics such as:

Programming Languages: Learn the basics of programming languages commonly used in data science, such as Python and R.

Statistics and Mathematics: Brush up on foundational concepts in statistics and mathematics, including probability, linear algebra, and calculus.

Data Manipulation: Explore techniques for data manipulation and analysis using libraries like Pandas and NumPy in Python.

Day 11-20: Dive Deeper into Machine Learning

During the next ten days, focus on understanding machine learning algorithms and techniques. Dive deeper into:

Supervised Learning: Learn about regression, classification, and other supervised learning algorithms.

Unsupervised Learning: Explore clustering, dimensionality reduction, and other unsupervised learning techniques.

Model Evaluation: Understand how to evaluate the performance of machine learning models and choose the right metrics for different tasks.

Day 21-30: Apply Your Knowledge

In the final stretch, it’s time to apply your knowledge to real-world projects and gain practical experience. Spend the remaining days:

Working on Projects: Take on data science projects, such as analyzing datasets, building predictive models, or solving Kaggle competitions.

Building Your Portfolio: Showcase your projects and skills by creating a portfolio website or GitHub repository.

Networking: Connect with other aspiring data scientists, join online communities, and participate in discussions to expand your network and learn from others.

Conclusion

While starting a data science career in 30 days may seem ambitious, it’s entirely feasible with dedication, focus, and the right resources. By following a structured learning plan, acquiring fundamental knowledge, and gaining practical experience through projects, you can lay the groundwork for a successful career in data science. Remember that learning is an ongoing process, and the key to success lies in continuous learning and improvement."
https://www.educationtimes.com/article/careers/99735297/portalexclusive-why-economics-with-data-science-offers-promising-career-prospects,PORTAL EXCLUSIVE: Why Economics with Data Science offers promising career prospects,"In today's digital age, all organisations rely heavily on complex data and statistical models to make strategic decisions. There is a huge demand for skilled individuals who can effectively handle large datasets, generate insights, drive innovation, and shape business strategies. Hence a degree in Economics and Data Science emerges as a field brimming with opportunities where students will acquire all the necessary skills to fit into the role of a data scientist. According to a report by IBM, data scientist jobs are expected to increase by 28% by 2025 and continue to grow in the future.

Data is a collection of facts in the form of numbers, or texts, or images, or even videos, or a mixture of these. Gathering data is not something new to us. We have been doing it for thousands of years. However, the form and scale of collecting data have changed, especially in the last two decades. This has a lot to do with the advent of large servers and cloud storage facilities where trillions of gigabytes of data can be stored, if not more. This transformation in data collection underpins the significance of integrating economic theories with data science methodologies.

Today, every detail of our lives pops up as a data point in some database or the other. For example, the number of hours we are on the phone, the time we spend on social media, the products we usually check on e-commerce platforms, the paths we generally take to go to school or work, the money we spend on food delivery apps, are all data points. Similarly, the thousands of goods imported and exported by India daily, stock prices at every second, the number of children going to school daily, and the monthly income of daily-wage workers, are all data too; we call it economic data. Since the volume of data is so large, we must rely on modern developments to handle Big Data. These modern techniques and approaches are what we learn in Data Science.

Economists rely on data to analyse and make causal inferences about economic phenomena, make forecasts, and formulate policies by employing data science tools such as machine learning algorithms, predictive modelling, and data visualisation techniques. Data science helps economists to make accurate predictions, optimise decision-making processes, and design better policies.

The complementarity between economic modelling and data science methods has gained traction in the past decade. Industries highly rate individuals proficient in economic theory and can use data science methods to model and analyse data and at the same time prescribe policies and strategies to governments or businesses. This recognition has opened a plethora of opportunities for individuals looking to pursue a career that blends traditional economic theories and cutting-edge data science techniques. Recognising this trend, several universities are offering a curriculum tailored to meet the demands of today's job market where candidates are valued for their abilities to handle and analyse data.

A degree in economics and data science equips students with the vital skills for today's data-driven economic decisions, including analysing complex datasets, applying statistical models, and using tools like Python, R, and SQL. It enhances critical thinking for real-world problem-solving and teaches effective communication of complex data to stakeholders. This skill set opens doors to numerous opportunities where analytical and strategic capabilities are crucial, preparing graduates for impactful roles across industries, including finance, government, healthcare, technology, and consulting, reflecting the broad applicability of the skills acquired through an economics and data science degree.

Here are some of the exciting career paths that students can explore in the field of Economics and Data Science:

● Data scientist

● Data analyst

● Policy analyst

● Economist

● Financial analyst

● Marketing analyst

● Business analyst

● Risk analyst

Most of these university programmes are interdisciplinary so that students develop a well-rounded skill set. Apart from modelling data, students receive training in result interpretation, policy and strategy recommendations, and the skills to prepare comprehensible reports for the stakeholders. The courses aim to develop a foundation of economic theory, along with hands-on experience with industry-standard tools and technologies.

Hence, choosing a career in economics and data science offers numerous possibilities in the job market. As businesses and governments continue to prioritise data-driven approaches, the demand for skilled professionals who can interpret and derive insights from data is expected to soar in the coming years."
https://datascientest.com/en/few-shot-prompting-refine-your-generative-ai-results,Few-Shot Prompting: Refine your generative AI results,"Few-Shot Prompting is a prompt engineering technique that involves showing the AI a few examples (or shots) of the desired results. Using the examples provided, the model learns a specific behaviour, enabling it to carry out similar tasks.

For example, a marketing manager could use this Prompting technique to classify customer comments as positive or negative. To do this, he presents the AI with several comments, specifying whether they are positive or negative.

Using these examples, the language model should be able to identify the patterns associated with positive and negative feelings. It will then be able to classify the other comments.

Since ChatGPT was democratised, some users have seen it as a revolutionary tool, while others still doubt its capabilities.

The difference between the two? Mastery of prompts. And in particular its techniques such as few shot Prompting.

Indeed, large language models (LLMs) like ChatGPT are trained with massive volumes of data. But all this information is supposed to meet the needs of as many people as possible.

However, if you ask it to carry out a specific task, there is a high risk that it will provide you with a very general response, far removed from your initial expectations.

Few-Shot Prompting is essential for refining artificial intelligence responses. In this sense, it’s a formidable fine-tuning strategy.

As artificial intelligence tools continue to develop, so do prompt engineering techniques. And not just Few-Shot Prompting. Other methods are appearing regularly. To find out which techniques are the most effective, it’s vital to get trained.

Fortunately, at DataScientest, we have created a course dedicated to prompt engineering. By the end of the programme, you’ll know how to communicate effectively with the major language models, obtaining all the results you want."
https://newsroom.unl.edu/announce/cse/17360/94829,Attend Data Science Open House event on May 2,"Data Science Open House

Thursday, May 2

3:30–5 PM

347 Avery Hall

The School of Computing is hosting a Data Science Open House event to share info and collect feedback about our new major.

Following a brief overview presentation at the beginning, this event will be held in an informal format similar to the Student Advisory Panel, so students may come and go as their schedule allows. Faculty and advisors will be present to answer questions about data science courses, enrolling in the major, adding it as a second major, and more. We also invite current data science majors to attend and share feedback about their first year in the program.

This event is open to current data science majors, other computing majors, and any UNL students who are interested learning more about the data science major.

There will be free pizza and beverages for students who attend!

RSVP and get more info here."
https://www.kdnuggets.com/popular-google-certification-for-all-areas-in-the-tech-industry,Popular Google Certification for All Areas in the Tech Industry,"Image by Author

When people say they work in the tech industry, many assume they are software engineers, know 3 different programming languages, and can build applications overnight. But the tech industry is way more than that.

As it continues to grow, we not only need software engineers and data scientists, but also cyber security analysts, marketers, design professionals, and more. If you’re looking for a career change but want to keep your options open and outside of coding, continue reading…

Data Analytics

Link: Google's Data Analytics Professional Certification

Let’s start with the most technical for those interested in working with data, prepping it and analysing it for the decision-making process.

Google's Data Analytics professional certification allows you to understand the practices and processes used by associate data analysts. You will learn how to clean data, analyze it, and create visualizations using tools such as SQL, R programming, and Tableau.

As we understand the value of data, we also understand that the value of data analysts will continue to grow.

Project Management

Link: Google's Project Management Professional Certification

The tech industry is moving quickly and new projects are getting released every day. This is where I introduce project management and its importance in any industry. Without project management, many of these new tools would never have been deployed for us to have access to them.

Project management is the application of processes, methods, skills, knowledge, and experience to achieve specific objectives to ensure that a project is successful. In this Google Project Management Professional certification, you will learn how to effectively document projects, learn the foundations of Agile project management, Scrum, and also practice strategic communication and develop your problem-solving skills.

Cybersecurity

Link: Google's Cybersecurity Professional Certification

Data is the new gold and just like gold, organisations have processes and tools in place to ensure its security.

In this Google Cybersecurity Professional Certification, you will learn about the best cybersecurity practices and their impacts on organisations. You will identify common risks and vulnerabilities and apply techniques on how to mitigate them.

Cybersecurity is all about protection, so dive into protecting networks, devices, data, and people with a variety of tools as well as hands-on experience with Python, Linux, and SQL.

IT Support

Link: Google's IT Support Professional Certification

The tech industry has so much to it. It’s like building a house from scratch and every contractor is responsible for ensuring that the different levels always meet the gold standard based on their expertise. This is where IT support comes into it.

In this Google IT Support Professional Certification, you will learn about the day-to-day tasks that IT support deals with which include computer assembly, wireless networking, installation, and customer service. You will also learn how to identify problems to troubleshoot and debug using tools such as Linux, Domain Name Systems, Command-Line Interface, and Binary Code.

Marketing & E-commerce

Link: Google's Marketing & E-commerce Professional Certification

You have your software engineers building products. You have your data analysts analyzing the data. You have the project managers ensuring that the product lands into production. You have the cybersecurity and IT support ensuring that everything is smooth and nothing attacks the organisation. So everything is in place, what now?

Selling the product. Making sure everybody knows about it. Making the money from the great product! This is where Google's Marketing & E-commerce Professional Certification comes in.

You will learn about the fundamentals of digital marketing and e-commerce, and how to attract and engage customers through a variety of digital marketing channels. You will then learn how to measure the performance of these through analytics and present insights.

Wrapping it Up

One industry, with 5 potential possibilities on how you can get your foot in the door. All these professionals are required and make up the organisation's success stack.

The tech world will continue to grow and with that, there are more areas and sectors you can transition into.

Start learning today!"
https://www.vanguardngr.com/2024/04/meet-aadam-bodunrin-agro-economist-revolutionizing-industries-through-data-science-analytics/,"Meet Aadam Bodunrin, agro-economist revolutionizing industries through Data Science, Analytics","By Miftaudeen Raji

The transformative power of Data Science techniques and Artificial Intelligence extends beyond borders, permeating industries vital to global prosperity. From safeguarding financial transactions through fraud detection to revolutionizing healthcare with early cancer detection, these technologies have reshaped our world. In tandem, agriculture benefits from predictive analytics, enhancing crop yield predictions and fostering sustainable practices.

Aadam Olalekan Bodunrin, a University of Ibadan-trained Agricultural Economist turned Data Scientist, is wielding the tools of data science and analytics to propel industries into new frontiers. His visionary approach influences sectors such as financial technology and e-commerce. As we delve into Bodunrin’s journey, we uncover how his expertise shapes industries and enriches lives worldwide.

READ ALSO: Meet Nigerian tech genius, Cyprian Emeka Uzoh, behind memory chips in dancing robots

Early Career – Grit, Hard work and Vision:

On the cusp of graduating from his Bachelor’s degree program and undergoing the National Youth Service Corp, Aadam served as the Corper Liasion Officer for Ikono Local Government in Akwa Ibom. He was instrumental in helping his fellow Corp members organise several community development programs centred on health and the environment, focusing on sustainability. Overseeing community development programs made him realise the impact and value that digital media can have in enriching people’s lives.

Circa 2018, Nigeria’s digital media landscape was fast-growing; Digify Africa, in collaboration with Facebook, launched “Digify Pro” – the first-of-its-kind Digital marketing program to train Nigerian graduates in Digital Marketing.

Aadam was part of the program’s first cohort and delved into social media management, Advertising and learning other 21st-century skills. He took his learning from this program to QVT Media as a Graduate media Trainee and was able to work with organisations such as Akin Fadeyi Foundation and Ampz Sports.

With his understanding of the Digital media landscape, he collaborated with colleagues to kickstart “Web of Business Owners” – a community-based platform that helps small and medium-scale businesses effectively use digital media platforms to improve their bottom line. Over 200 businesses have benefitted from this endeavour.

When answering what led him to the world of Data Science and Analytics, he chuckled and said, “I stumbled on the infamous Data Science is the sexiest job of the 21st century”. That article changed his approach to a burgeoning career and his dream of being a Data Scientist was gradually coming to fruition after he was selected as a Data Science fellow with the Information and Data Analytics Foundation (iDAF).

Diverse Experience – Working with National and International Organisations:

The Data Science fellowship with iDAF was fulfilling and enriching; it broadened his scope and the level of impact he can achieve in his community. During the program, he was part of the team that built a COVID-19 tracking dashboard and led a project that used natural language processing (NLP) to automate the extraction of entities to record business transactions securely.

In layman’s terms, they built a model that helped small and medium-scale business owners store their daily transactions on their mobile devices.

His proudest moment came when he worked as a Power BI Analyst with Survey54, now Safiyo, a United Kingdom-based company. Safiyo uses data to help brands in the African continent measure food, drink (Alcoholic and Non-Alcoholic), and traditional media consumption.

The project was pan-African, an international gig that combined his two loves—Data Science and Digital Media, and he is glad they were able to help tons of African-oriented businesses make data-driven decisions.

“There is a lot of application of Data Science to E-commerce in Nigeria. The likes of Konga and Jumia already utilize recommender systems to suggest items for customers to buy”. I can tell he was glad to have worked with Omnibiz and helped the organisation understand its customers’ buying patterns.

Before deciding to further his education – a Master’s degree in Data Science at East Carolina University, he worked with Kippa – a Financial technology company, as its first Data Analyst hire. Apart from his regular work, he collaborated with his colleagues to create an MSME report for Nigeria – a feat first achieved by a financial technology company in Nigeria. “I am always happy whenever anyone reaches out to ask how we were able to create the report and present a holistic look at small and medium scale business across all industries in Nigeria”.

Working in different industries, from Digital Media to a brief stint at Information technology consulting, E-commerce and Financial Technology, he has used data-driven decisions to grow several businesses and enrich his community.

Future Endeavors:

“As I look at the culmination of my Masters Degree in Data Science, I will be looking forward to taking my skills, wealth of experience across different Industries to the health Sector”. His stint at Ancestry further propelled him to use his knowledge and solve some of the biggest problems in global Health.

In addition, his research and collaboration with the global burden of Diseses network on Cardiovascular Diseases, Nervous Disorders and age-sex-specific mortality & life expectancy which can all be found via his research gate profile will help national and International government institutions to make effective health-related policies.

There are two major problems he is looking to significantly contribute to in the next years – glaucoma and Memory loss. On memory loss, he recently submitted a topic titled “Application of Machine Learning and Computer Vision to aid Memory Enhancement” to the Pycon Nigeria 2024 conference and will take on this issue with a team of amazing Data professionals.

You can reach out to Aadam via LinkedIn and Research Gate"
https://appinventiv.com/blog/ai-analytics-for-businesses/,Use Cases and Benefits of AI Analytics for Businesses,"The implementation of AI is pervasive across all industries, bringing a shift in how businesses operate and innovate. Its applications range from cost reduction and error prevention to improved customer assistance, efficiency improvement, and routine task automation.

One significant advancement in this AI-driven expansion is the emergence of revolutionary technologies such as generative AI. This new frontier opens up new possibilities for data-driven decision-making in enterprises and expands the application of AI analytics. AI’s trajectory promises innovative solutions to complex problems as it continues to develop, highlighting its critical role in forming the modern business environment.

In this curated blog, we will study the need and importance of AI-powered analytics for businesses, its benefits, use cases, and real-life examples of prominent brands. Let’s take a quick walkthrough.

The Importance of AI Analytics for Businesses

A survey by NVP reflects the strategic changes businesses have made to become data-driven organizations. These changes have included a move from data management to Big Data and, most recently, AI and analytics. Interestingly, 73.7% of companies now employ Chief Data or Analytics Officers, a notable increase from 12.0%, as recorded in 2012. The survey also indicates a noteworthy trend, with 92.1% of companies reporting returns on their data and AI investments in 2024, a significant rise from 48.1% in 2017.

Overall, this report sheds light on how businesses strategically use AI analytics to boost efficiency and innovation. AI analytics is critical to the contemporary business landscape since it revolutionizes decision-making and ensures operational effectiveness.

Businesses can obtain previously unattainable insights by using AI algorithms to examine large datasets, facilitating strategic planning and well-informed decision-making. By forecasting market trends, AI predictive analytics maximizes the use of available resources. AI-powered analytics improve risk assessment and individualized patient care in industries like finance and healthcare.

In addition to streamlining procedures, this revolutionary tool gives organizations the flexibility they need to adapt to a constantly changing marketplace. AI analytics can transform how businesses function, encourage innovation, and provide a competitive advantage in today’s fast-paced business environment.

AI Analytics vs Traditional Data Analytics

AI analytics represents a transformative evolution beyond traditional data analytics methods. While both approaches aim to extract valuable insights from data, they differ significantly in their capabilities and methodologies. Here’s a quick comparison:

The above table highlights the key distinctions between advanced AI analytics and traditional data analytics. For a deeper exploration and understanding of the evolving landscape of data analytics, you can refer to our comprehensive guide on data analytics.

Benefits of AI Analytics for Business

Businesses can reap several benefits from AI analytics as it transforms decision-making processes and boosts overall operational effectiveness. Let’s check those out.

Personalization

AI analytics uses machine learning algorithms to react dynamically to changing customer preferences, going beyond simple analysis. AI analytics helps understand unique user behaviors by continuously extracting information from user interactions, enabling companies to provide hyper-personalized experiences.

The precise understanding obtained from AI data analytics facilitates the development of customized marketing strategies, guaranteeing that every customer engagement is pertinent and captivating. AI’s capacity to forecast consumer preferences solidifies its position as a driver for companies looking to surpass customer expectations in a market that is becoming more and more competitive.

Data-Driven Decision Making

AI analytics solutions can quickly scan large datasets and identify important patterns and connections that conventional statistical methods would miss. Its capacity to recognize complex data patterns enables companies to find untapped possibilities and possible challenges. Furthermore, the iterative nature of AI-powered analytics guarantees steady advancement in decision-making precision.

An agile and responsive business strategy, facilitated by adaptive learning, is crucial in navigating changing market dynamics. Ultimately, AI analytics’ speed, ability to identify patterns, and potential for ongoing improvement make it a vital tool for any organization looking to prosper in the data-driven business environment.

Predictive Analytics

AI’s predictive data analysis extends beyond the mere identification of market trends. It adopts a strategic approach to understand and optimize business operations comprehensively. AI in data analytics not only evaluates possible risks and opportunities but also anticipates changes in consumer behavior and industry trends by utilizing past data patterns.

Predictive analytics empowers businesses to take preventative action and mitigate potential risks. Moreover, AI systems’ flexibility allows them to improve projections, guaranteeing forecast accuracy in changing circumstances continuously. Being proactive improves strategic decision-making, allowing companies to remain ahead of the curve, seize new opportunities, and precisely handle challenges.

Fraud Detection and Security

Artificial intelligence analytics acts as a defender in the financial industry, using sophisticated algorithms to identify complex patterns indicative of fraudulent activity. Because of its real-time monitoring capabilities, transaction irregularities can be quickly identified, providing dynamic protection against ever-evolving cyber threats.

Leveraging AI in analytics can help identify fraud prevention strategies, increasing the resilience of security measures. This strengthens AI’s position as a vital ally in protecting the financial sector from cyber threats and fraudulent schemes.

Also Read: 10 Ways in Which AI is Revolutionizing the Financial Sector

Versatile Data Analysis

Artificial Intelligence in data analytics broadens its scope to include complicated and unstructured datasets. This flexibility allows businesses to extract insights from structured databases and sources like text, photos, and multimedia.

AI for data analytics offers a more comprehensive insight into operations, markets, and customer behavior by supporting various data formats. This adaptability is especially helpful in sectors where data is available in various formats, allowing for a more thorough and intricate approach to strategic planning and decision-making.

Equipment Fault Detection

Businesses are utilizing artificial intelligence’s capabilities to scan large datasets and identify trends for defect detection and predictive maintenance. By utilizing AI in data analytics, it becomes possible to predict machinery faults or maintenance needs by closely examining equipment sensors and previous data.

By utilizing AI for fault detection, businesses can seamlessly schedule maintenance, which minimizes costly downtime and maximizes operational efficiency. Predictive maintenance using AI improves equipment reliability and serves as a strategic asset for numerous sectors, guaranteeing efficient operations and economical maintenance procedures.

Also Read- How is AI in Business Bringing Transformation?

AI Analytics in Business – Use Cases and Real-Life Examples

AI analytics’s numerous uses are revolutionizing businesses in various industries. These use cases demonstrate the critical role of AI and data analytics in transforming processes and decisions in several business sectors. Let’s explore some real-life examples of AI analytics for businesses:

1. Netflix’s AI-Based Customer Segmentation Strategy

Customer segments can be created using AI data according to their demographics, preferences, and behavior. By using segmentation, organizations can improve client satisfaction and retention by providing distinct customer groups with individualized experiences, focused marketing efforts, and product suggestions.

For instance, Netflix divides its user base into various groups using AI analytics to examine user behavior, including viewing history and preferences. This boosts customer satisfaction and engagement by enabling them to offer tailored suggestions for movies and web series.

2. PayPal’s Security Enhancements Measures

Real-time AI analytics in finance can identify patterns of fraudulent activities. Businesses can lessen financial losses and improve the security of financial systems by promptly identifying and stopping fraudulent transactions. This can be done by closely monitoring and comparing them to pre-established patterns.

PayPal, for instance, uses artificial intelligence to recognize and stop fraudulent transactions instantly. PayPal’s AI system examines user behavior, transaction patterns, and other pertinent data through advanced algorithms and machine learning models to promptly identify anomalies or suspicious activity.

It helps PayPal reduce the likelihood of financial fraud with users and the platform by taking prompt action, such as stopping or flagging transactions. In addition to improving the security of financial transactions, the real-time aspect of this AI-driven fraud detection helps retain users’ confidence and trust in PayPal’s services.

At Appinventiv, we worked with Bajaj Finserv, a leading Indian FinTech enterprise, helping them solve their security challenges. We utilized advanced security methods to prevent and predict financial fraud. This helped them increase their merchant onboarding to 300+, ensuring 3 lacs+ transactions per day.

3. Salesforce’s Sales Projection Analysis with AI

Predictive analytics forecasts future sales by utilizing market trends, customer behavior, and past data. This aids companies in resource allocation, marketing strategy planning, and inventory optimization. It lessens the effects of erratic market conditions and helps in quick decision-making.

Salesforce, for instance, uses artificial intelligence-driven data analytics to prioritize leads for sales teams and forecast sales trends. Sales forecasting, pricing optimization, and potential upsell chances are all aided by Salesforce’s Einstein Analytics, which analyzes past data, current market conditions, and customer behavior.

4. Walmart’s AI-Optimized Supply Chain Optimization

AI analytics for businesses helps organizations analyze enormous volumes of data involving logistics, demand, and inventory levels to optimize the supply chain. By using data for a better decision-making process, businesses can optimize transportation routes, change production schedules, and save money while increasing overall efficiency.

For instance, Walmart utilizes the power of AI insights to optimize its supply chain. Through meticulous analysis of inventory levels, product demand, and transportation logistics, Walmart increases operational efficiency and cuts costs. Also, it helps Walmart ensure continuous stocking of products in stores, guaranteeing a streamlined and customer-centric shopping experience.

At Appinventiv, we helped a global manufacturing company enhance its operational efficiency by deploying an efficient supply chain management solution. This resulted in a 60% increase in supply chain visibility, a 30% rise in operational efficiency, and a 40% reduction in transportation costs.

5. Amazon’s Utilization of Chatbots for Customer Data Analysis

Enterprises widely use AI chatbots for customer support services, offering real-time, and round-the-clock client service. In addition to assisting users with troubleshooting procedures and, if necessary, elevating complex issues to human agents, they can respond to commonly asked inquiries through their AI analytics capabilities. This increases the effectiveness of customer care, speeds up response times, and improves the general customer experience.

Amazon, one of the biggest eCommerce technologies, has effectively incorporated AI chatbots into its customer support services, which has also helped them overcome manual data analysis challenges. AI chatbots can analyze data faster than humans, which ensures that customers always get timely assistance. Leveraging AI analytics capabilities, chatbots can analyze consumer behavior and offer users personalized recommendations.

6. IBM’s AI-Based Healthcare Diagnostics

Medical experts can diagnose illnesses and offer personalized treatment options using AI analytics in healthcare. AI systems can offer insightful analysis of a large volume of clinical trial data, patient information, and medical journals. This analysis helps healthcare providers provide more precise and timely care.

For instance, IBM Watson uses AI analytics for Oncology to help oncologists diagnose and treat cancer. Based on the unique illness trait of each patient, Watson can prescribe a specific course of treatment by reviewing clinical trial data, medical literature, and patient information.

Appinventiv has successfully implemented AI in healthcare with projects like YouCOMM, an in-hospital patient communication system. With YouCOMM, patients can communicate in real-time with nurses and other hospital staff. The platform has a user-friendly interface that incorporates voice commands and other manual alternatives like head movements.

YouCOMM has significantly improved the efficiency of patient-staff contact and allowed hospital administration to maintain thorough records of the timelines with which patients’ requests are completed. The effectiveness of this solution led to its adoption by over 5 hospital chains in the US, resulting in a significant 60% increase in nurses’ real-time response rates.

7. Delta Airlines’ Dynamic Pricing Strategy

Dynamic pricing algorithms analyze competitor pricing, market conditions, and customer behavior to adjust prices in real time. This approach increases revenue by capitalizing on demand variations, providing discounts when demand is low, and optimizing pricing strategies according to various criteria.

Delta Airlines utilizes AI and data analytics to structure its dynamic pricing strategy. The airline can modify ticket rates in real-time to optimize revenue and fill available seats by evaluating the demand, time to departure, and competitor pricing.

8. Hootsuite’s Social Media Analytics Approach

Businesses can better understand their online presence by using AI analytics in social media. Sentiment analysis, for example, determines public opinion on a particular product or brand. These insights help organizations improve engagement, hone their social media strategies, and effectively address client feedback.

For example, Hootsuite uses AI analytics to examine user engagement and social media trends. The platform analyzes sentiment, audience behavior, and the success of social media campaigns using machine learning algorithms.

Also Read: How to Create a Social Media App in 2023 – A Comprehensive Guide

9. Unilever’s HR Data Analysis and Recruitment with AI

AI-powered HR analytics automates applicant matching and resume screening, which expedites the hiring process. This frees HR personnel to concentrate on more strategic hiring tasks like conducting in-depth interviews and determining cultural fit. AI analytics can also aid staff retention by pinpointing the elements influencing job satisfaction.

For example, Unilever uses AI analytics in its hiring process. The business uses AI algorithms to evaluate resumes and find suitable applicants based on their credentials and skill sets. This facilitates the hiring process and helps identify candidates who are best fit for open positions.

10. Uber’s Reliability or Uptime Maximization Through AI

Maintaining uninterrupted operations is more important in a fast-paced world of digital services. Downtime or interruptions can seriously affect organizations trying to fulfill customers’ ever-increasing needs. These consequences can affect customer satisfaction, trust, and the entire company’s reputation.

Uber, a leader in the ride-sharing space, depends on AI to guarantee maximum reliability and uptime. AI systems continuously monitor and analyze enormous databases, seeing possible dangers, inefficiencies, and disruptions in real-time. Uber’s AI-driven method improves platform reliability by proactively addressing issues. In addition to preventing service disruptions, this also lessens the possibility of accidents, hacking, and human error.

Uber’s use of AI in its operations is a prime example of the revolutionary effect on uptime and reliability. It demonstrates how this state-of-the-art technology can improve user experiences and maintain the smooth running of services in the constantly changing digital landscape.

Also Read- AI in Business: A Comprehensive Integration Guide

Navigating Challenges and Considerations in Implementing AI Analytics for Businesses

Implementing artificial intelligence-driven data analytics comes with several complex challenges that can be tackled with proper considerations or solutions. Let’s check those out:

Data Privacy and Security

As data analytics and AI become increasingly significant, concerns about sensitive data security and privacy are growing. The use of enormous datasets presents certain concerns as firms incorporate AI into their operations, such as abuse, unwanted access, and data breaches. The challenge lies in protecting this data from ever-changing cyber threats. Resolving these issues is critical to building trust with stakeholders and users, requiring strong steps to reinforce data privacy and security systems.

Solution: Adopt anonymity strategies, put strong encryption techniques into place, and follow stringent compliance guidelines like GDPR. Frequent evaluations and audits can guarantee data security, and open privacy practices foster user trust.

Integration Complexities

Many intricate components are involved in integrating AI analytics into well-established business processes. Integration challenges arise because firms’ current infrastructures frequently run on multiple platforms and technologies. Compatibility issues, data discrepancies, and disruptions might result from this complexity. These issues must be resolved to fully utilize AI analytics for businesses without interfering with regular operations and to guarantee a smooth and harmonious integration into the organizational structure.

Solution: The best course of action is to invest in interoperability standards, plan for a phased integration strategy, and thoroughly evaluate the current infrastructure. Hiring an experienced software development team ensures a smoother integration process, guaranteeing efficiency in your project.

Skill Gaps and Training

The lack of qualified experts in creating, deploying, and overseeing AI analytics solutions prevents this game-changing technology from being widely used. There is a critical shortage of specialized AI talents since demand exceeds the available talent pool. This challenge affects people in various fields, including data scientists and AI developers.

Solution: Invest in extensive training programs to help current employees become more proficient with AI. Establish partnerships with educational institutions to develop a talent pool for artificial intelligence. Additionally, consider working with AI providers or utilizing managed IT services to compensate for talent gaps and guarantee businesses’ successful adoption of AI analytics.

Ethical Usage of AI

Handling the intricacies of bias, transparency, and potential misuse is necessary to ensure the ethical deployment of AI. Creating algorithms and models that are ethically sound and devoid of biases is the challenging part.

Solution: Ensure AI development teams conduct regular audits of algorithms for bias and foster continuous discussions on ethical AI practices inside the company and across the industry.

Embrace AI-Powered Analytics for Your Business with Appinventiv

AI analytics is no longer an option but rather a necessary tool for accelerating growth and reshaping how businesses operate. AI analytics for businesses is changing industries and operational paradigms in various areas, including manufacturing, retail, healthcare, finance, and marketing. Leveraging AI, businesses can better make strategic decisions, predict future trends, uncover hidden insights, automate complex data analysis, and analyze various data sources.

Unlock the power of sophisticated analytics, improve decision-making, and grow your business skills with Appinventiv’s AI analytics services. Our customized AI solutions are made to push your business toward steady expansion and guarantee a forward-thinking strategy for realizing the full potential of AI in achieving success.

Whether it’s predicting market trends, personalizing customer experiences, or enhancing operational efficiency, our artificial intelligence development services are crafted to meet the unique needs of your business.

Explore the transformative realm of AI analytics for businesses by connecting with our experts today.

FAQs

Q. How is AI analytics used for businesses?

A. Businesses across different sectors utilize AI analytics to enhance decision-making processes, extract valuable insights, and optimize operations. The use of AI analytics for businesses is reflected in these examples:

Data processing and analysis

Quality control

Supply chain optimization

Employee productivity

Predictive analytics

Personalization

Fraud detection

Competitive analysis

Risk management

Q. What is the future of AI analytics for businesses?

A. The future of AI-powered analytics for business is expected to continue to advance toward more complex and personalized insights. Businesses can expect enhanced predictive capabilities as AI technologies develop, offering greater insights into consumer behavior, market trends, and possible hazards. The widespread adoption of automation and increased efficiency will permeate various sectors, integrating AI insights into daily decision-making processes.

AI’s influence will increase when combined with other cutting-edge technologies like edge computing and the Internet of Things (IoT). The increasing prominence of ethical considerations and responsible AI techniques will ensure the appropriate and transparent use of data. Overall, AI analytics will become an invaluable asset for businesses in the coming years, fostering innovation and industry competitiveness.

Q. What is the role of AI analytics in a business?

A. AI analytics offers a transformational impact on the decision-making processes and other pivotal operations of businesses. By leveraging AI algorithms, businesses can attract valuable insights from huge datasets that help them enhance overall efficiency.

AI-powered analytics for business help improve the resource allocation processes, facilitate personalized customer experiences, and reduce costs by identifying the inefficiencies within the organization. It also helps businesses remain agile and responsive in the ever-changing market dynamics. Overall, it helps organizations in navigating complex business challenges."
https://www.msn.com/en-xl/news/other/how-to-become-a-data-scientist-in-2024/ar-BB1lvunq,MSN,
https://blogs.oracle.com/ai-and-datascience/post/serving-llm-using-huggingface-and-kubernetes-oci,Serving LLMs using HuggingFace and Kubernetes on OCI,"Large language models (LLMs) have made significant strides in text generation, problem-solving, and following instructions. As businesses integrate LLMs to develop cutting-edge solutions, the need for scalable, secure, and efficient deployment platforms becomes increasingly imperative. Kubernetes has risen as the preferred option for its scalability, flexibility, portability, and resilience.

In this blog post, we demonstrate how to deploy fine-tuned LLM inference containers on Oracle Container Engine for Kubernetes (OKE), an Oracle Cloud Infrastructure (OCI)-managed Kubernetes service that simplifies deployments and operations at scale for enterprises. This service enables them to retain the custom model and datasets within their own tenancy without relying on a third-party inference API.

HuggingFace text generation inference

Text generation inference (TGI) is an open source toolkit available in containers for serving popular LLMs. The example fine-tuned model in this post is based on Llama 2, but you can use TGI to deploy other open source LLMs, including Mistral, Falcon, BLOOM, and GPT-NeoX. TGI enables high-performance text generation with various optimization features supported on multiple AI accelerators, including NVIDIA GPUs with CUDA 12.2+.

GPU memory consideration

The GPU memory requirement is largely determined by pretrained LLM size. For example, Llama 2 7B (7 billion parameters) loaded in 16-bit precision requires 7 billion * 2 bytes = 14 GB for the model weights. Quantization is a technique used to reduce model size and improve inferencing performance by decreasing precision without significantly sacrificing accuracy. In this example, we use the quantization feature of TGI to load a fine-tuned model based on Llama 2 13B in 8-bit precision and fit it on VM.GPU.A10.1 (single NVIDIA A10 Tensor Core GPU with 24-GB VRAM). The following image depicts the real memory utilization after the inference container loads the quantized model. Alternatively, consider employing a smaller model, opting for a GPU instance with larger memory capacity, or selecting an instance with multiple GPUs, such as VM.GPU.A10.2 (2x NVIDIA A10 GPUs), to prevent the CUDA out-of-memory error. By default, TGI shards across and uses all available GPUs to run the model.

Model loading

TGI supports loading models from HuggingFace model hub or locally. To retrieve a custom LLM from the OCI Object Storage service, we created a Python script using the OCI Python software developer SDK, packaged it as a container, and stored the Docker image on the OCI Container Registry. This model-downloader container runs before the initialization of TGI containers. It retrieves the model files from Object Storage and stores them on the emptyDir volumes, enabling sharing with TGI containers within the same pod.

Deploying the LLM container on OKE

(optional) Take one of the pretrained LLMs from HuggingFace model hub, such as Meta Llama2 13B, and fine-tune it with a targeted dataset on an OCI NVIDIA GPU Compute instance.

Save the customized LLM locally and upload it to OCI Object Storage as a model repository.

Deploy an OKE cluster and create a node pool consisting of an A10.1 virtual machine (VM) Compute instance powered by NVIDIA A10 Tensor Core GPUs (or any other Compute instance you want). OKE offers worker node images with preinstalled NVIDIA GPU drivers.

Install NVIDIA device plugin for Kubernetes, a DaemonSet that allows you to run GPU enabled containers in the Kubernetes cluster.

Build a Docker image for the model-downloader container to pull model files from Object Storage service. (The previous session provides more details.)

Create a Kubernetes deployment to roll out the TGI containers and model-downloader container. To schedule the TGI container on GPU, specify the resources limit using “nvidia.com/gpu.” Run model-downloader as Init Container to ensure that TGI container only starts after the successful completion of model downloads.

Create a Kubernetes service of type “Loadbalancer.” OKE automatically spawns an OCI load balancer to expose the TGI application API externally.

To interact with the model, you can use curl by sending a request to <Load Balancer IP address>:<port>/generate, or deploy an inference client, such as Gradio, to observe your custom LLM in action.

Conclusion

Deploying a production-ready LLM becomes straightforward when using the HuggingFace TGI container and OKE. This approach allows you to harness the benefits of Kubernetes without the complexities of deploying and managing a Kubernetes cluster. The customized LLMs are fine-tuned and hosted within your Oracle Cloud Infrastructure tenancy, offering complete control over data privacy and model security.

For more information, see the following resources:"
https://www.analyticsinsight.net/ibm-vs-meta-where-will-your-data-science-future-thrive/,IBM vs Meta: Where Will Your Data Science Future Thrive?,"IBM vs Meta: Navigating Your Data Science Career Path In the Year 2024

In the rapidly evolving landscape of data science, choosing the right employer can significantly impact your career trajectory and opportunities for growth. Two prominent players in the tech industry, IBM and Meta (formerly Facebook), offer distinct environments and opportunities for data scientists. In this article, we’ll explore the key differences between IBM and Meta and where your data science future may thrive.

IBM: A Legacy of Innovation

With a history spanning over a century, IBM is a global leader in technology and innovation. Known for its pioneering work in computing, cloud services, and artificial intelligence, IBM offers data scientists a diverse range of projects and opportunities. Key aspects of IBM’s data science ecosystem include:

Enterprise Solutions: IBM provides data scientists with opportunities to work on large-scale enterprise projects, leveraging cutting-edge technologies such as IBM Watson for AI-powered insights.

Research and Development: IBM’s commitment to research and development fosters an environment of innovation, with opportunities for data scientists to contribute to groundbreaking projects in areas such as quantum computing and blockchain.

Industry Partnerships: IBM collaborates with industry partners across various sectors, providing data scientists with exposure to real-world challenges and opportunities to apply their skills in diverse domains.

Meta: A Tech Powerhouse

As the parent company of social media giant Facebook, Meta is a dominant force in the tech industry, with a strong emphasis on data-driven decision-making and innovation. Data scientists at Meta can expect:

Massive Data Scale: Meta operates one of the largest data infrastructures in the world, providing data scientists with access to vast amounts of user-generated data for analysis and insights.

Cutting-Edge Research: Meta invests heavily in research and development, particularly in areas such as machine learning, natural language processing, and computer vision, offering data scientists the opportunity to work on cutting-edge projects.

Impactful Products: Data scientists at Meta have the opportunity to contribute to the development of products and features that impact billions of users worldwide, shaping the future of social media and technology.

Choosing Your Data Science Future

When considering where your data science future may thrive, it’s essential to weigh the factors that align with your career goals and aspirations:

Career Development: Evaluate the opportunities for career advancement, mentorship, and professional growth offered by each company.

Company Culture: Consider the company culture, values, and work environment to ensure alignment with your personal preferences and values.

Impact and Purpose: Reflect on the impact and purpose of the work you’ll be doing, whether it’s driving innovation, solving complex problems, or making a positive impact on society.

Conclusion

Both IBM and Meta offer exciting opportunities for data scientists to make significant contributions to the technology landscape. Whether you’re drawn to IBM’s legacy of innovation and enterprise solutions or Meta’s scale and impact in the tech industry, the choice ultimately depends on your career aspirations and goals. By carefully evaluating the factors that matter most to you, you can make an informed decision about where your data science future will thrive."
https://www.psu.edu/news/clinical-and-translational-science-institute/story/infrastructure-consultation-multi-site-studies/,"Infrastructure, consultation for multi-site studies using electronic health data","UNIVERSITY PARK, Pa. — Advanced machine learning methods can now help predict and understand health risks and outcomes. These methods use large sets of clinical data, including electronic health records, socio-demographic data and medical imaging. Until recently, Penn State researchers had limited access to big biomedical and health research data, such as electronic health records (EHR). This has now changed with the establishment of the Penn State Digital Collaboratory for Precision Health Research (DCPHR), an initiative led by Penn State Clinical and Translational Sciences Institute (CTSI) and the Penn State Center for Artificial Intelligence Foundations and Scientific Applications (CENSAI).

DCPHR offers the infrastructure and research capacity to allow Penn State researchers to pursue collaborative data-intensive research projects using large clinical data sets and high-performance Artificial Intelligence/Machine Learning (AI/ML) data analytic workflows.

“DCPHR aims to significantly lower the barriers to collaboration between clinical and translational scientists at Penn State College of Medicine, and data scientists and AI/ML experts at other Penn State campuses, for pursuing data-intensive, AI/ML-powered biomedical and health research,” said Vasant Honavar, Dorothy Foehr Huck and J. Lloyd Huck Chair in Biomedical Data Sciences and Artificial Intelligence, director of CENSAI, and Penn State CTSI Informatics Core co-lead.

The DCPHR currently provides access to de-identified Penn State Health EHR data organized according to the Observational Medical Outcomes Partnership (OMOP) common data model (CDM) developed by the NIH-funded Observational Health Data Sciences Initiative (OHDSI) (pronounced “odyssey”) consortium. The OMOP common data model powers many large-scale biomedical data science efforts such as the NIH’s National COVID Cohort Collaborative (N3C).

OHDSI access is the newest addition to the DCPHR initiated by the CTSI Informatics Core. OHDSI aims “to improve health by empowering a community to collaboratively generate the evidence that promotes better health decisions and better care.” OHDSI links over 3000 collaborators across approximately 80 countries with OMOP-based EHR data repositories that collectively contain over 900 million unique patient records (representing about 12% of the world’s population). Each member of OHDSI maintains EHR data for its patient population in an OMOP-based institutional EHR data repository, like the one maintained by DCPHR at Penn State.

Any institution that is a member of the OHDSI consortium can propose a multi-site study with a defined study protocol and solicit participation from collaborators across the OHDSI consortium.

Each site that joins the study executes the study protocol on its data and the results of analyses are pooled to answer the research questions targeted by the study. There is no need for participating institutions to share EHR data with other sites, which significantly lowers the barriers to multi-institutional collaborations.

“DCPHR allows Penn State researchers to be part of large multi-site studies in ways that were not previously possible,” said Avnish Katoch, research informatics project manager with Penn State CTSI.

Now, Penn State researchers can perform multi-site studies in collaboration with OHDSI. Penn State researchers interested in accessing the OHDSI community or developing proposals for multi-site studies should request a free informatics consultation.

In addition, CTSI’s Informatics Core can assist with study design, including use of AI/ML. The Informatics Core empowers researchers in several ways, including the following:

help with study design and feasibility analysis;

help with cohort definition and data extraction;

support for data preparation;

support for analysis of large data sets (characterization, prediction, effect estimation);

support for model interpretation, interrogation, deployment, inference; and

AI/ML support for research proposal development

As a proof of concept of the OHDSI community, Penn State participated in Project HERA - Health Equity Research Assessment in which investigators looked to characterize health and healthcare disparities across different groups, outcomes and databases/countries. Investigators used HERA to ask: Are there systematic patterns of diagnosis coding prevalence for Black and white patients across a network of observational health datasets and across all diagnoses? A publication of this study is currently underway.

“During the past year, the Informatics team completed testing the Penn State instance of OMOP-based EHR data repository, set up processes for its periodic refresh, assessed data for quality and completeness, and identified steps to improve data quality. The data repository recently transitioned from the test environment to the production environment, allowing us to open it up for use by the larger biomedical data sciences and clinical research communities at Penn State,” said Honavar. “The next milestone for DCPHR is to support the integration of EHR data with other data sets or individual level socio-demographic data, deidentification of the integrated data, and provision of AI/ML workflows for analyses of multi-modal health data,” he added.

Other informatics data information

Penn State CTSI informatics core provides access to Electronic Health Records (EHR) data from TrinetX, which includes 80+ institutional partners of the TrinetX research network. The TrinetX platform supports basic statistical analyses. Trinetx is better suited for preliminary analyses of large EHR datasets. Basic statistical characterization of TrinetX EHR data can be carried out using this platform whereas more extensive analyses, e.g., using machine learning, require retrieving the relevant data and running it through AI/ML pipelines (often with assistance from the CTSI Informatics Core’s data science team).

The CTSI Informatics Core

Artificial intelligence and machine learning are necessary for researchers who are interacting with large data sets. However, it can be challenging to understand how to best access and interface with these giant databases. Many research groups at Penn State are working through the CTSI Informatics Core to leverage data science methods to advance their work.

For more information on how the CTSI Informatics Core works, watch this replay of “Harnessing the Power of EHR Data and IA to Advance Biomedical Research,” which includes how and why current research groups have applied artificial intelligence to their research, and offers examples of how the computational consulting team can support Penn State researchers' data science projects."
https://datascientest.com/en/google-colab-the-power-of-the-cloud-for-machine-learning,Google Colab: the power of the cloud for machine learning,"Google Colab, short for Google Colaboratory, is a platform offered free of charge by Google that lets you write and run python code in your browser. In particular, it lets you run Jupyter notebooks without having to worry about your hardware or the software installed on your computer.

Google Colab is a tool that also facilitates access to computing resources and common machine learning libraries.

Hosting in the cloud

A key feature of Google Colab is that it is hosted in the cloud. This means that there is no need to install Python or other libraries on your computer. Everything happens directly in a web browser. All you have to do is sign in to your Google Account and you’re ready to go.

Pre-installation of numerous libraries

Google Colab comes with many Python libraries pre-installed. This includes libraries commonly used for data science such as NumPy, Pandas, Scikit-learn, TensorFlow and PyTorch, as well as visualisation libraries such as Matplotlib, Seaborn and Plotly, making it easy to create graphs, charts and visualisations to explore and present data. You don’t need to worry about installing these libraries, which greatly simplifies the configuration of your environment.

Google Colab allows you to run system commands directly from a notebook. So if you need specific libraries that aren’t pre-installed, you can install them directly from a notebook using the pip command. This allows you to extend the functionality of your environment.

Access to computing resources

Google Colab offers free access to graphics processing units (GPUs) and tensor processing units (TPUs), which are extremely useful for computationally intensive tasks such as deep learning models. You can activate these hardware accelerations with just a few clicks. This speeds up the model training process, reducing the time needed to obtain results.

Storage on Google Drive

Notebooks created in Google Colab are automatically backed up to Google Drive at regular intervals, so you can easily store and share them with other developers without having to worry about losing your work in the event of a technical problem.

Importing and exporting notebooks

You can import existing notebooks into Google Colab or export your own notebooks in ipynb format. This makes it easy to share projects between Google Colab and other development environments such as Jupyter Notebook.

Real-time collaboration

Google Colab supports real-time collaboration. You can share a notebook with other users, who can then access it, see changes in real time and add their own comments. This makes it an ideal tool for teamwork on development projects.

Git integration

You can use Github directly in Google Colab. This enables code version tracking, source code management and collaboration between developers.

Access to external data

Google Colab provides easy access to external data. You can import datasets from your computer or from URLs, making it easier to access the data you need for your projects, including accessing files stored on your Google Drive.

Variety of execution modes

Google Colab notebooks offer several execution modes. You can run the code for a single cell, a selection of cells, in a shell session, or run the whole notebook at once. This flexibility means you can adapt the execution mode to suit your needs."
https://www.analyticsinsight.net/top-10-youtube-channels-for-data-science-updates/,Top 10 YouTube Channels for Data Science Updates,"Explore the curated list of 10 YouTube Channels for data science updates

Since the subject of data science is always changing, both experts and hobbyists need to remain current on the newest methods, resources, and trends. With so many channels that meet the educational needs of data scientists of all skill levels, YouTube has evolved into a veritable gold mine of information. The top 10 YouTube channels for data science updates in 2024 that are essential viewing for everyone interested in data science will be discussed in this post.

1. 3 Blue1Brown

3 Blue1Brown is a channel that excels in providing high-quality content, especially when it comes to explaining complex math concepts through engaging animations. This channel has more than 5.62M subscribers and is a popular choice among those who want to learn more about the mathematics behind the algorithms of data science and data science updates.

2. Joma Tech

Joma Tech, who used to be a data scientist at some of the biggest tech companies such as Facebook, Microsoft, and Google, brings a unique combination of humor and knowledge to the channel. With over 2.27M subscribers, he provides insights into the field of data science as well as practical career tips.

3. Ken Jee

Ken Jee is a channel that is a gold mine for people who want to learn about data science in practical ways. His clear, concise teaching style combined with his hands-on data science approach make his content very valuable to learners.

4. Krish Naik

Project-Based Learning

Project-based learning is Krishna Naik’s channel and is a great resource for anyone who wants to learn about applied learning. It covers the entire life cycle of a data science project.

5. Simplifyarn

Simplifyarn offers a wide range of educational content in the fields of data science, machine learning, and more. With over 3.58M subscribers, Simplifyarn is the most popular structured learning channel.

6. FreeCodeCamp

With over 8.73M subscribers, FreeCodeCamp is among the largest learning channels for coding and learning data science. The channel offers comprehensive tutorials for beginners and advanced learners.

7. Siraj

Siraj is an AI and Machine Learning channel. He teaches complex topics in an easy-to-understand way.

8. CodeBasics

With over 901M subscribers, CodeBasics is well-known for its easy-to-follow tutorials on the basics of Data Science and Programming.

9. Data Professor

Data Professor is a channel that provides concise and interesting content in the field of Data Science and Bioinformatics.

10. AnalyticsVidhya

With 49.8M subscribers, AnalyticsVidhya provides a wide range of data science content in a variety of formats."
https://www.analyticsinsight.net/can-data-science-jobs-stay-in-demand-for-the-next-5-years/,Can Data Science Jobs Stay in Demand for The Next 5 Years?,"Explore these factors that keep data science jobs in demand

Data science has been among the fields of study which are blossomed very fast in recent years and the major contributing factors, to this include the digitization of business processes as well as the growing sense of data-driven decision-making. Whereas advanced technology boosts its popularity and new cycles come to the line, some have questions concerning when data scientist jobs become obsolete for the next five years. In this article, we discuss if the data science jobs stay ahead in the next 5 years.

Growing Data Complexity

Considering the rise of digital devices and IoT (the Internet of Things) the complexity of data generated will continue to rise exponentially. With the intensified data complexity faced by organizations, there are both challenges and chances inherent in the process that eventually led to the state wherein data scientists are needed to extract these actionable insights from a set of large and diverse volumes of data.

Expanding Use Cases

Data science is no longer just a limited field of work but has emerged to be a key practice of commerce in almost every industry including healthcare, finance, retail, etc. As companies realize the power of evidence-based decisions, the demand for data science expertise is projected to go beyond just industry to brands, thereby stirring the growth of data science roles even further.

Let us mention that one of the major factors is the progress in the sphere of Psychological Warfare and Deep Fakes. AI and ML are being used the most in the data science field all over the world and thus enterprises can automate processes, strengthen forecasting, and get so many ways of understanding the data. As the market grows and more companies introduce AI and ML applications into their businesses, the main need will be for trained data scientists and analysts working on these problems to remain current.

Shortage of Skilled Talent

Regardless of the increasing number of data science positions that are expected to be employed, there is still a lack of enough trained talent in the field. According to the various sector reports, the existing gap between the data science industry professionals and the demand is massive, with employers being unable to recruit candidates with the right experience and skills. Data science professionals are projected to be in high demand coming years as this skill gap is anticipated to extend into the foreseeable future.

Emerging Technologies and Trends

Nonetheless, the requirement of data science jobs will undoubtedly ensue just as the technology and skills needed to perform the roles are set to advance or change. Technologies like cloud computing, big data analytics, and blockchain are going to rise and they may reshape the skills and the way experts approach data science in the future. Data scientists who have competencies in an array of the latest technologies, and know the current trends, will fill the jobs that already exist and the new positions that will come as a result of constant changes in the job market.

Conclusion

The high demand for data science jobs in the next five years and years beyond will remain strong. Organizations that use data to invent and become leaders in the market, will become more and more reliant on data scientists who can get insights from big and complicated data sets. Nevertheless, the technological world is moving very fast, thus, staying relevant in data science will require continuous learning and embracing new technologies and trends."
https://www.business-standard.com/companies/interviews/we-have-a-team-that-innovates-with-data-science-and-ai-dtici-md-ceo-124041600741_1.html,We have a team that innovates with data science and AI: DTICI MD & CEO,
https://www.analyticsinsight.net/10-countries-paying-highest-salaries-to-data-science-professionals/,10 Countries Paying Highest Salaries to Data Science Professionals,"The Top 10 Countries Offering the Highest Salaries to Data Science Professionals

Data science has emerged as one of the most sought-after fields in the modern workforce, with professionals skilled in extracting insights from data in high demand across industries. As organizations worldwide recognize the value of data-driven decision-making, the demand for data science talent continues to soar, leading to competitive salaries and lucrative opportunities for skilled professionals. This article explores the top 10 countries paying the highest salaries to data science professionals, examining the factors driving salary trends and the opportunities available in each location.

United States:

The United States is a global leader in data science, with a thriving ecosystem of technology companies, research institutions, and startups driving innovation in the field. Silicon Valley, in particular, is known for offering some of the highest salaries to data science professionals, thanks to the presence of leading tech giants such as Google, Facebook, and Amazon. Data scientists in the U.S. can expect to earn six-figure salaries on average, with opportunities for lucrative bonuses and stock options in addition to base pay.

Switzerland:

Switzerland is renowned for its high quality of life and strong economy, making it an attractive destination for data science professionals seeking competitive salaries and career opportunities. With a robust financial sector, pharmaceutical industry, and technology hub in cities like Zurich and Geneva, Switzerland offers lucrative salaries to data scientists, reflecting the country’s commitment to innovation and technology-driven growth.

Australia:

Australia has emerged as a hotspot for data science talent, with a growing demand for skilled professionals across industries such as finance, healthcare, and e-commerce. Cities like Sydney and Melbourne offer competitive salaries to data scientists, with opportunities for career advancement and a favorable work-life balance. The Australian government’s initiatives to promote innovation and digital transformation further contribute to the country’s appeal as a destination for data science professionals.

Germany:

Germany boasts a strong industrial base and a thriving startup ecosystem, creating a wealth of opportunities for data science professionals. Cities like Berlin, Munich, and Frankfurt are hubs of innovation, with companies in sectors such as automotive, manufacturing, and finance investing in data analytics and AI-driven technologies. Data scientists in Germany can command high salaries, particularly in sectors undergoing digital transformation and disruption.

Canada:

Canada is home to a vibrant tech scene, with cities like Toronto, Vancouver, and Montreal attracting talent from around the world. The country’s progressive immigration policies and diverse workforce make it an attractive destination for data science professionals seeking new opportunities. Salaries for data scientists in Canada are competitive, with opportunities for career growth and a high quality of life.

Netherlands:

The Netherlands has emerged as a hub for data science and artificial intelligence, with companies investing in advanced analytics and machine learning to drive innovation and competitiveness. Cities like Amsterdam and Eindhoven offer attractive salaries to data scientists, with opportunities to work in cutting-edge industries such as finance, healthcare, and logistics. The Netherlands’ favorable business environment and strong support for research and development contribute to its appeal as a destination for data science professionals.

United Kingdom:

Despite uncertainties surrounding Brexit, the United Kingdom remains a global leader in data science and technology, with a thriving ecosystem of startups, research institutions, and multinational corporations. Cities like London, Cambridge, and Manchester offer competitive salaries to data scientists, with opportunities to work in diverse industries such as finance, healthcare, and gaming. The UK’s strong emphasis on innovation and entrepreneurship continues to drive demand for data science talent.

Singapore:

Singapore has positioned itself as a regional hub for technology and innovation, attracting talent from around the world with its dynamic business environment and high quality of life. The city-state’s government has made significant investments in building a digital economy, creating opportunities for data scientists to work on cutting-edge projects in areas such as smart cities, fintech, and healthcare. Salaries for data scientists in Singapore are competitive, reflecting the country’s status as a global financial and technology hub.

Sweden:

Sweden is known for its progressive policies, innovative companies, and high standard of living, making it an attractive destination for data science professionals. Cities like Stockholm and Gothenburg are home to a thriving tech scene, with opportunities to work in sectors such as gaming, e-commerce, and clean energy. Salaries for data scientists in Sweden are competitive, with opportunities for career advancement and a strong focus on work-life balance.

France:

France has emerged as a leader in artificial intelligence and data science, with companies investing in research and development to drive innovation and competitiveness. Cities like Paris, Lyon, and Toulouse offer attractive salaries to data scientists, with opportunities to work in diverse industries such as aerospace, automotive, and healthcare. The French government’s initiatives to promote innovation and digital transformation further contribute to the country’s appeal as a destination for data science professionals.

In conclusion, data science professionals are in high demand worldwide, with companies across industries seeking skilled talent to drive innovation and growth. The top 10 countries offering the highest salaries to data science professionals provide attractive opportunities for career advancement, competitive compensation, and a high quality of life. Whether in established tech hubs like the United States and Switzerland or emerging destinations like Australia and Canada, data scientists can find rewarding opportunities to make an impact and thrive in their careers. As the demand for data science talent continues to grow, these countries are likely to remain at the forefront of innovation and technology-driven growth in the years to come."
https://www.analyticsinsight.net/how-data-science-transforms-managers-into-leaders-of-success/,How Data Science Transforms Managers into Leaders of Success,"Transforming Managers into Successful Leaders with Data Science

In recent times, the domain of data science has overhauled the cogitation, conduct, and operation of managers in the corporate realm. By unleashing the potential of extensive data, managers can amass insights, strategic acumen, and the capability to resolve predicaments in novel and inventive manners. Let’s delve into how data science can metamorphose managers and aid them in spearheading organizations towards success. Let’s delve into how data science can metamorphose managers and aid them in spearheading organizations towards success.

Enlightened Verdicts

Data science equips managers with potent tools and methodologies to formulate verdicts grounded in data. Through scrutinizing vast volumes of data, managers can pinpoint patterns, trends, and correlations that may elude other approaches. Whether streamlining logistical processes, foreshadowing market demand, or grasping customer inclinations, data-driven verdict-making empowers managers to align strategies with business objectives and yield quantifiable outcomes.

Tactical Blueprinting

Tactical planning serves as the linchpin for institutional expansion and competitive edge. Data science enables managers to craft robust tactical blueprints by comprehending market trajectories, competitive milieus, and nascent trends. With the aid of data scrutinization, managers can discern openings, foresee hurdles, and devise nimble strategies that capitalize on market prospects and mitigate risks.

Tactical planning steered by data grants managers the capacity to delineate a lucid trajectory and strategically apportion resources to propel long-standing outcomes.

Performance Optimization

Data science allows managers to streamline workflows, improve employee engagement, and improve customer satisfaction across the entire organization. Whether it’s operations and marketing, finance, or human resources, managers can use data science to analyze performance metrics, identify barriers to success and optimize processes to drive productivity, efficiency, and profitability across the organization.

Whether it’s workflows that need to be streamlined, employee engagement that needs to be enhanced, or customer satisfaction that needs to be improved across the entire organization, data driven performance optimization initiatives allow managers to make measurable improvements and Leadership Transformation efforts.

Innovation and Creativity

In order to thrive and distinguish themselves in the market, every thriving organization necessitates originality. Data science, through offering supervisors opportunities for experimentation, exploration, and investigation, nurtures inventiveness. Supervisors can uncover unaddressed needs, explore innovative product ideas, and devise imaginative answers that resonate with customers through the analysis of market information, consumer actions, and emerging technologies. By promoting trial and creativity, data-driven innovation empowers managers to guide radical transformation and position their enterprises as pioneers in their specific fields.

Risk Management

Efficient hazard control is crucial for safeguarding institutional possessions, shielding against ambiguities, and ensuring enduring sustainability. The realm of data science empowers administrators to pinpoint, evaluate, and alleviate hazards through leveraging data, uncovering plausible weaknesses, and envisioning forthcoming consequences. Be it overseeing financial hazards, cyber threats, or logistical hitches, data-centered hazard control methodologies authorize administrators to preemptively tackle obstacles and safeguard the welfare of their institutions. Through harnessing data analysis and anticipatory modeling, administrators can render judicious judgments that curtail hazards and optimize prospects for triumph.

In closing, data science is reshaping managers into trailblazers of triumph by endowing them with invaluable perceptions, strategic prescience, and inventive quandary-resolution proficiencies. From well-informed verdict formulation and strategic scheming to efficiency enhancement, originality, and hazard control, data-infused methodologies empower managers to propel institutional eminence and attain enduring prosperity in the contemporary business milieu."
https://www.analyticsinsight.net/power-of-chatgpt-how-it-can-supercharge-data-science-career/,Power of ChatGPT: How It Can Supercharge Data Science Career,"ChatGPT: Revolutionizing data science careers through enhanced learning and workflow

Data science career demands everyone to continuously sharpen their skills, stay organized, and keep up with the time-consuming routine. Enter ChatGPT, the AI tool that could be a real game changer as it offers a wide range of options even to people in this profession as well as those just starting up. From changing the structure of learning processes to assisting full-stack development, ChatGPT looks like an indispensable app we didn’t even know we needed.

Learning Processes:

Data scientists have to work with a complex data environment where continuous learning and upgrading are required. With the help of ChatGPT, the professionals can highly speed up the process of education and realize productivity growth. Data professionals can achieve better performance and maximize the use of their time by integrating AI tools into their work process.

Automation of repetitive tasks, boosting production rates, as well as quality assurance are some of the added advantages. Additionally, AI tools like ChatGPT can readily provide employees with the information they need. While it can be a no-frills back to basics brushing up of foundational principles or a worthwhile delve into the most advanced topics, ChatGPT still becomes a dynamic and interactive learning tool.

Elevating Interview Preparation

The interview for data scientist ascends a terrorist to a state of tension as you need to fully understand the technical concepts and be able to talk about them lucidly. ChatGPT works as a human virtual coach, helping users to practice how to tackle real-world interview questions such as on technical topics and imitating real-life scenarios. Alongside developing their capabilities, data science hopefuls can gain courage and wrestle for an interviewing success with ChatGPT.

Revolutionizing Workflow with Plugins

In addition to being a very remarkable aspect, ChatGPT’s versatility is due mostly to the vast list of plugins that have been developed in order to cater for different needs. These tools have changed mainstream data scientists literally creating an analogy that results in retrieval of the information, and the calculation process ends up pairing it with the third-party service while making sure that its safety is in action. ChatGPT plugins help data scientists work faster and smarter, ranging from conducting research to carrying out complex computations, and deriving insights from tables.

Applications Beyond Data Science

Despite the fact that most people associate ChatGPT with the science of data retrieval, the use of digital services goes much further than this. Full-stack developers can use ChatGPT to work on their communication system. This system interacts in a natural and conversational manner and they can code that is right and can do so. Also, they are updated with the latest technologies and practices. Beyond all that, ChatGPT could work on complex data analysis tasks and recognize trends and patterns, create automatic data exchange methods, improve software security, and supply secure solutions for routine software-related queries like personally transmitting files with limited access.

Conclusion:

The requirement for data science specialists is steadily growing ever the demand for their services to meet the high demand. Thus, being at the forefront by adopting innovative tools and technologies is the key to maintaining competitiveness in this field. ChatGPT embodies the next-generation of data-science, that offers innovative tools tailored to forecast the future and propel professional careers to higher levels. Whether it be improving learning processes, revolutionizing interview preparation, creating workflow productivity with plugins, or expanding the scope of data science applications while it works to change the exacting methodology of business professionals. Through the utilization of data science, the ChatGPT system offers scientists ways to exploit new opportunities, cope with setbacks, and remain valid in a data-based world."
https://www.thisdaylive.com/index.php/2024/04/11/data-science-expert-wants-healthcare-boosted-with-ai-innovations,Data Science Expert Wants Healthcare Boosted with AI Innovations,"Data science expert, Adeyinka Mayowa-Majaro has disclosed that Artificial Intelligence is a powerful tool to help the growth of global health sector.

The expert added that if artificial intelligence is embraced in Africa just like it is in the United States, it can help to revolutionize the sector.

Disclosing this to the press recently, she stated the need to leverage on AI innovations to address critical challenges and enhance patient care.

She said, “The healthcare sector in the US is an example of how technology through AI innovations, is enhancing patient care and streamlining operations

“As a data scientist and a contributor to the tech space,

My goal is not just to innovate for the sake of innovation but to create tangible improvements in healthcare delivery.”

Mayowa-Majaro said this sentiment underscores her commitment to implementing AI-driven technologies that have a meaningful impact on healthcare outcomes.

One of the key areas of focus in Mayowa-Majaro’s endeavors is the development of predictive modeling and machine learning algorithms tailored to healthcare settings.

By harnessing the power of AI, she aims to enhance diagnostic accuracy, optimize treatment plans, and ultimately improve patient outcomes.

“AI has the potential to revolutionize how healthcare is delivered,” she explains. “By analyzing vast amounts of data, we can identify patterns and trends that enable us to predict diseases earlier and tailor treatments to individual patients.”

Central to Mayowa-Majaro’s approach is the concept of personalized medicine, wherein AI algorithms analyze patient data to tailor treatment plans based on individual characteristics and medical histories.

This personalized approach not only maximizes the efficacy of treatments but also minimizes adverse effects, ultimately leading to better patient experiences.

Furthermore, Mayowa-Majaro emphasizes the importance of AI in streamlining administrative tasks and operational processes within healthcare facilities.

By automating routine tasks and optimizing resource allocation, AI technologies can free up healthcare professionals to focus on delivering quality care to patients.

“AI has the potential to alleviate some of the administrative burdens faced by healthcare providers, allowing them to devote more time and attention to patient care,” she asserts.

Looking ahead, Mayowa-Majaro remains steadfast in her commitment to advancing AI-driven solutions in healthcare.

Collaborating with industry stakeholders and academic institutions, she seeks to further refine and deploy her innovations to address the evolving needs of the healthcare sector.

“The journey towards revolutionizing U.S. healthcare with AI is ongoing,” she concludes. “But with continued collaboration and innovation, we can access the full potential of AI to improve patient outcomes and transform the healthcare sector for the better.”

Mayowa-Majaro continues to lead the charge in integrating AI into healthcare, her visionary work not only showcases the potential of data science but also birth a new era of innovation in the healthcare industry.

With a commitment to advancing patient care through technology, Mayowa-Majaro’s contributions reflects her dedication to revolutionizing healthcare delivery and improving outcomes for patients worldwide."
https://www.mcgill.ca/cscds/channels/event/edugenai-shaping-tomorrows-learning-generative-ai-356066,EduGenAI: Shaping Tomorrow's Learning with Generative AI,"DATE: May 13th & 14th 2024

LOCATION : Arts Building, room 150, McGill University, 853 Sherbrooke West

Note: On May 13th AM, there will be an Upskill Workshop at Google HQ. Further details will be sent to participants.

Overview: In this 2-day symposium, we will explore the impacts and applications of generative AI (GenAI) in higher education from academic and industry perspectives, focusing on both the theoretical and practical sides of GenAI.

Designed to foster connections between academic research and real-world application, this bilingual event (English and French) will take place at the McGill University Downtown campus and will feature presentations from academic and industry leaders through keynote speeches, panel discussions, and hands-on workshops, exploring best practises and the ethical implications of using GenAI in higher education.

Food and beverages will be provided. Spaces are limited, so please make sure to register below. Further event details will be sent to registrants by email.

Schedule

Co-Hosted by:"
https://www.nasa.gov/earth/nasas-pace-data-on-ocean-atmosphere-climate-now-available/,"NASA’s PACE Data on Ocean, Atmosphere, Climate Now Available","NASA is now publicly distributing science-quality data from its newest Earth-observing satellite, providing first-of-their-kind measurements of ocean health, air quality, and the effects of a changing climate.

The Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) satellite was launched on Feb. 8, and has been put through several weeks of in-orbit testing of the spacecraft and instruments to ensure proper functioning and data quality. The mission is gathering data that the public now can access at https://pace.oceansciences.org/access_pace_data.htm.

PACE data will allow researchers to study microscopic life in the ocean and particles in the air, advancing the understanding of issues including fisheries health, harmful algal blooms, air pollution, and wildfire smoke. With PACE, scientists also can investigate how the ocean and atmosphere interact with each other and are affected by a changing climate.

“These stunning images are furthering NASA’s commitment to protect our home planet,” said NASA Administrator Bill Nelson. “PACE’s observations will give us a better understanding of how our oceans and waterways, and the tiny organisms that call them home, impact Earth. From coastal communities to fisheries, NASA is gathering critical climate data for all people.”

“First light from the PACE mission is a major milestone in our ongoing efforts to better understand our changing planet. Earth is a water planet, and yet we know more about the surface of the moon than we do our own oceans. PACE is one of several key missions – including SWOT and our upcoming NISAR mission – that are opening a new age of Earth science,” said Karen St. Germain, NASA Earth Science Division director.

The satellite’s Ocean Color Instrument, which was built and managed by NASA’s Goddard Space Flight Center in Greenbelt, Maryland, observes the ocean, land, and atmosphere across a spectrum of ultraviolet, visible, and near infrared light. While previous ocean color satellites could only detect a handful of wavelengths, PACE is detecting more than 200 wavelengths. With this extensive spectral range, scientists can identify specific communities of phytoplankton. Different species play different roles in the ecosystem and carbon cycle — most are benign, but some are harmful to human health — so distinguishing phytoplankton communities is a key mission of the satellite.

PACE’s two multi-angle polarimeters, HARP2 and SPEXone, measure polarized light that has reflected off clouds and tiny particles in the atmosphere. These particles, known as aerosols, can range from dust to smoke to sea spray and more. The two polarimeters are complementary in their capabilities. SPEXone, built at the Netherlands Institute for Space Research (SRON) and Airbus Netherlands B.V., will view Earth in hyperspectral resolution – detecting all the colors of the rainbow – at five different viewing angles. HARP2, built at the University of Maryland, Baltimore County (UMBC), will observe four wavelengths of light, with 60 different viewing angles.

With these data, scientists will be able to measure cloud properties — which are important for understanding climate — and monitor, analyze, and identify atmospheric aerosols to better inform the public about air quality. Scientists will also be able to learn how aerosols interact with clouds and influence cloud formation, which is essential to creating accurate climate models.

“We’ve been dreaming of PACE-like imagery for over two decades. It’s surreal to finally see the real thing,” said Jeremy Werdell, PACE project scientist at NASA Goddard. “The data from all three instruments are of such high quality that we can start distributing it publicly two months from launch, and I’m proud of our team for making that happen. These data will not only positively impact our everyday lives by informing on air quality and the health of aquatic ecosystems, but also change how we view our home planet over time.”

The PACE mission is managed by NASA Goddard, which also built and tested the spacecraft and the ocean color instrument. The Hyper-Angular Rainbow Polarimeter #2 (HARP2) was designed and built by the University of Maryland, Baltimore County, and the Spectro-polarimeter for Planetary Exploration (SPEXone) was developed and built by a Dutch consortium led by Netherlands Institute for Space Research, Airbus Defence, and Space Netherlands.

By Erica McNamee

NASA’s Goddard Space Flight Center, Greenbelt, Md."
https://www.kdnuggets.com/ai-con-usa-navigate-the-future-of-ai-2,AI Con USA: Navigate the Future of AI 2024,"Partnership Content

AI Con USA, happening June 2–7, 2024 in Las Vegas and online, is not to be missed.

The conference week includes in-depth pre-conference training, deep-dive tutorials, visionary keynotes, concurrent sessions on the latest topics, a leadership summit, an Expo packed with solutions, myriad networking opportunities, and more!

A Preview of Keynotes at AI Con USA:

I Got 99 Problems, but AI Ain’t One—Dona Sarkar, Microsoft

AI/ML Adoption Strategies for Enterprises—Hien Luu, DoorDash

The Unseen Engine of AI: How 5 Innovation-Minded Companies Optimized for Operational Efficiency—Nevra Ledwon, DecisionBrain

Operationalizing Disruptive Technologies: A Strategic Framework for Harnessing the Power of GenAI—Mary Thorn, S&P Global Ratings

AI: A Moderated Panel Discussion—Dionny Santiago, Indeed

Humanizing AI—Tariq King, Test IO

Realizing the Potential of AI Tools for Software Development—Matthew Gunter, GitHub

Embrace AI Holistically and Unlock Your Growth Potential—Tania Katan and Rob Nicoletti, HALO Strategies

Join us for a year’s worth of education packed into one amazing week.

Can't join us in-person? A curated, free virtual conference option is also available."
https://www.datanami.com/this-just-in/rivos-raises-more-than-250m-targeting-data-analytics-and-generative-ai-markets/,Rivos Raises More Than $250M Targeting Data Analytics and Generative AI Markets,"SANTA CLARA, Calif., April 16, 2024 — Rivos, a RISC-V accelerated platform company targeting data analytics and Generative AI, announced it has raised more than $250M in its oversubscribed Series A-3 funding round, and Matrix Capital Management, the largest investor in this round, has joined the Board represented by Romit Shah.

The funding saw participation from new investors including Intel Capital, MediaTek, Cambium Capital, CIDC, Capital TEN, and Hotung Venture Group, and increased participation from existing investors Walden Catalyst, Dell Technologies Capital, Koch Disruptive Technologies, and VentureTech Alliance.

The funding will enable the company to tapeout its first silicon product and to meet increasing customer demand by expansion of manufacturing operations, platform hardware, software engineering, and support functions.

“The rapid changes in LLMs and the merger with the data analytics stack makes it vital that accelerators be easy to program and debug, and that data can seamlessly move between CPU and accelerator. Rivos addresses this need through our recompile-not-redesign approach,” said Puneet Kumar, co-founder and CEO of Rivos. “I’m grateful for Matrix’s vote of confidence in this approach and pleased to welcome Romit to the Board.”

Rivos provides power optimized chips combining high performance server-class RISC-V CPUs and a Data Parallel Accelerator (a GPGPU optimized for large language models (LLMs) and data analytics) that work with today’s software programming models and rack server constraints. The tight integration of CPU and parallel computation sharing a uniform memory across DDR DRAM and HBM is ideal for today’s models and databases that need terabytes of memory. Rivos is one of the leaders of the RISC-V open ecosystem. Calista Redmond, CEO of RISC-V International, commented: “Rivos quickly became an active participant in the community. The depth of experience from the Rivos engineers has enabled RISC-V to leap ahead in ISA, system and security specifications.” On the software side Rivos was an instigator of the RISC-V Software Ecosystem (RISE) initiative.

“Expanding the application of data analytics and large language models to encompass not just traditional data types such as text, images, and video, but also genomic and medical data, unlocks unprecedented opportunities for innovation in research and treatment,” remarked David Goel, Managing General Partner of Matrix Capital Management. “The Rivos team has adeptly integrated the groundbreaking new RISC-V architecture with an inventive accelerator, effectively bringing this vision to life. Their prototype chip serves as a compelling demonstration of their unique capability to leverage the advanced TSMC 3nm process node – a feat few startups have managed to achieve. We eagerly anticipate furthering our support for the company’s move into new realms of achievement, marking each milestone with shared enthusiasm and commitment to excellence.”

Headquartered in Santa Clara, CA, with offices in the USA, UK, EU and Asia, Rivos is hiring engineering talent across multiple disciplines.

About Rivos

Founded in May 2021, Rivos has assembled a world class team of silicon, software and platform designers. The company is backed by premier financial and strategic investors who share its long term vision of building industry-leading power efficient, high performance, secure server solutions, based on RISC-V, using workload-defined hardware. Rivos supports the intense requirements of the large language models and data analytics that will remake the enterprise, by providing the full solution of optimized chips combining RISC-V CPUs and a Data Parallel Accelerator, a reference multi-chip OCP modular server, and a full firmware-to-application open software stack. Customer workloads are easily deployed using their existing models giving an immediate TCO benefit. Headquartered in Santa Clara, CA, with offices in the USA, UK, EU and Asia, Rivos is hiring engineering talent across multiple disciplines.

Source: Rivos"
https://www.analyticsinsight.net/which-programming-language-is-the-backbone-of-data-science/,Which Programming Language is the Backbone of Data Science,"Exploring the Backbone of Data Science: The Role of Programming Languages

In data science, programming is crucial for finding patterns, creating models, and making informed decisions. Data scientists use different languages to uncover insights from large datasets and drive progress in various industries. A few key programming languages stand out and play essential roles in data science.

Python: The All-Purpose Language

Python is the top choice for data science due to its flexibility, user-friendliness, and many useful tools. Data scientists can quickly test out ideas and work with complex data thanks to Python’s clear and easy-to-understand code. Python also has libraries like NumPy, Pandas, and Matplotlib, which help with data analysis and visualization. It’s also widely used for machine learning, making Python the preferred language for everything from basic data exploration to advanced model building.

R: The Statistical Powerhouse

While Python is popular, R is still important, especially for statistics and data visualization. R was designed by statisticians for statisticians, and it offers a wide range of packages for different analytical tasks, which is why it’s favored by researchers and academics. With packages like dplyr, ggplot2, and caret, R enables users to conduct complex data manipulations, create compelling visualizations, and build advanced statistical models. Its focus on statistical accuracy and exploring data in-depth makes R essential for statisticians and data scientists.

SQL: The Language of Data Manipulation

While Python and R excel in data analysis and statistical computing, SQL (Structured Query Language) serves as the backbone for data manipulation and database management. Essential for extracting, transforming, and querying data from relational databases, SQL forms an integral part of the data science toolkit. Data scientists leverage SQL to perform tasks such as filtering datasets, aggregating information, and joining tables, enabling efficient data wrangling and preprocessing. With the proliferation of relational databases in business environments, proficiency in SQL is indispensable for data professionals seeking to harness the full potential of their data assets.

Java and Scala: Powering Big Data Processing

In the era of big data, Java and Scala emerge as stalwarts for distributed computing and parallel processing. With frameworks like Apache Hadoop and Apache Spark revolutionizing the way data is processed at scale, these languages play a vital role in building robust, scalable data pipelines. Java’s widespread adoption and robust ecosystem make it a natural choice for developing enterprise-grade applications, while Scala’s concise syntax and compatibility with Spark’s functional programming paradigm offer unparalleled performance for data-intensive tasks. Together, Java and Scala empower data scientists to tackle challenges associated with massive datasets and real-time analytics, laying the groundwork for innovation in the age of big data.

Julia: The Rising Star of Scientific Computing

While Python and R dominate the landscape of data science, Julia emerges as a promising contender for scientific computing and numerical analysis. Designed for high-performance computing, Julia combines the ease of use of dynamic languages with the speed of traditional compiled languages, making it an ideal choice for computationally intensive tasks. With built-in support for parallelism and distributed computing, Julia excels in domains such as mathematical optimization, machine learning, and numerical simulations. While still gaining traction within the data science community, Julia’s growing popularity underscores its potential to reshape the future of scientific computing.

In the multifaceted world of data science, programming languages serve as the backbone upon which analytical insights are derived, models are built, and decisions are made. From Python’s versatility and ease of use to R’s statistical prowess, from SQL’s data manipulation capabilities to Java and Scala’s prowess in big data processing, each language brings its own unique strengths to the table."
https://www.analyticsinsight.net/ibm-free-certification-courses-in-data-science-2024/,"IBM FREE Certification Courses in Data Science, 2024","Free Certification Courses in Data Science Offered by IBM in 2024

IBM is a leading American tech company that has clients in over 175 countries. IBM helps businesses with data and to save money. IBM is offering free online data science certification courses available on the IBM Training Platform. Ranging from introductory data science courses to practical introduction to machine learning with Python, these data science courses are designed for everyone whether you are a beginner or an experienced professional. Here are the free data science classes from IBM to help you succeed.

1. Fundamentals of Data Science on IBM Cloud

The Fundamentals of Data Science course involves the service of IBM Cloud Pak for Data. IBM Cloud Pak is a web-based solution provided by IBM. The curriculum of course includes topics such as basics of data management in the cloud, manipulation of data, and data analysis and the key focus is publishing, developing, and monitoring machine learning solutions.

2. Introduction to Tools and Techniques of Data Science

The course Introduction to Tools and Techniques of Data Science provides a basic understanding of data science involving data science concepts. This course is designed for beginners providing thorough guidance on the usage of data science tools to learners and data science enthusiasts so that they get in-depth knowledge about data science tools. This course covers Jupyter Notebooks, SQL, Python, and other open-source data sources.

3. Databases and SQL for Data Science with Python

Learn how to use SQL and Python for data analysis within relational databases in this course. This course enables data science enthusiasts to use Data Definition Language (DDL) commands for the management of databases and work with multiple tables. This course covers topics for advanced data analysis including stored procedures, views, and transactions designed for professional data analysts.

4. Data Analysis with Python

Learn Python coding for data analysis and data preparation with Data Analysis with Python course offered by IBM. The curriculum of the course includes Numpy, Pandas and Scipy for data manipulation and data summarization with data frames. The course teaches you to regression models for data correlation by using the scikit-learn library for future forecasting and data-driven decision-making.

5. Machine Learning with Python: A Practical Introduction

The Machine Learning with Python: A Practical Introduction course teaches learners about the basics of machine learning and machine learning algorithms covering classification and regression. This course covers the topics providing realistic examples that can applied on the real world and hands-on experience on models like Train/Test Split and Random Forests.

6. Introduction to Big Data with Spark and Hadoop

Introduction to Big Data with Spark and Hadoop course is designed for beginners and is subdivided into two different programs namely IBM Data Engineering Professional Certificate and NoSQL, Big Data, and Spark Foundations Specialization. Learners will learn the application of Big Data in Big Data Analytics, Big Data analysis with Hadoop, and Resilient Distributed Datasets (RDDs). The data analysis enthusiasts will learn about Big Data processing tools, functions of Big Data processing tools, limitations of Big Data processing tools, and their applications in the real world.

With these recognized data analysis courses offered by IBM, data science enthusiasts will gain hands-on experience and will learn the real-world applications of data science to enhance their skills in data analysis in this competitive field of data science."
https://viterbischool.usc.edu/news/2024/04/sophomores-outshine-global-competitors-in-adobe-analytics-challenge/,Sophomores Outshine Global Competitors in Adobe Analytics Challenge - USC Viterbi,"Colin Wang (’26 Applied Mathematics/USC Dornsife), Tommy Zhan Shu (’26 Biology/USC Dornsife and Data Science/USC Viterbi), and Claude Hana Yoo (’26 Computer Science/USC Viterbi), guided by mentor Mike Lee, Senior Lecturer in Applied Analytics and Enterprise Systems at the School of Advanced Computing, clinched second place in the Adobe Analytics Challenge on March 25, 2024, in Las Vegas. Competing against over 2,000 teams globally, their achievement highlights the practical application of their studies in a highly competitive setting.

This competition, lauded for its intensity and scope, is a cornerstone event for students to engage with real-world data through Adobe’s analytics tools. “One of the largest academic collegiate competitions in the world,” as described by Sandor Jones, Senior Product Marketing Manager at Adobe, it challenges participants to tackle genuine business cases with advanced analytical solutions.

The journey to the finals was rigorous. After surpassing thousands of teams to enter the semi-finals among the top 20, they excelled further to secure a spot in the final four. Competing against all graduate students, their presence in the finals underscored their exceptional talent and preparation. Nate Smith, Director of Product Marketing at Adobe, acknowledged their unique achievement, stating, “the team was the youngest ever to make the finals in the 18-year history of the competition.”

Claude Hana Yoo reflected on the competition, emphasizing the real-world skills it honed. “My favorite part of the competition was how it gave us the chance to exercise so many real-world skills. Our data was very raw, which is realistic to most data analysis tasks, and we had to professionally present our findings with clean slide decks and data visualizations to senior executives.”

Colin Wang shared his thoughts on the practical experience gained, “The Adobe Analytics Challenge was one of the most enjoyable and challenging competitions I’ve had the opportunity to be a part of. Working with real company data, which was terribly organized, was a practical way to apply the skills I learned in my classes.”

Tommy Zhan Shu shared his perspective on diving into data analysis, “At the onset of looking at data, it’s always a pile of organized chaos and never something you can directly just understand. But as you delve deeper and start to compile information and sort out the data, stories become clearer and numbers more comprehensive. To me, it’s incredible to realize how the data points represent individual storylines that each person contributed, encapsulated through the figures present. That’s what makes these experiences interesting—it’s not just about understanding numbers but rather the people behind them.”

Mentor Mike Lee spoke to the broader educational philosophy their success represented: “This shows the strength of our interdisciplinary approach to undergraduate education, which is also one of the underpinnings of the new School of Advanced Computing. This approach equips our students to tackle complex challenges, blending theory with practical application in a real-world setting.”

Published on April 10th, 2024"
